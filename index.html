<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title property="foaf:name schema:name">Storing and Querying Evolving Knowledge Graphs on the Web</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" media="all"    href="styles/katex.css" />
  <meta name="citation_title" content="Storing and Querying Evolving Knowledge Graphs on the Web">
  <meta name="citation_author" content="Ruben Taelman" />
  
  <meta name="citation_publication_date" content="2019/12/18" />
</head>

<body prefix="dctypes: http://purl.org/dc/dcmitype/ pimspace: http://www.w3.org/ns/pim/space# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# lsc: http://linkedscience.org/lsc/ns#" typeof="schema:CreativeWork sioc:Post prov:Entity lsc:Research">
  <div class="header-wrapper">
<div class="empty-page">&nbsp;</div>
<div class="empty-page">&nbsp;</div>
<header>
    <h1 id="storing-and-queryingbr-evolving-knowledge-graphsbr-on-the-web">Storing and Querying<br />Evolving Knowledge Graphs<br />on the Web</h1>

    <h2 id="de-opslag-en-bevragingbr-van-evoluerende-kennisgrafenbr-op-het-web">De opslag en bevraging<br />van evoluerende kennisgrafen<br />op het Web</h2>

    <div style="font-size: 1.3em">
      <ul id="authors">
        <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://www.rubensworks.net/" typeof="foaf:Person schema:Person" resource="https://www.rubensworks.net/#me">Ruben Taelman</a></li>
      </ul>
    </div>

    <div class="purpose">Thesis for obtaining the degree Doctor of Computer Science Engineering</div>
    <div class="printonly canonical" style="height: 8em">
    <br />Canonical version: <a href="https://phd.rubensworks.net/">https:/​/​phd.rubensworks.net/</a>
</div>

    <div style="font-size: 1.1em">
      <p><br />
<strong>Advisors:</strong>
<br /></p>

      <ul>
        <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://ruben.verborgh.org/" typeof="foaf:Person schema:Person" resource="https://ruben.verborgh.org/profile/#me">prof. dr. ir. Ruben Verborgh</a></li>
        <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="#" typeof="foaf:Person schema:Person" resource="https://data.verborgh.org/people/miel_vander_sande">dr. Miel Vander Sande</a></li>
      </ul>
    </div>

    <p><a href="https://ugent.be/"><img src="img/logo_ugent.svg" alt="Ghent University" class="screenonly" style="float: left; height: 150px" /></a>
<a href="https://ugent.be/"><img src="img/logo_ugent_black.svg" alt="Ghent University" class="printonly" style="float: left; height: 150px" /></a></p>

    <ul id="affiliations">
      <li id="idlab"><a href="https://www.ugent.be/ea/idlab/en">IDLab</a><br />
          <a href="https://www.elis.ugent.be/">Department of Electronics and Information Systems</a><br />
          <a href="https://www.ugent.be/ea">Faculty of Engineering and Architecture</a><br />
          <a href="https://ugent.be/">Ghent University</a> – <a href="https://www.imec.org/">imec</a><br />
          Academic year: 2019–2020</li>
    </ul>

  </header>
</div>

<!-- Print-only page with ISBN and stuff -->
<div class="printonly">
<div style="height: 50em"></div>
<span style="font-style: italic">Storing and Querying Evolving Knowledge Graphs on the Web</span><br />
ISBN: TBD<br />
NUR-code(s): TBD<br />
Legal deposit number: TBD<br />
<div class="empty-page">&nbsp;</div>
</div>

<section id="frontmatter">
  <section id="abstract" inlist="" rel="schema:hasPart" resource="#abstract">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Abstract</h2>

      <!-- Context      -->
      <p>The Web has become our most valuable tool for sharing information.
Currently, this Web is mainly targeted at humans,
whereas machines typically have a hard time understanding information on the Web.
Using <em>knowledge graphs</em>, this information can be linked in a structured way,
so that intelligent agents can act upon this data autonomously.
Current knowledge graphs remain however rather static.
As there is a lot of value in acting upon <em>evolving</em> knowledge,
<!-- Need         -->
there is a need for <em>evolving knowledge graphs</em>,
and ways to manage them.
<!-- Task         -->
As such, the goal of this PhD is to allow such evolving knowledge graphs to be <em>stored</em> and <em>queried</em>,
taking into account the <em>decentralized</em> nature of the Web
where anyone should be able say anything about anything.
<!-- Object       -->
Concretely, four challenges related to this goal are investigated:
(1) generation of evolving data,
(2) storage of evolving data,
(3) querying over heterogeneous datasets,
and (4) querying evolving data.
<!-- Findings     -->
For each of these challenges, techniques and algorithms have been developed,
<!-- Conclusion   -->
which prove to be valuable for storing and querying evolving knowledge graphs on the Web.
<!-- Perspectives -->
This work therefore brings us closer towards a Web
in which both human and machine can act upon evolving knowledge.</p>

    </div>
</section>

  <section id="examination-board" inlist="" rel="schema:hasPart" resource="#examination-board">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Examination Board</h2>

      <ul>
        <li>Prof. Antoon Bronselaer</li>
        <li>Prof. Olaf Hartig</li>
        <li>Prof. Katja Hose</li>
        <li>Prof. Erik Mannens</li>
        <li>Prof. Femke Ongenae</li>
      </ul>

      <p><strong>Chair</strong></p>

      <ul>
        <li>Prof. Filip De Turck</li>
      </ul>

      <p><strong>Advisors</strong></p>

      <ul>
        <li>Prof. Ruben Verborgh</li>
        <li>Dr. Miel Vander Sande</li>
      </ul>

    </div>
</section>

  <section id="preface" inlist="" rel="schema:hasPart" resource="#preface">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Preface</h2>

      <p>For as long as I can remember, <em>making things</em> has been my greatest passion.
As I was fortunate enough to grow up with computers around me,
I quickly became fascinated by them,
and their ability to automate processes through sequences of instructions.
Next to that, I was born at the perfect time to experience the introduction of the Web to the public.
Hence, I grew up together with it, which has impacted my life significantly.
Thanks to the Web, I was able to easily <em>access information</em> on computers, programming, and Web technologies.
This allowed me to <em>learn</em>, and to <em>create</em> new things.</p>

      <p>As a teenager, I was never really the type of person that played a lot of video games.
Nevertheless, like most people of my generation, I often came in contact with them.
Instead of being captivated to <em>play</em> these games,
I was often intrigued by the way these games <em>worked</em>.
This caused me to ponder on their internal processes,
and wondering what it would take to make it myself.
As such, I occasionally set out to <a href="https://www.rubensworks.net/projects/">implement or extend certain games</a>.
The thing I enjoyed the most, however, was building things that connect people over the Web,
which is why I also spent quite some time building Web sites to <a href="https://www.rubensworks.net/projects/tgcgames/">distribute games</a>
and <a href="https://www.rubensworks.net/projects/allectroradio/">broadcast music</a>.</p>

      <p>After I graduated secondary school,
the obvious choice was to pursue a further education related to computers and programming,
which is how I ended up at Ghent University.
The most impactful year for me was the final year of my Master’s,
which is when I worked on my thesis.
Because of my existing interest in Web technologies,
I chose for a topic in the <em>Semantic Web</em> domain.
Under the excellent supervision of <a href="https://ruben.verborgh.org/">Ruben Verborgh</a> and <a href="https://pietercolpaert.be/">Pieter Colpaert</a>,
I investigated <a href="https://www.rubensworks.net/publications/taelman_mastersthesis/"><em>continuous querying</em></a> within the Semantic Web.
Due to the motivating guidance of Ruben and Pieter, and my interest in the domain,
I continued upon that research as a PhD topic, and became their colleague at Multimedia Lab.</p>

      <p>Working at <strike>Multimedia Lab</strike>, <strike>Data Science Lab</strike>, <a href="https://www.ugent.be/ea/idlab/en">IDLab</a>
has been very exciting so far.
I came in contact with many new interesting people,
learned about new technologies,
and traveled around the world.
Most importantly, I was able to (at least slightly) advance the research domain
through the contributions that are described in this PhD thesis.
All of this was of course impossible without <a href="https://en.wikiquote.org/wiki/Isaac_Newton">standing on the shoulders of giants</a>.
These giants are on the one hand all researchers that my work builds upon,
and on the other hand everyone that has directly or indirectly supported this work.</p>

      <p>I thank all my current and past IDLab colleagues from our Semantic Web office:
Anastasia, Ben, Brecht, Dieter D. P., Dieter D. W., Dörthe, Erik, Gerald, Harm, Julián, Joachim, Laurens, Martin, Miel, Pieter H., Pieter C., Ruben, Sven, Tom.
Each of them has shaped this research in one way or another.
Either by providing feedback, coming up with new ideas that inspired me, or by simply offering help when I needed it.
In particular, I thank Ruben and Miel for their tireless enthusiasm and motivation.
I was able to learn a lot from you, which has definitely helped me in becoming a better researcher.
I am grateful to both Ruben and Pieter C. for inviting me to pursue a PhD at IDLab,
it has influenced my life in a very positive way, and I can not dream of a better job.
I am also thankful of all the critical and constructive feedback Ruben, Miel, Pieter and Anastasia have given me.</p>

      <p>I thank my family for being there all my life, and shaping me into the person I have become.
Mom and dad, thank you being there,
for all the time and effort you have spent in me,
and all the opportunities you have provided,
I love you both.
Finally, Elke, I love you for showing me the sides of life that were unknown to me,
and I am glad to be living it with you.</p>

      <p><br />
Ruben<br />
August 2019</p>

    </div>
</section>

  <section id="toc" inlist="" rel="schema:hasPart" resource="#toc">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Table of Contents</h2>

      <ol class="index-entries index-entries-root" depth="0"><li><a href="#introduction" class="index-entry-name">Introduction</a><ol class="index-entries" depth="1"><li><a href="#introduction-the-web" class="index-entry-name">The Web</a><ol class="index-entries" depth="2"><li><a href="#catalysts-for-human-progress" class="index-entry-name">Catalysts for Human Progress</a></li><li><a href="#impact-of-the-web" class="index-entry-name">Impact of the Web</a></li><li><a href="#knowledge-graphs" class="index-entry-name">Knowledge Graphs</a></li><li><a href="#evolving-knowledge-graphs" class="index-entry-name">Evolving Knowledge Graphs</a></li><li><a href="#decentralized-knowledge-graphs" class="index-entry-name">Decentralized Knowledge Graphs</a></li></ol></li><li><a href="#introduction-research-question" class="index-entry-name">Research Question</a><ol class="index-entries" depth="2"></ol></li><li><a href="#introduction-outline" class="index-entry-name">Outline</a><ol class="index-entries" depth="2"></ol></li></ol></li><li><a href="#generating" class="index-entry-name">Generating Synthetic Evolving Data</a><ol class="index-entries" depth="1"><li><a href="#generating_introduction" class="index-entry-name">Introduction</a><ol class="index-entries" depth="2"></ol></li><li><a href="#generating_related-work" class="index-entry-name">Related Work</a><ol class="index-entries" depth="2"></ol></li><li><a href="#generating_public-transit-background" class="index-entry-name">Public Transit Background</a><ol class="index-entries" depth="2"><li><a href="#public-transit-planning" class="index-entry-name">Public Transit Planning</a></li><li><a href="#transit-feed-formats" class="index-entry-name">Transit Feed Formats</a></li></ol></li><li><a href="#generating_research-question" class="index-entry-name">Research Question</a><ol class="index-entries" depth="2"></ol></li><li><a href="#generating_methodology" class="index-entry-name">Method</a><ol class="index-entries" depth="2"><li><a href="#region" class="index-entry-name">Region</a></li><li><a href="#stops" class="index-entry-name">Stops</a></li><li><a href="#edges" class="index-entry-name">Edges</a></li><li><a href="#routes" class="index-entry-name">Routes</a></li><li><a href="#generating_subsec-methodology-trips" class="index-entry-name">Trips</a></li></ol></li><li><a href="#generating_implementation" class="index-entry-name">Implementation</a><ol class="index-entries" depth="2"><li><a href="#podigg" class="index-entry-name">PoDiGG</a></li><li><a href="#podigg-lc" class="index-entry-name">PoDiGG-LC</a></li><li><a href="#configuration" class="index-entry-name">Configuration</a></li></ol></li><li><a href="#generating_evaluation" class="index-entry-name">Evaluation</a><ol class="index-entries" depth="2"><li><a href="#generating_subsec:evaluation:coherence" class="index-entry-name">Coherence</a></li><li><a href="#generating_subsec:evaluation:distance" class="index-entry-name">Distance to Gold Standards</a></li><li><a href="#generating_subsec:evaluation:performance" class="index-entry-name">Performance</a></li><li><a href="#dataset-size" class="index-entry-name">Dataset size</a></li></ol></li><li><a href="#generating_discussion" class="index-entry-name">Discussion</a><ol class="index-entries" depth="2"><li><a href="#characteristics" class="index-entry-name">Characteristics</a></li><li><a href="#usage-within-benchmarks" class="index-entry-name">Usage within Benchmarks</a></li><li><a href="#limitations-and-future-work" class="index-entry-name">Limitations and Future Work</a></li><li><a href="#podigg-in-use" class="index-entry-name">PoDiGG In Use</a></li></ol></li><li><a href="#generating_conclusions" class="index-entry-name">Conclusions</a><ol class="index-entries" depth="2"></ol></li></ol></li><li><a href="#storing" class="index-entry-name">Storing Evolving Data</a><ol class="index-entries" depth="1"><li><a href="#storing_introduction" class="index-entry-name">Introduction</a><ol class="index-entries" depth="2"></ol></li><li><a href="#storing_related-work" class="index-entry-name">Related Work</a><ol class="index-entries" depth="2"><li><a href="#general-rdf-indexing-and-compression" class="index-entry-name">General </a></li><li><a href="#storing_related-work-archiving" class="index-entry-name">RDF Archiving</a></li><li><a href="#related-work-benchmarks" class="index-entry-name">RDF Archiving Benchmarks</a></li><li><a href="#query-atoms" class="index-entry-name">Query atoms</a></li></ol></li><li><a href="#storing_problem-statement" class="index-entry-name">Problem statement</a><ol class="index-entries" depth="2"></ol></li><li><a href="#storing_fundamentals" class="index-entry-name">Overview of Approaches</a><ol class="index-entries" depth="2"><li><a href="#storing_snapshot-delta-chain" class="index-entry-name">Snapshot and Delta Chain</a></li><li><a href="#storing_indexes" class="index-entry-name">Multiple Indexes</a></li><li><a href="#storing_local-changes" class="index-entry-name">Local Changes</a></li><li><a href="#storing_addition-deletion-counts" class="index-entry-name">Addition and Deletion counts</a></li></ol></li><li><a href="#storing_storage" class="index-entry-name">Hybrid Multiversion Storage</a><ol class="index-entries" depth="2"><li><a href="#storing_snapshot-storage" class="index-entry-name">Snapshot storage</a></li><li><a href="#storing_dictionary" class="index-entry-name">Delta Chain Dictionary</a></li><li><a href="#storing_delta-storage" class="index-entry-name">Delta Storage</a></li><li><a href="#storing_addition-counts" class="index-entry-name">Addition Counts</a></li><li><a href="#storing_deletion-counts" class="index-entry-name">Deletion Counts</a></li><li><a href="#storing_metadata" class="index-entry-name">Metadata</a></li></ol></li><li><a href="#storing_ingestions" class="index-entry-name">Changeset Ingestion Algorithms</a><ol class="index-entries" depth="2"><li><a href="#storing_batch-ingestion" class="index-entry-name">Batch Ingestion</a></li><li><a href="#storing_streaming-ingestion" class="index-entry-name">Streaming Ingestion</a></li></ol></li><li><a href="#storing_querying" class="index-entry-name">Versioned Query Algorithms</a><ol class="index-entries" depth="2"><li><a href="#version-materialization" class="index-entry-name">Version Materialization</a></li><li><a href="#delta-materialization" class="index-entry-name">Delta Materialization</a></li><li><a href="#version-query" class="index-entry-name">Version Query</a></li></ol></li><li><a href="#storing_evaluation" class="index-entry-name">Evaluation</a><ol class="index-entries" depth="2"><li><a href="#storing_implementation" class="index-entry-name">Implementation</a></li><li><a href="#experimental-setup" class="index-entry-name">Experimental Setup</a></li><li><a href="#results-3" class="index-entry-name">Results</a></li><li><a href="#discussion" class="index-entry-name">Discussion</a></li></ol></li><li><a href="#storing_conclusions" class="index-entry-name">Conclusions</a><ol class="index-entries" depth="2"></ol></li></ol></li><li><a href="#querying" class="index-entry-name">Querying a heterogeneous Web</a><ol class="index-entries" depth="1"><li><a href="#querying_introduction" class="index-entry-name">Introduction</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying_related-work" class="index-entry-name">Related Work</a><ol class="index-entries" depth="2"><li><a href="#the-different-facets-of-sparql" class="index-entry-name">The Different Facets of SPARQL</a></li><li><a href="#linked-data-fragments" class="index-entry-name">Linked Data Fragments</a></li><li><a href="#software-design-patterns" class="index-entry-name">Software Design Patterns</a></li></ol></li><li><a href="#querying_features" class="index-entry-name">Requirement analysis</a><ol class="index-entries" depth="2"><li><a href="#sparql-query-evaluation" class="index-entry-name">SPARQL query evaluation</a></li><li><a href="#modularity" class="index-entry-name">Modularity</a></li><li><a href="#heterogeneous-interfaces" class="index-entry-name">Heterogeneous interfaces</a></li><li><a href="#federation" class="index-entry-name">Federation</a></li><li><a href="#web-based" class="index-entry-name">Web-based</a></li></ol></li><li><a href="#querying_architecture" class="index-entry-name">Architecture</a><ol class="index-entries" depth="2"><li><a href="#customizable-wiring-at-design-time-through-dependency-injection" class="index-entry-name">Customizable Wiring at Design-time through Dependency Injection</a></li><li><a href="#flexibility-at-run-time-using-the-actormediatorbus-pattern" class="index-entry-name">Flexibility at Run-time using the Actor–Mediator–Bus Pattern</a></li><li><a href="#modules" class="index-entry-name">Modules</a></li></ol></li><li><a href="#querying_implementation" class="index-entry-name">Implementation</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying_comparison-tpf-client" class="index-entry-name">Performance Analysis</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying_conclusions" class="index-entry-name">Conclusions</a><ol class="index-entries" depth="2"></ol></li></ol></li><li><a href="#querying-evolving" class="index-entry-name">Querying Evolving Data</a><ol class="index-entries" depth="1"><li><a href="#querying-evolving_introduction" class="index-entry-name">Introduction</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying-evolving_related-work" class="index-entry-name">Related Work</a><ol class="index-entries" depth="2"><li><a href="#querying-evolving_related-work_annotations" class="index-entry-name">RDF Annotations</a></li><li><a href="#temporal-data-in-the-rdf-model" class="index-entry-name">Temporal data in the </a></li><li><a href="#sparql-streaming-extensions" class="index-entry-name">SPARQL Streaming Extensions</a></li><li><a href="#triple-pattern-fragments" class="index-entry-name">Triple Pattern Fragments</a></li></ol></li><li><a href="#querying-evolving_problem-statement" class="index-entry-name">Problem Statement</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying-evolving_use-case" class="index-entry-name">Use Case</a><ol class="index-entries" depth="2"></ol></li><li><a href="#querying-evolving_dynamic-data-representation" class="index-entry-name">Dynamic Data Representation</a><ol class="index-entries" depth="2"><li><a href="#query-evolving_subsec:temporaldomains" class="index-entry-name">Time Labeling Types</a></li><li><a href="#query-evolving_sec:tatypes" class="index-entry-name">Methods for Time Annotation</a></li></ol></li><li><a href="#querying-evolving_query-engine" class="index-entry-name">Query Engine</a><ol class="index-entries" depth="2"><li><a href="#architecture" class="index-entry-name">Architecture</a></li><li><a href="#algorithms" class="index-entry-name">Algorithms</a></li></ol></li><li><a href="#querying-evolving_evaluation" class="index-entry-name">Evaluation</a><ol class="index-entries" depth="2"><li><a href="#querying-evolving_subsec:Results-ServerCost" class="index-entry-name">Server Cost</a></li><li><a href="#querying-evolving_subsec:Results-ClientCost" class="index-entry-name">Client Cost</a></li><li><a href="#querying-evolving_subsec:Results-AnnotationMethods" class="index-entry-name">Annotation Methods</a></li></ol></li><li><a href="#querying-evolving_conclusions" class="index-entry-name">Conclusions</a><ol class="index-entries" depth="2"><li><a href="#server-cost" class="index-entry-name">Server cost</a></li><li><a href="#client-cost" class="index-entry-name">Client cost</a></li><li><a href="#caching" class="index-entry-name">Caching</a></li><li><a href="#request-reduction" class="index-entry-name">Request reduction</a></li><li><a href="#performance" class="index-entry-name">Performance</a></li><li><a href="#annotation-methods" class="index-entry-name">Annotation methods</a></li></ol></li></ol></li><li><a href="#conclusions" class="index-entry-name">Conclusions</a><ol class="index-entries" depth="1"><li><a href="#contributions" class="index-entry-name">Contributions</a><ol class="index-entries" depth="2"><li><a href="#generating-evolving-data" class="index-entry-name">Generating Evolving Data</a></li><li><a href="#indexing-evolving-data" class="index-entry-name">Indexing Evolving Data</a></li><li><a href="#heterogeneous-web-interfaces" class="index-entry-name">Heterogeneous Web Interfaces</a></li><li><a href="#publishing-and-querying-evolving-data" class="index-entry-name">Publishing and Querying Evolving Data</a></li><li><a href="#overview" class="index-entry-name">Overview</a></li></ol></li><li><a href="#limitations" class="index-entry-name">Limitations</a><ol class="index-entries" depth="2"><li><a href="#generating-evolving-data-1" class="index-entry-name">Generating Evolving Data</a></li><li><a href="#indexing-evolving-data-1" class="index-entry-name">Indexing Evolving Data</a></li><li><a href="#heterogeneous-web-interfaces-1" class="index-entry-name">Heterogeneous Web Interfaces</a></li><li><a href="#publishing-and-querying-evolving-data-1" class="index-entry-name">Publishing and Querying Evolving Data</a></li></ol></li><li><a href="#open-challenges" class="index-entry-name">Open Challenges</a><ol class="index-entries" depth="2"></ol></li></ol></li></ol>

    </div>
</section>

  <section id="acronyms" inlist="" rel="schema:hasPart" resource="#acronyms">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Acronyms</h2>

      <table class="acronyms"><tr><th>API</th><td>Application Programming Interface</td></tr><tr><th>BEAR</th><td>Benchmark of RDF Archives</td></tr><tr><th>C-SPARQL</th><td>Continuous SPARQL</td></tr><tr><th>CB</th><td>Change-based</td></tr><tr><th>CM</th><td>Change Materialization</td></tr><tr><th>CQELS</th><td>Continuous Query Evaluation over Linked Stream</td></tr><tr><th>CPU</th><td>Central Processing Unit</td></tr><tr><th>CSV</th><td>Comma Separated Values</td></tr><tr><th>CV</th><td>Cross-version Join</td></tr><tr><th>CVS</th><td>Concurrent Versions System</td></tr><tr><th>DM</th><td>Delta Materialization</td></tr><tr><th>DSMS</th><td>Data Stream Management System</td></tr><tr><th>GTFS</th><td>General Transit Feed Specification</td></tr><tr><th>HDT-FoQ</th><td>HDT Focus on Querying</td></tr><tr><th>HDT</th><td>Header Dictionary Triples</td></tr><tr><th>HOBBIT</th><td>Holistic Benchmarking of Big Linked Data</td></tr><tr><th>HTTP</th><td>Hypertext Transfer Protocol</td></tr><tr><th>IC</th><td>Independent Copies</td></tr><tr><th>IRI</th><td>Internationalized Resource Identifier</td></tr><tr><th>LUBM</th><td>Lehigh University Benchmark</td></tr><tr><th>OSTRICH</th><td>Offset-Enabled Triple Store for Changesets</td></tr><tr><th>PoDiGG</th><td>POpulation DIstribution-based GTFS Generator</td></tr><tr><th>RAM</th><td>Random Access Memory</td></tr><tr><th>RDF</th><td>Resource Description Framework</td></tr><tr><th>SPARQL</th><td>SPARQL Protocol and RDF Query Language</td></tr><tr><th>TB</th><td>Timestamp-based</td></tr><tr><th>TPF</th><td>Triple Pattern Fragments</td></tr><tr><th>VM</th><td>Version Materialization</td></tr><tr><th>VTPF</th><td>Versioned Triple Pattern Fragments</td></tr><tr><th>VQ</th><td>Version Query</td></tr></table>

    </div>
</section>

  <section id="summary" inlist="" rel="schema:hasPart" resource="#summary">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Summary</h2>

      <p>Over the last 30 years, the Web has significantly enhanced the way we share information,
which has lead to major transformations of our society.
Initially, information on the Web was targeted at humans,
and machines had a difficult time understanding information on the Web
in the same way as humans can.
This hindered <em>intelligent agents</em> in performing certain tasks autonomously,
such as finding all stores that sell a certain product in your current area,
or determining the time to leave for catching your flight on time based on the current traffic and weather conditions.
To enable such intelligent agents, researchers have been investigating technologies and introducing standards
for making the Web understandable for machines.
In the recent years, these technologies are being used to build so-called <em>knowledge graphs</em>,
which are collections of structured information to support intelligent agents such as Siri and Google Assistant.</p>

      <p>Most research on knowledge graphs has focused on <em>static</em> data.
However, there is however a huge amount of <em>evolving</em> data available,
such as traffic events from highway sensors or continuous heart rate measurements.
There is a lot of value in evolving knowledge,
such as for example the ability to determine daily busy traffic periods,
or sending alerts when the heart rate is too high for an unexpectedly long period of time.
As such, it is important to <em>store</em> this information in <em>evolving knowledge graphs</em>,
and to make it <em>searchable</em>.</p>

      <p>Just like the Web, knowledge graphs are continuously becoming more and more <em>centralized</em>,
which means that information becomes increasingly more in the hands of a few large entities.
This leads to information only having limited availability for the public,
which endangers the democratic and <em>decentralized</em> nature of the Web.
Events in recent years have shown that centralizing information at this scale is problematic,
as it leads to issues such as censorship and manipulation of information.
For these reasons, there is an ongoing effort to <em>re-decentralize</em> the Web,
to make the Web a democratic platform again by giving back the power to the people.
As such, an underlying focus within my research is to enable this decentralization and democratization
of information on the Web,
in the form of knowledge graphs.</p>

      <p>To facilitate the usage of <em>evolving knowledge graphs</em>,
<strong>the goal of this PhD is allowing <em>evolving</em> knowledge graphs to be <em>published</em> and <em>queried</em> on the <em>Web</em>.</strong>
To investigate this topic, I focus on four challenges related to this topic.
First, to allow systems that handle evolving knowledge graphs to be evaluated,
I look into the <em>generation of evolving data</em>.
Second, I investigate methods to <em>store evolving data</em>,
so that the data can be published and queried on the Web efficiently.
Third, I design a flexible system to <em>query</em> various kinds of data on the <em>Web</em>.
Finally, I investigate methods for <em>publishing and querying evolving data on the Web</em>.
In the scope of this PhD, I consider slowly evolving knowledge graphs that update with a periodicity in the order of minutes or slower,
because faster periodicities as required for stream processing require significantly different technical requirements.
Below, I will explain the four challenges in more detail.</p>

      <p>In order to properly evaluate systems that handle evolving knowledge graphs,
one must first <em>have</em> evolving knowledge graphs to test these systems with.
As existing evolving knowledge graphs are limited to having only specific sizes,
they are unsuited for the needs of extensive system evaluations,
where configurable evolving knowledge graph sizes are required.
This is why this first challenge focuses on the generation of evolving data, as a prerequisite to the next challenges.
Concretely, I designed an algorithm to generate synthetic public transport network datasets,
based on population distributions as input.
I provide an implementation of this algorithm,
and evaluated it in terms of realism and performance.
Results show that this algorithm is valuable for evaluating systems that handle evolving knowledge graphs,
while still guaranteeing that the datasets are sufficiently realistic with respect to real-world analogues.</p>

      <p>The second challenge focuses on investigating a Web-friendly trade-off between storage size and query efficiency
for evolving knowledge graphs.
For this, I designed a storage approach that can index evolving data,
and I developed accompanying algorithms for querying over this evolving data in an efficient manner.
The index is based on a hybrid between different kinds of storage mechanisms,
to enable efficient lookups for different temporal access patterns.
The query algorithms supports <em>offsets</em> and <em>limits</em>,
to enable random access to subsets of query results,
which is important for Web-friendly query interfaces.
Based on my implementation of this storage approach and querying algorithms,
experimental results show that this system achieves a trade-off between storage size and query efficiency
that is useful for hosting evolving knowledge graphs on the Web.
Concretely, query execution time is reduced at the cost of an increase in storage size.
This cost is acceptable due to storage typically being cheap.</p>

      <p>In the third challenge, the <em>heterogeneous</em> nature of the Web is investigated.
Concretely, I designed a query engine (Comunica) that can query over various kinds of Web interfaces,
based on different kinds of query algorithms.
The engine is designed in a modular way,
so that new interfaces and algorithms can be developed and plugged in flexibly.
This also allows different approaches to be compared fairly,
which makes it a useful research platform.</p>

      <p>Finally, the last challenge ties everything together,
and focuses on publishing evolving data on the Web via a queryable interface.
Concretely, I introduced a query interface for evolving data,
and a client-side algorithm for continuous querying over this interface in a polling-based manner.
This is done by annotating evolving data server-side with predetermined expiration times,
so that clients can determine the optimal polling frequency,
and non-expired data can be reused when other more volatile data expires.
Results show that this approach achieves a lower server load compared to fully server-side continuous query engines,
at the cost of an increase in execution time and bandwidth usage.</p>

      <p>Within these four challenges,
methods are designed to allow evolving knowledge graphs to be stored and queried
in a Web-friendly way.
Concretely, evolving knowledge graphs can be stored in the hybrid storage system from challenge two.
On top of this, a low-cost temporal Web interface can be setup such as the one designed for the fourth challenge,
which can then be queried client-side to reduce server load as seen in challenge three and four.
All of this can then be evaluated using synthetic evolving knowledge graphs
as generated with the algorithm from challenge one.</p>

      <p>While this PhD shows a way to store and query evolving knowledge graphs on the Web,
there does not exist a single perfect way to achieve this,
and different trade-offs exist for different solutions.
For example, storing evolving knowledge graphs over small, slowly evolving IoT sensors
may involve restricted storage capabilities.
On the other hand, highly volatile and sensitive sensors within nuclear reactor infrastructure
may require massive storage capabilities.
In the future, more research will be needed to come up with techniques to store and query
these various kinds of evolving knowledge graphs on the Web.</p>

    </div>
</section>

  <section id="samenvatting" inlist="" rel="schema:hasPart" resource="#samenvatting">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Samenvatting</h2>

      <p>In de afgelopen 30 jaar heeft het Web de manier waarop we informatie delen significant bevorderd,
wat geleid heeft tot grote transformaties van onze samenleving.
Origineel was informatie op het Web bedoeld voor mensen,
en machines hadden het moeilijk om deze informatie te begrijpen op dezelfde manier als mensen.
Dit hinderde <em>intelligente assistenten</em> om bepaalde taken autonoom uit te voeren,
zoals bijvoorbeeld alle winkels vinden die een bepaald product verkopen in jouw huidige omgeving,
of bepalen wanneer het tijd is om te vertrekken om een vlucht te halen gebaseerd op de huidige verkeerssituatie en het weer.
Om deze intelligente assistenten mogelijk te maken hebben onderzoekers gewerkt aan technologieën en standaarden
om het Web begrijpbaar te maken voor machines.
Sinds de afgelopen jaren worden <em>kennisgrafen</em> gebouwd op basis van deze technologieën
om intelligente assistenten zoals Siri en Google Assistant deze taken te kunnen laten uitvoeren.</p>

      <p>Het meeste onderzoek in de context van kennisgrafen is gefocust op <em>statische</em> gegevens.
Er is echter een grote hoeveelheid <em>evoluerende</em> gegevens beschikbaar,
zoals verkeersdata van snelweg sensoren of continue hartslag metingen.
Er zit veel waarde vervat zit in evoluerende kennis
zoals bijvoorbeeld het bepalen van drukke dagelijkse momenten op de snelweg,
of meldingen sturen wanneer de hartslag te hoog blijft voor onverwacht lange perioden.
Daarom is het belangrijk om deze informatie <em>op te slaan</em> in <em>evoluerende kennisgrafen</em>,
en om deze <em>doorzoekbaar</em> te maken.</p>

      <p>Net zoals het Web, worden kennisgrafen meer en meer <em>gecentraliseerd</em>,
wat betekent dat informatie meer en meer in de handen komt van enkele grote entiteiten.
Dit leidt tot een beperkte beschikbaarheid van informatie voor het publiek,
waardoor de democratische en <em>gedecentraliseerde</em> eigenschappen van het Web in gedrang komen.
Gebeurtenissen in de afgelopen jaren hebben aangetoond dat de centralisatie van informatie op deze schaal problematisch is,
aangezien het leidt tot problemen zoals censuur en manipulatie van informatie.
Om deze redenen is er een voortgaande inspanning om het Web <em>opnieuw te decentraliseren</em>,
en om het Web opnieuw een democratisch platform te maken door de macht terug te geven aan de mensen.
Aldus is decentralisatie en democratisatie van informatie op het Web in de vorm van kennisgrafen een onderliggende focus van mijn onderzoek.</p>

      <p>Om het gebruik van <em>evoluerende kennisgrafen</em> te vergemakkelijken,
is <strong>het doel van dit doctoraat om het mogelijk te maken om <em>evoluerende</em> kennisgrafen te <em>publiceren</em> en <em>bevragen</em> op het Web</strong>.
Om dit onderwerp te onderzoeken focus ik op vier uitdagingen gerelateerd aan dit onderwerp.
Te eerste, om systemen die evoluerende kennisgrafen beheren te evalueren,
kijk ik naar de <em>generatie van evoluerende gegevens</em>.
Ten tweede onderzoek ik manieren om <em>evoluerende gegevens op te slaan</em>,
zodat gegevens efficiënt op het Web gepubliceerd en bevraagd kunnen worden.
Ten derde ontwerp ik een flexibel systeem om verschillende soorten gegevens te bevragen op het <em>Web</em>.
Tot slot onderzoek ik manieren om <em>evoluerende gegevens te publiceren en bevragen op het Web</em>.
In de context van dit doctoraat ga ik uit van traag evoluerende kennisgrafen die veranderen met een periodiciteit in de orde van minuten of trager,
omdat snellere periodiciteiten zoals relevant binnen stroom verwerking beduidend andere technische vereisten nodig hebben.
Hierna zal ik de vier uitdagingen in meer detail uitleggen.</p>

      <p>Om op een degelijke manier systemen te evalueren die evoluerende kennisgrafen beheren,
is het nodig om eerst evoluerende kennisgrafen te <em>hebben</em> om deze systemen mee te testen.
Aangezien bestaande evoluerende kennisgrafen beperkt zijn tot specifieke groottes,
zijn deze niet geschikt voor de noden van uitgebreide systeemevaluaties
waar configureerbare groottes van evoluerende kennisgrafen nodig zijn.
Dit is waarom de eerste uitdaging focust op de generatie van evoluerende gegevens, als een vereiste voor de volgende uitdagingen.
Concreet ontwerp ik een algoritme om synthetische datasets over het openbaar vervoer te genereren,
gebaseerd op populatie distributies als invoer.
Dit algoritme is geïmplementeerd en geëvalueerd in termen van realiteit en prestatie.
Resultaten tonen aan dat dit algoritme nuttig is voor de evaluatie van systemen die evoluerende kennisgrafen beheren,
met de garantie dat datasets voldoende realistisch zijn ten opzichte van analogen in de echte wereld.</p>

      <p>De tweede uitdaging focust op het onderzoek van een Web-vriendelijke afweging tussen opslagruimte en opzoek efficiëntie
voor evoluerende kennisgrafen.
Hiervoor ontwerpte ik een opslagtechniek die in staat is om evoluerende gegevens te indexeren,
en samenhorige algoritmes werden ontwikkeld voor het doorzoeken van evoluerende gegevens op een efficiënte manier.
Deze index is gebaseerd op een hybride van verschillende soorten opslagtechnieken,
om verschillende temporele toegangspatronen efficiënt te maken.
De zoekalgoritmen ondersteunen <em>startafstanden</em> en <em>limieten</em>,
om willekeurige toegang tot deelverzamelingen van zoekresultaten mogelijk te maken,
wat belangrijk is voor een Web-vriendelijke zoek toegang.
Gebaseerd op een implementatie van deze opslagtechniek en zoekalgoritmen,
tonen experimentele resultaten aan dat dit systeem er in slaagt om een afweging te bereiken tussen opslagruimte en opzoek efficiëntie
die waardevol is voor het plaatsen van evoluerende kennisgrafen op het Web.
Concreet worden zoektijden gereduceerd ten koste van een toename in opslagruimte.
Deze kost is acceptabel aangezien opslag meestal vrij goedkoop is.</p>

      <p>In de derde uitdaging wordt de <em>heterogeniteit</em> van het Web onderzocht.
Concreet ontwerpte ik zoekmachine (Comunica) die in staat is om te zoeken over verschillende soorten Web toegangen,
gebaseerd op verschillende zoekalgoritmen.
De zoekmachine is ontworpen op een modulaire manier,
zodat nieuwe soorten Web toegangen en algoritmen ontwikkeld en ingeplugd kunnen worden op een flexibelen manier.
Dit maakt het mogelijk om verschillende Web toegangen en algoritmen op een eerlijke manier met elkaar te vergelijken,
wat dit een nuttig onderzoeksplatform maakt.</p>

      <p>Tot slot verbindt de laatste uitdaging alle voorgaande uitdagingen met elkaar,
en focust op de publicatie van evoluerende gegevens op het Web via een doorzoekbare toegang.
Concreet introduceerde ik een doorzoekbare toegang voor evoluerende gegevens,
en een algoritme aan de client-zijde voor de continue bevraging over deze toegang op een herhalende manier.
Dit wordt gedaan door evoluerende gegevens te annoteren met bepaalde vervaltijden,
zo dat clienten de optimale opzoekfrequentie kunnen bepalen,
en dat niet-vervallen gegevens kunnen hergebruikt worden wanneer vluchtigere gegevens vervallen.
Resultaten tonen aan dat deze manier er in slaagt om een lagere server belasting te bereiken in vergelijking met een continue zoekmachine die volledig aan de server zijde draait,
ten koste van een toename in uitvoeringstijd en bandbreedte.</p>

      <p>In deze vier uitdagingen werden technieken ontwikkeld om evoluerende kennisgrafen op te slaan en te bevragen
op een Web-vriendelijke manier.
Concreet kan dit gedaan worden door evoluerende kennisgrafen op te slaan in een hybride systeem van de tweede uitdaging.
Hier bovenop kan een Web toegang opgezet worden zoals deze ontworpen in de vierde uitdaging,
welke bevraagd kan worden van de client-side om server belasting te verlagen zoals gedaan wordt in uitdaging drie en vier.
Deze kunnen allemaal geëvalueerd worden met behulp van synthetische evoluerende kennisgrafen
die gegenereerd kunnen worden met het algoritme van de eerste uitdaging.</p>

      <p>Alhoewel dit onderzoek een manier aantoont om evoluerende kennisgrafen op te slaan en te bevragen op het Web,
bestaat er geen één perfecte manier om dit te doen,
maar er bestaan wel verschillende afwegingen voor verschillende toepassingen.
Bijvoorbeeld, evoluerende kennisgrafen opslaan over kleine, traag evoluerende sensoren in het <em>internet der dingen</em>,
kunnen beperkt zijn in opslagruimte.
Aan de andere kant kunnen zeer vluchtige en gevoelige sensoren in nucleare reactoren
zeer grote opslagruimte vereisen.
In de toekomst zal meer onderzoek nodig zijn om technieken te onderzoeken
om het mogelijk maken om verschillende soorten evoluerende kennisgrafen op te slaan en te bevragen op het Web.</p>

    </div>
</section>

</section>

<main>
  <section id="introduction" inlist="" rel="schema:hasPart" resource="#introduction">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Introduction</h2>

      <h3 id="introduction-the-web">The Web</h3>

      <h4 id="catalysts-for-human-progress">Catalysts for Human Progress</h4>

      <p>Since the dawn of mankind, biological evolution has shaped us into social creatures.
The social capabilities of humans are however <em>much more evolved</em> than most other other species.
For example, <a href="https://pursuit.unimelb.edu.au/articles/why-we-show-the-whites-of-our-eyes">humans are one of the only animals that have clearly visible eye whites</a>.
This allows people to see what other people are looking at,
which simplifies <em>collaborative</em> tasks.
Furthermore, <a href="https://www.sciencedirect.com/topics/neuroscience/theory-of-mind"><em>theory of mind</em></a>, the ability to understand that others have different perspectives, <a href="https://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262016056.001.0001/upso-9780262016056">is much more pronounced in humans than other animals</a>, which also strengthens our ability to <em>collaborate</em>.
While our collaborative capabilities were initially limited to physical tasks,
the invention of <em>language</em> and <em>writing</em> allowed us to share <em>knowledge</em> with each other.</p>

      <p>Methods for sharing knowledge are essential catalysts for human progress,
as shared knowledge allows larger groups of people to share goals
and accomplish tasks that would have otherwise been impossible.
Due to our <em>technological</em> progress,
the <em>bandwidth</em> of these methods for sharing knowledge is always growing broader,
which is continuously increasing the rate of human and technological progress.</p>

      <p>Throughout the last centuries, we saw three major revolutions in bandwidth.
First, the invention of the printing press in the 15th century
drastically increased rate at which books could be duplicated.
Secondly, there was the invention of radio and television in the 20th century.
As audio and video are cognitively less demanding than reading,
this lowered the barrier for spreading knowledge even further.
Third, we had the development of the internet near the end of the 20th century,
and the invention of the World Wide Web in 1989 on top of that,
which gave us a globally interlinked information space.
Like the inventions before, the Web is fully <em>open</em> and <em>decentralized</em>,
where anyone can say anything about anything.
With the Web, bandwidth for knowledge sharing has become nearly unlimited,
as knowledge no longer has to go through a few large radio or tv stations,
but can now be shared over a virtually unlimited amount of Web pages,
which leads to a more <em>social</em> human species.</p>

      <h4 id="impact-of-the-web">Impact of the Web</h4>

      <p>At the time of writing, the Web is 30 years old.
Considering <a href="http://humanorigins.si.edu/evidence/human-fossils/species/homo-sapiens">our species is believed to be 300,000 years old</a>,
this is just 0.01% of the time we have been around.
To put this in perspective in terms of a human life,
the Web would only be a baby of just under 3 days old,
assuming <a href="https://countrymeters.info/en/Belgium">a life expectancy of 80 years</a>.
This means that the Web <em>just</em> got started,
and it will take a long time for it to mature and to achieve its full potential.</p>

      <p>Even in this short amount of time,
the Web has already transformed our world in an unprecedented way.
Most importantly, it has given more than <a href="https://internetworldstats.com/stats.htm">56% of the global population</a>
access to most of all human knowledge behind a finger’s touch.
Secondly, <em>social media</em> has enabled people to communicate with anyone on the planet near-instantly,
and even with multiple people at the same time.
Furthermore, it has <a href="https://ieeexplore.ieee.org/document/8267982">impacted politics</a>
and even <a href="https://www.mic.com/articles/10642/twitter-revolution-how-the-arab-spring-was-helped-by-social-media">caused oppressive regimes to be overthrown</a>.
Next to that, it is also significantly <a href="https://stratechery.com/2015/airbnb-and-the-internet-revolution/">disrupting businesses models that have been around since the industrial revolution, and creating new ones</a>.</p>

      <h4 id="knowledge-graphs">Knowledge Graphs</h4>

      <p>The Web has made a positive significant impact on the world.
Yet, the goal of curiosity-driven researchers is to uncover
what the next steps are to improve the world <em>even more</em>.</p>

      <p>In 2001, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www-sop.inria.fr/acacia/cours/essi2006/Scientific%20American_%20Feature%20Article_%20The%20Semantic%20Web_%20May%202001.pdf">Tim Berners-Lee shared his dream</a> <span class="references">[<a href="#ref-1">1</a>]</span> where machines
would be able to help out with our day-to-day tasks
by analyzing data on the Web and acting as <em>intelligent agents</em>.
Back then, the primary goal of the Web was to be <em>human-readable</em>.
In order for this dream to become a reality,
the Web had to become <em>machine-readable</em>.
This Web extension is typically referred to as the <em>Semantic Web</em>.</p>

      <p>Now, almost twenty years later, several standards and technologies have been developed to make this dream a reality,
<a href="http://iswc2013.semanticweb.org/content/keynote-ramanathan-v-guha.html">In 2013, more than four million Web domains were already using these technologies</a>.
Using these Semantic Web technologies, so-called <em>knowledge graphs</em> are being constructed by many major companies world-wide,
such as <a href="https://developers.google.com/knowledge-graph/">Google</a> and <a href="https://developer.microsoft.com/en-us/graph/">Microsoft</a>.
A knowledge graph is a collection of structured information that is organized in a graph.
These knowledge graphs are being used to support tasks that were part of Tim Berners-Lee’s original vision,
such as managing day-to-day tasks with the <a href="https://www.google.com/intl/nl/landing/now/">Google Now assistant</a>.</p>

      <p>The standard for modeling knowledge graphs is the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">Resource Description Framework (RDF)</a> <span class="references">[<a href="#ref-2">2</a>]</span>.
Fundamentally, it is based around the concept of <em>triples</em> that are used to make statements about <em>things</em>.
A triple is made up of a <em>subject</em>, <em>predicate</em> and <em>object</em>,
where the <em>subject</em> and <em>object</em> are resources (or <em>things</em>), and the <em>predicate</em> denotes their relationship.
For example, <a href="#introduction-figure-triple">Fig. 1</a> shows a simple triple indicating the nationality of a person.
Multiple resources can combined with each other through multiple triples, which forms a <em>graph</em>.
<a href="#introduction-figure-graph">Fig. 2</a> shows an example of such a graph, which contains <em>knowledge</em> about a person.
In order to look up information within such graphs, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>
was introduced as a standard.
Essentially, <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> allows <abbr title='Resource Description Framework'>RDF</abbr> data to be looked up through combinations of <em>triple patterns</em>,
which are triples where any of its elements can be replaced with <em>variables</em> such as <code>?name</code>.
For example, <a href="#introduction-code-sparql">Listing 1</a> contains a <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query that find the names of all people that Alice knows.</p>

      <figure id="introduction-figure-triple">
<img src="img/triple.svg" alt="A triple" />
<figcaption>
          <p><span class="label">Fig. 1:</span> A triple indicating that Alice knows Bob.</p>
        </figcaption>
</figure>

      <figure id="introduction-figure-graph">
<img src="img/graph.svg" alt="A knowledge graph" />
<figcaption>
          <p><span class="label">Fig. 2:</span> A small knowledge graph about Alice.</p>
        </figcaption>
</figure>

      <figure id="introduction-code-sparql" class="listing">
<pre><code>SELECT ?name WHERE {
</code><code>  Alice knows ?person.
</code><code>  ?person name ?name.
</code><code>}</code></pre>
<figcaption>
          <p><span class="label">Listing 1:</span> A simplified <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query selecting the names of all people that Alice knows.
The results of this query would be <code>"Bob"</code> and <code>"Carol"</code>.</p>
        </figcaption>
</figure>

      <h4 id="evolving-knowledge-graphs">Evolving Knowledge Graphs</h4>

      <p>Within <em>Big Data</em>, we talk about the three V’s: <em>volume</em>, <em>velocity</em>, and <em>variety</em>.
As the Web meets these three requirements, it can be seen as a global <em>Big Data</em>set.
Specifically, the Web is highly <em>volatile</em>,
as it is continuously evolving,
and it does so at an increasing rate.
For example, <a href="https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#6907ae2460ba">Google is processing more than <em>40,000 search requests</em> <em>every second</em></a>,
<a href="https://expandedramblings.com/index.php/youtube-statistics/"><em>500 hours of video</em> are being uploaded to YouTube <em>every minute</em></a>,
and <a href="https://blog.hootsuite.com/twitter-statistics/">more than <em>5,000</em> tweets are being sent <em>every second</em></a>.</p>

      <p>A lot of research and engineering work is needed to make it possible to handle this evolving data.
For instance, it should be possible to <em>store</em> all of this data as fast as possible,
and to make it <em>searchable</em> for <em>knowledge</em> as soon as possible.
This is important, as there is a lot of value in evolving knowledge.
For example, by tracking the evolution of biomedical information, the spread of diseases can be reduced,
and by observing highway sensors, traffic jams may be avoided by preemptively rerouting traffic.</p>

      <p>Due to the <a href="https://www.w3.org/TR/rdf11-concepts/#change-over-time">(RDF) knowledge graph model currently being <em>atemporal</em></a>,
the usage of evolving knowledge graphs remains limited.
As such, research and engineering effort is needed for new
models, storage techniques, and query algorithms
for evolving knowledge graphs.
As such, <em>evolving</em> knowledge graphs are the main focus of my research.</p>

      <h4 id="decentralized-knowledge-graphs">Decentralized Knowledge Graphs</h4>

      <p>As stated by Tim Berners-Lee, <a href="https://twitter.com/timberners_lee/status/228960085672599552">the Web is for everyone</a>.
This means that the Web is a <em>free</em> platform (as in <em>freedom</em>, not <em>free beer</em>),
where anyone can <em>say</em> anything about anything,
and anyone can <em>access</em> anything that has been said.
This is directly compatible with Article 19 of <a href="https://www.un.org/en/universal-declaration-human-rights/">the Universal Declaration of Human Rights</a>,
which says the following:</p>

      <blockquote>
        <p>Everyone has the right to freedom of opinion and expression;
this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas
through any media and regardless of frontiers.</p>
      </blockquote>

      <p>The original Web standards and technologies have been designed with this fundamental right in mind.
However, over the recent years, the Web has been growing towards more <em>centralized</em> entities,
where this right is being challenged.</p>

      <p>The current <em>centralized</em> knowledge graphs do not match well with the original <em>decentralized</em> nature of the Web.
At the time of writing, these new knowledge graphs are in the hands of a few large corporations,
and intelligent agents on top of them are restricted to what these corporations allow them to do.
As people depend on the capabilities of these knowledge graphs,
large corporations gain significant control over the Web.
In the last couple of years, these centralized powers have proven to be problematic,
for example when <a href="https://fs.blog/2017/07/filter-bubbles/">the flow of information is redirected to influence election results</a>,
when <a href="https://www.theguardian.com/technology/live/2018/apr/10/mark-zuckerberg-testimony-live-congress-facebook-cambridge-analytica">personal information is being misused</a>,
or when <a href="https://quillette.com/2019/06/06/against-big-tech-viewpoint-discrimination/">information is being censored due to idealogical differences</a>.
This shows that our freedom of expression is being challenged by these large centralized entities,
as there is clear interference of opinions through redirection of the information flow,
and obstruction to receive information through censorship.</p>

      <p>For these reasons, there is a massive push for <a href="https://ruben.verborgh.org/articles/redecentralizing-the-web/"><em>re-decentralizing the Web</em></a>,
where people regain <em>ownership</em> of their data.
Decentralization is however a technologically difficult thing,
as applications typically require a single <em>centralized</em> entrypoint from which data is retrieved,
and no such single entrypoint exist in a truly decentralized environment.
As people do want ownership of their data, they do not want to give up their intelligent agents.
As such, this decentralization wave requires significant research effort to achieve the same capabilities as these <em>centralized</em> knowledge graphs,
which is why this is an important factor within my research.
Specifically, I focus on supporting knowledge graphs <em>on the Web</em>,
instead of only being available behind closed doors,
so that they are available for everyone.</p>

      <h3 id="introduction-research-question">Research Question</h3>

      <p>The goal of my research is to allow people to <em>publish</em> and <em>find</em> knowledge
without having to depend on large centralized entities,
with a focus on knowledge that <em>evolves</em> over time.
This lead me to the following research question for my PhD:</p>

      <blockquote id="research-question" class="strong">
        <p>How to store and query evolving knowledge graphs on the Web?</p>
      </blockquote>

      <p>During my research, I focus on <em>four</em> main challenges
related to this research question:</p>

      <ol>
        <li><strong>Experimentation requires <em>representative</em> evolving data.</strong>
 <br />
 In order to <em>evaluate</em> the performance of systems that handle <em>evolving</em> knowledge graphs,
 a flexible method for <em>obtaining</em> such data needs to be available.</li>
        <li><strong>Indexing evolving data involves a <em>trade-off</em> between <em>storage efficiency</em> and <em>lookup efficiency</em>.</strong>
 <br />
 Indexing techniques are used to improve the efficiency of querying,
 but comes at the cost of increased storage space and preprocessing time.
 As such, it is important to find a good <em>balance</em> between the amount of <em>storage</em> space with its indexing time,
 and the amount of <em>querying speedup</em>,
 so that evolving data can be stored in a Web-friendly way.</li>
        <li><strong>Web interfaces are highly <em>heterogeneous</em>.</strong>
 <br />
 Before knowledge graphs can be queried from the Web,
 different <em>interfaces</em> through which data is available,
 and different <em>algorithms</em> with which the data can be retrieved
 need to be combinable.</li>
        <li><strong>Publishing <em>evolving</em> data via a <em>queryable interface</em> involves <em>continuous</em> updates to clients.</strong>
 <br />
 Centralized querying interfaces are hard to scale for an increasing number of concurrent clients,
 especially when the knowledge graphs that are being queried over are continuously evolving,
 and clients need to be notified of data updates continuously.
 New kinds of interfaces and querying algorithms are needed to cope with this scalability issue.</li>
      </ol>

      <h3 id="introduction-outline">Outline</h3>

      <p>Corresponding to my four research challenges,
this thesis bundles the following four peer-reviewed publications as separate chapters,
for which I am the lead author:</p>

      <ul>
        <li>Ruben Taelman et al. <a href="https://www.rubensworks.net/raw/publications/2018/podigg.pdf">Generating Public Transport Data based on Population Distributions for <abbr title='Resource Description Framework'>RDF</abbr> Benchmarking</a>.
  <br />In: <em>In Semantic Web Journal</em>. IOS Press, 2019.</li>
        <li>Ruben Taelman et al. <a href="https://rdfostrich.github.io/article-jws2018-ostrich/">Triple Storage for Random-Access Versioned Querying of <abbr title='Resource Description Framework'>RDF</abbr> Archives</a>.
  <br />In: <em>Journal of Web Semantics</em>. Elsevier, 2019.</li>
        <li>Ruben Taelman et al. <a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica: a Modular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Engine for the Web</a>.
  <br />In: <em>International Semantic Web Conference</em>. Springer, October 2018.</li>
        <li>Ruben Taelman et al. <a href="https://www.rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf">Continuous Client-side Query Evaluation over Dynamic Linked Data</a>.
  <br />In: <em>The Semantic Web: ESWC 2016 Satellite Events, Revised Selected Papers</em>. Springer, May 2016.</li>
      </ul>

      <p>In <a href="#generating">Chapter 2</a> a mimicking algorithm (<em>PoDiGG</em>) is introduced for generating <em>realistic</em> evolving public transport data,
so that it can be used to benchmark systems that work with evolving data.
This algorithm is based on established concepts for designing public transport networks,
and takes into account population distributions for simulating the flow of vehicles.
Next, in <a href="#storing">Chapter 3</a>, a storage architecture and querying algorithms are introduced
for managing evolving data.
It has been implemented as a system called <em>OSTRICH</em>,
and extensive experimentation shows that this systems introduces a useful trade-off between storage size and querying efficiency
for publishing evolving knowledge graphs on the Web.
In <a href="#querying">Chapter 4</a>, A modular query engine called <em>Comunica</em> is introduced that is able to cope with the heterogeneity of data on the Web.
This engine has been designed to be highly flexible, so that it simplifies research within the query domain,
where new query algorithms can for example be developed in a separate module, and plugged into the engine without much effort.
In <a href="#querying-evolving">Chapter 5</a>, a low-cost publishing interface and accompanying querying algorithm (<em>TPF Query Streamer</em>) is introduced and evaluated
to enable continous querying of <em>evolving</em> data with a low volatility.
Finally, this work is concluded in <a href="#conclusions">Chapter 6</a> and future research opportunities are discussed.</p>

    </div>
</section>

  
  <section class="sub-paper">
    <h2 id="generating">Generating Synthetic Evolving Data</h2>

    <section class="sub-preface">
<div datatype="rdf:HTML" property="schema:description">
        <p>In this chapter, we address the first challenge of this PhD,
namely: “Experimentation requires <em>realistic</em> evolving data”.
This challenge is a prerequisite to the next challenges, in which storage and querying techniques are introduced for evolving data.
In order to evaluate the performance of storage and querying systems that handle evolving knowledge graphs,
we must first have such knowledge graphs available to us.
Ideally, real-world knowledge graphs should be used,
as these can show the true performance of such systems in various circumstances.
However, these real-world knowledge graphs have limited public availability,
and do not allow for the required flexibility when evaluating systems.
For example, the evaluation of storage systems can require the ingestion of evolving knowledge graphs of varying sizes,
but real-world datasets only exist in fixed sizes.</p>

        <p>To solve this problem, we focus on the generation of evolving knowledge graphs
assuming that we have population distributions as input.
For this, we started from the research question:
“Can population distribution data be used to generate realistic synthetic public transport networks and scheduling?”
Concretely, we introduce a mimicking algorithm for generating <em>realistic</em> synthetic evolving knowledge graphs
with configurable sizes and properties.
The algorithm is based on established concepts from the domain of public transport networks design,
and takes population distributions as input to generate realistic transport networks.
The algorithm has been implemented in a system called <em>PoDiGG</em>,
and has been evaluated to measure its performance and level of realism.</p>

      </div>
</section>

    <p class="published-as">Ruben Taelman, Pieter Colpaert, Erik Mannens, and Ruben Verborgh. 2019. <strong><a href="https://www.rubensworks.net/raw/publications/2018/podigg.pdf">Generating Public Transport Data based on Population Distributions for <abbr title='Resource Description Framework'>RDF</abbr> Benchmarking</a></strong>. Semantic Web Journal 10, 2 (January 2019), 305–328.</p>

    <section id="generating_abstract" inlist="" rel="schema:hasPart" resource="#generating_abstract">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name" class="no-label-increment">Abstract</h3>

        <!--context-->
        <p>When benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems such as public transport route planners,
system evaluation needs to happen under various realistic circumstances,
which requires a wide range of datasets with different properties.
Real-world datasets are almost ideal, as they offer these realistic circumstances,
but they are often hard to obtain and inflexible for testing.
For these reasons, synthetic dataset generators are typically preferred
over real-world datasets due to their intrinsic flexibility.
Unfortunately, many synthetic dataset that are generated within benchmarks are insufficiently realistic,
raising questions about the generalizability of benchmark results to real-world scenarios.
<!--need-->
In order to benchmark geospatial and temporal <abbr title='Resource Description Framework'>RDF</abbr> data management systems
such as route planners
with sufficient external validity and depth,
<!--task-->
we designed <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>,
a highly configurable generation algorithm for synthetic public transport datasets
with realistic geospatial and temporal characteristics
comparable to those of their real-world variants.
The algorithm is inspired by real-world public transit network design
and scheduling methodologies.
<!--object-->
This article discusses the design and implementation of <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>
and validates the properties of its generated datasets.
<!--findings-->
Our findings show that the generator achieves a sufficient level of realism,
based on the existing coherence metric and new metrics we introduce specifically for the public transport domain.
<!--conclusions-->
Thereby, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>
provides a flexible foundation for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems with geospatial and temporal data.</p>

      </div>
</section>

    <section id="generating_introduction" inlist="" rel="schema:hasPart" resource="#generating_introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Introduction</h3>

        <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">Resource Description Framework (RDF)</a> <span class="references">[<a href="#ref-2">2</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a> <span class="references">[<a href="#ref-4">4</a>]</span> technologies enable distributed use and management of semantic data models.
Datasets with an interoperable domain model can be stored and queried by different data owners in different ways.
In order to discover the strengths and weaknesses of different storage and querying possibilities,
data-driven benchmarks with different sizes of datasets and varying characteristics can be used.</p>

        <p>Regardless of whether existing data-driven benchmarks use real or synthetic datasets,
the <em>external validity</em> of their results can be too limited,
which makes a generalization to other datasets difficult.
Real datasets, on the one hand, are often only scarcely available for testing,
and only cover very specific scenarios,
such that not all aspects of systems can be assessed.
Synthetic datasets, on the other hand, are typically generated by
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-SPARQL-Benchmark-IJSWIS.pdf"><em>mimicking algorithms</em></a> <span class="references">[<a href="#ref-5">5</a>, <a href="#ref-6">6</a>, <a href="#ref-7">7</a>, <a href="#ref-8">8</a>]</span>,
which are <a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">not always sufficiently realistic</a> <span class="references">[<a href="#ref-9">9</a>]</span>.
Features that are relevant for real-world datasets may not be tested.
As such, conclusions drawn from existing benchmarks
do not always apply to the envisioned real-world scenarios.
One way to get the best of both worlds
is to design mimicking algorithms that generate realistic synthetic datasets.</p>

        <p>The <em>public transport</em> domain provides data with both geospatial and temporal properties,
which makes this an especially interesting source of data for benchmarking.
Its representation as Linked Data is valuable because
1) of the many shared entities, such as stops, routes and trips, across different existing datasets on the Web.
2) These entities can be distributed over different datasets
and 3) benefit from interlinking for the improvement of discoverability.
Synthetic public transport datasets are particularly important and needed
in cases where public transport route planning algorithms are evaluated.
The <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1486/paper_28.pdf">Linked Connections framework</a> <span class="references">[<a href="#ref-10">10</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://i11www.iti.kit.edu/extra/publications/dpsw-isftr-13.pdf">Connection Scan Algorithm</a> <span class="references">[<a href="#ref-11">11</a>]</span>
are examples of such public transport route planning systems.
Because of the limited availability of real-world datasets with desired properties,
these systems were evaluated with only a very low number of datasets, respectively one and three datasets.
A synthetic public transport dataset generator would make it easier for researchers
to include a higher number of realistic datasets with various properties in their evaluations,
which would be beneficial to the discovery of new insights from the evaluations.
Network size, network sparsity and temporal range are examples of such properties,
and different combinations of them may not always be available in real datasets,
which motivates the need for generating synthetic, but realistic datasets with these properties.</p>

        <p>Not only are public transport datasets useful for benchmarking route planning systems,
they are also highly useful for benchmarking <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf">geospatial</a> <span class="references">[<a href="#ref-12">12</a>, <a href="#ref-13">13</a>]</span> and <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1860702.1860705"><a href="http://doi.acm.org/10.1145/1860702.1860705">temporal</a></span> <span class="references">[<a href="#ref-14">14</a>, <a href="#ref-15">15</a>]</span> <abbr title='Resource Description Framework'>RDF</abbr> systems
due to the intrinsic geospatial and temporal properties of public transport datasets.
While synthetic dataset generators already exist in the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://dx.doi.org/10.1007/978-3-642-41338-4_22">geospatial and temporal domain</a> <span class="references">[<a href="#ref-16">16</a>, <a href="#ref-17">17</a>]</span>,
no systems exist yet that focus on realism, and specifically look into the generation of public transport datasets.
As such, the main topic that we address in this work, is solving the need for realistic public transport datasets
with geospatial and temporal characteristics,
so that they can be used to benchmark <abbr title='Resource Description Framework'>RDF</abbr> data management and route planning systems.
More specifically, we introduce a mimicking algorithm for generating realistic public transport data,
which is the main contribution of this work.</p>

        <p>We observed a significant correlation between transport networks and the population distributions of their geographical areas,
which is why population distributions are the driving factor within our algorithm.
The cause of this correlation is obvious, considering transport networks are frequently used to transport people,
but other – possibly independent – factors exist that influence transport networks as well,
like certain points of interest such as tourist attractions and shopping areas.
Our algorithm is subdivided into five sequential steps,
inspired by existing methodologies from the domains of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">public transit planning</a> <span class="references">[<a href="#ref-18">18</a>]</span>
as a means to improve the realism of the algorithm’s output data.
These steps include the creation of a geospatial region, the placement of stops, edges and routes, and the scheduling of trips.
We provide an implementation of this algorithm, with different parameters to configure the algorithm.
Finally, we confirm the realism of datasets that are generated by this algorithm
using the existing <a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">generic structuredness metric</a> <span class="references">[<a href="#ref-9">9</a>]</span>
and new metrics that we introduce, which are specific to the public transport domain.
The notable difference of this work compared to other synthetic dataset generators
is that our generation algorithm specializes in generating public transit networks,
while other generators either focus on other domains, or aim to be more general-purpose.
Furthermore, our algorithm is based on population distributions and existing methodologies from public transit network design.</p>

        <p>In the next section, we introduce the related work on dataset generation,
followed by the background on public transit network design, and transit feed formats in <a href="#generating_public-transit-background">Section 2.3</a>.
In <a href="#generating_research-question">Section 2.4</a>, we introduce the main research question and hypothesis of this work.
Next, our algorithm is presented in <a href="#generating_methodology">Section 2.5</a>, followed by its implementation in <a href="#generating_implementation">Section 2.6</a>.
In <a href="#generating_evaluation">Section 2.7</a>, we present the evaluation of our implementation,
followed by a discussion and conclusion in <a href="#generating_discussion">Section 2.8</a> and <a href="#generating_conclusions">Section 2.9</a>.</p>
      </div>
</section>

    <section id="generating_related-work" inlist="" rel="schema:hasPart" resource="#generating_related-work">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Related Work</h3>

        <p>In this section, we present the related work on spatiotemporal and <abbr title='Resource Description Framework'>RDF</abbr> dataset generation,</p>

        <p>Spatiotemporal database systems store instances that are described using an identifier, a spatial location and a timestamp.
In order to evaluate spatiotemporal indexing and querying techniques with datasets,
automatic means exist to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/cc2e/d20e54dfbc86dddcc77e2a6e37ec16e4be9c.pdf">generate such datasets with predictable characteristics</a> <span class="references">[<a href="#ref-19">19</a>]</span>.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">Brinkhoff</a> <span class="references">[<a href="#ref-20">20</a>]</span> argues that moving objects tend to follow a predefined network.
Using this and other statements, he introduces a spatiotemporal dataset generator.
Such a network can be anything over which certain objects can move,
ranging from railway networks to air traffic connections.
The proposed parameter-based generator restricts the existence of the spatiotemporal objects to
a predefined time period <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><msub><mi>t</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>t</mi><mtext>max</mtext></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\lbrack t_\text{min},t_\text{max})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.
It is assumed that each edge in the network has a maximum allowed speed and capacity
over which objects can move at a certain speed.
The eventual speed of each object is defined by the maximum speed of its class,
the maximum allowed speed of the edge, and the congestion of the edge based on its capacity.
Furthermore, external events that can impact the movement of the objects, such as weather conditions,
are represented as temporal grids over the network, which apply a <em>decreasing factor</em> on the maximum speed of the objects in certain areas.
The existence of each object that is generated starts at a certain timestamp,
which is determined by a certain function,
and <em>dies</em> when it arrives at its destination.
The starting node of an object can be chosen based on three approaches:</p>

        <ul>
          <li><strong>dataspace-oriented approaches</strong>: Selecting the nearest node to a position picked from a two-dimensional distribution function that maps positions to nodes.</li>
          <li><strong>region-based approaches</strong>: Improvement of the data-space oriented approach where the data space is represented as a collection of cells, each having a certain chance of being the place of a starting node.</li>
          <li><strong>network-based approaches</strong>: Selection of a network node based on a one-dimensional distribution function that assigns a chance to each node.</li>
        </ul>

        <p>Determining the destination node using one of these approaches leads to non-satisfying results.
Instead, the destination is derived from the preferred length of a route.
Each route is determined as the fastest path to a destination, weighed by the external events.
Finally, the results are reported as either textual output, insertion into a database or a figure of the generated objects.
Compared to our work, this approach assumes a predefined network,
while our algorithm also includes the generation of the network.
For our work, we reuse the concepts of object speed and region-based node selection with relation to population distributions.</p>

        <p>In order to improve the testability of Information Discovery Systems,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">generic synthetic dataset generator</a> <span class="references">[<a href="#ref-21">21</a>]</span> was developed
that is able to generate synthetic data based on declarative graph definitions.
This graph is based on objects, attributes and relationships between them.
The authors propose to generate new instances, such as people, based on a set of dependency rules.
They introduce three types of dependencies for the generation of instances:</p>

        <ul>
          <li><strong>independent</strong>: Attribute values that are independent of other instances and attributes.</li>
          <li><strong>intra-record (horizontal) dependencies</strong>: Attribute values depending on other values of the same instance.</li>
          <li><strong>inter-record (vertical) dependencies</strong>: Relationships between different instances.</li>
        </ul>

        <p>Their engine is able to accept such dependencies as part of a semantic graph definition,
and iteratively create new instances to form a synthetic dataset.
This tool however outputs non-RDF <abbr title='Comma Separated Values'>CSV</abbr> files, which makes it impossible to directly use this system for
the generation of public transport datasets in <abbr title='Resource Description Framework'>RDF</abbr> using existing ontologies.
For our public transport use case, individual entities such as stops, stations and connections
would be possible to generate up to a certain level using this declarative tool.
However, due to the underlying relation to population distributions
and specific restrictions for resembling real datasets,
declarative definitions are too limited.</p>

        <p>The need for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems is illustrated by the existence of the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.sti-innsbruck.at/sites/default/files/fileadmin/documents/articles/p27-angles.pdf">Linked Data Benchmark Council</a> <span class="references">[<a href="#ref-22">22</a>]</span>
and the <a href="http://project-hobbit.eu/" class="mandatory" data-link-text="http:/​/​project-​hobbit.eu/​">HOBBIT H2020 EU project</a> for benchmarking of Big Linked Data.
<abbr title='Resource Description Framework'>RDF</abbr> benchmarks are typically based on certain datasets that are used as input to the tested systems.
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">Many of these datasets are not always very closely related to real datasets</a> <span class="references">[<a href="#ref-9">9</a>]</span>,
which may result in conclusions drawn from benchmarking results that do not translate to system behaviours in realistic settings.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">Duan et al.</a> <span class="references">[<a href="#ref-9">9</a>]</span> argue that the realism of an <abbr title='Resource Description Framework'>RDF</abbr> dataset can be measured
by comparing the <em>structuredness</em> of that dataset with a realistic equivalent.
The authors show that real-world datasets are typically less structured than their synthetic counterparts,
which can results in significantly different benchmarking results,
since this level of structuredness can have an impact on how certain data is stored in <abbr title='Resource Description Framework'>RDF</abbr> data management systems.
This is because these systems may behave differently on datasets with different levels of structuredness,
as they can have certain optimizations for some cases.
In order to measure this structuredness, the authors introduce the <em>coherence</em>
metric of a dataset <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span> with a type system <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">T</mi></mrow><annotation encoding="application/x-tex">\mathcal{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.25417em;">T</span></span></span></span></span> that can be calculated as follows:</p>

        <span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>C</mi><mi>H</mi><mo>(</mo><mi mathvariant="script">T</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo><mo>=</mo><munder><mo>&#x2211;</mo><mrow><mi mathvariant="normal">&#x2200;</mi><mrow><mi>T</mi><mo>&#x2208;</mo><mi mathvariant="script">T</mi></mrow></mrow></munder><mi>W</mi><mi>T</mi><mo>(</mo><mi>C</mi><mi>V</mi><mo>(</mo><mi>T</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo><mo>)</mo><mo>&#x2217;</mo><mi>C</mi><mi>V</mi><mo>(</mo><mi>T</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    CH(\mathcal{T}, D) = \sum_{\forall{T \in \mathcal{T}}} WT(CV(T, D)) * CV(T, D)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.679488em;vertical-align:-1.089744em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.589744em;"><span style="top:-3.589744em;"><span class="pstrut" style="height:3.050005em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal" style="margin-right:0.25417em;">T</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">&#x2200;</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mrel mtight">&#x2208;</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.25417em;">T</span></span></span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.329483em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2217;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.089744em;"><span></span></span></span></span></span></span></span></span></span></span></span>

        <p>The type system <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">T</mi></mrow><annotation encoding="application/x-tex">\mathcal{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.25417em;">T</span></span></span></span></span> contains all the <abbr title='Resource Description Framework'>RDF</abbr> types that are present in a dataset.
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mi>V</mi><mo>(</mo><mi>T</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">CV(T, D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> represents the <em>coverage</em> of a type <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> in a dataset <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span>,
and is calculated as the fraction of type instances that set a value for all its properties.
The factor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mi>T</mi><mo>(</mo><mi>C</mi><mi>V</mi><mo>(</mo><mi>T</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">WT(CV(T, D))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> is used to weight this sum,
so that the coherence is always a value between 0 and 1, with 1 representing a perfect structuredness.
A maximal coherence means that all instances in the dataset have values for all possible properties in the type system,
which is for example the case in relational databases without optional values.
Based on this metric, the authors introduce a generic method for creating variants of real datasets
with different sizes while maintaining a similar structuredness.
The authors describe a method to calculate the coverage value of this dataset,
which has been <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1700/paper-02.pdf">implemented as a procedure in the Virtuoso <abbr title='Resource Description Framework'>RDF</abbr> store</a> <span class="references">[<a href="#ref-8">8</a>]</span>.
As the goal of our work is to generate <em>realistic</em> <abbr title='Resource Description Framework'>RDF</abbr> public transport datasets,
we will use this metric to compare the realism of generated datasets with real datasets.
As this high-level metric is used to define <em>realism</em> over any kind of <abbr title='Resource Description Framework'>RDF</abbr> dataset,
we will introduce new metrics to validate the realism for specifically the case of public transport datasets.</p>

      </div>
</section>

    <section id="generating_public-transit-background" inlist="" rel="schema:hasPart" resource="#generating_public-transit-background">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Public Transit Background</h3>

        <p>In this section, we present background on public transit planning that is essential to this work.
We discuss existing public transit network planning methodologies
and formats for exchanging transit feeds.</p>

        <h4 id="public-transit-planning">Public Transit Planning</h4>

        <p>The domain of public transit planning entails the design of public transit networks,
rostering of crews, and all the required steps inbetween.
The goal is to maximize the quality of service for passengers while minimizing the costs for the operator.
Given a public demand and a topological area, this planning process aims to obtain routes, timetables and vehicle and crew assignment.
A survey about 69 existing public transit planning approaches
shows that these processes are typically subdivided into <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">five sequential steps</a> <span class="references">[<a href="#ref-18">18</a>]</span>:</p>

        <ol>
          <li><strong>route design</strong>, the placement of transit routes over an existing network.</li>
          <li><strong>frequencies setting</strong>, the temporal instantiation of routes based on the available vehicles and estimated demand.</li>
          <li><strong>timetabling</strong>, the calculation of arrival and departure times at each stop based on estimated demand.</li>
          <li><strong>vehicle scheduling</strong>, vehicle assignment to trips.</li>
          <li><strong>crew scheduling and rostering</strong>, the assignment of drivers and additional crew to trips.</li>
        </ol>

        <p>In this paper, we only consider the first three steps for our mimicking algorithm,
which lead to all the required information
that is of importance to passengers in a public transit schedule.
We present the three steps from this survey in more detail hereafter.</p>

        <p>The first step, route design, requires the topology of an area and public demand as input.
This topology describes the network in an area, which contains possible stops and edges between these stops.
Public demand is typically represented as <em>origin-destination</em> (OD) matrices,
which contain the number of passengers willing to go from origin stops to destination stops.
Given this input, routes are designed based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives</a> <span class="references">[<a href="#ref-18">18</a>]</span>:</p>

        <ul>
          <li><strong>area coverage</strong>: The percentage of public demand that can be served.</li>
          <li><strong>route and trip directness</strong>: A metric that indicates how much the actual trips from passengers deviate from the shortest path.</li>
          <li><strong>demand satisfaction</strong>: How many stops are close enough to all origin and destination points.</li>
          <li><strong>total route length</strong>: The total distance of all routes, which is typically minimized by operators.</li>
          <li><strong>operator-specific objectives</strong>: Any other constraints the operator has, for example the shape of the network.</li>
          <li><strong>historical background</strong>: Existing routes may influence the new design.</li>
        </ul>

        <p>The next step is the setting of frequencies, which is based on the routes from the previous step, public demand and vehicle availability.
The main objectives in this step are based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">metrics</a> <span class="references">[<a href="#ref-18">18</a>]</span>:</p>

        <ul>
          <li><strong>demand satisfaction</strong>: How many stops are serviced frequently enough to avoid overcrowding and long waiting times.</li>
          <li><strong>number of line runs</strong>: How many times each line is serviced – a trade-off between the operator’s aim for minimization and the public demand for maximization.</li>
          <li><strong>waiting time bounds</strong>: Regulation may put restrictions on minimum and maximum waiting times between line runs.</li>
          <li><strong>historical background</strong>: Existing frequencies may influence the new design.</li>
        </ul>

        <p>The last important step for this work is timetabling, which takes the output from the previous steps as input, together with the public demand.
The objectives for this step are the following:</p>

        <ul>
          <li><strong>demand satisfaction</strong>: Total travel time for passengers should be minimized.</li>
          <li><strong>transfer coordination</strong>: Transfers from one line to another at a certain stop should be taken into account during stop waiting times, including how many passengers are expected to transfer.</li>
          <li><strong>fleet size</strong>: The total amount of available vehicles and their usage will influence the timetabling possibilities.</li>
          <li><strong>historical background</strong>: Existing timetables may influence the new design.</li>
        </ul>

        <h4 id="transit-feed-formats">Transit Feed Formats</h4>

        <p>The de-facto standard for public transport time schedules is
the <a href="https://developers.google.com/transit/gtfs/" class="mandatory" data-link-text="https:/​/​developers.google.com/​transit/​gtfs/​">General Transit Feed Specification (GTFS)</a>.
<abbr title='General Transit Feed Specification'>GTFS</abbr> is an exchange format for transit feeds, using a series of <abbr title='Comma Separated Values'>CSV</abbr> files contained in a zip file.
The specification uses the following terminology to define the rules for a public transit system:</p>

        <ul>
          <li><strong>Stop</strong> is a geospatial location where vehicles stop and passengers can get on or off, such as platform 3 in the train station of Brussels.</li>
          <li><strong>Stop time</strong> indicates a scheduled arrival and departure time at a certain stop.</li>
          <li><strong>Route</strong> is a time-independent collection of stops, describing the sequence of stops a certain vehicle follows in a certain public transit line. For example the train route from Brussels to Ghent.</li>
          <li><strong>Trip</strong> is a collection of stops with their respective stop times, such as the route from Brussels to Ghent at a certain time.</li>
        </ul>

        <p>The \zip file is put online by a public transit operator, to be downloaded by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1007/978-3-642-02094-0_7">route planning</a> <span class="references">[<a href="#ref-23">23</a>]</span> software.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1227161.1227166">Two models are commonly used to then extract these rules into a graph</a> <span class="references">[<a href="#ref-24">24</a>]</span>.
In a <em>time-expanded model</em>, a large graph is modeled with arrivals and departures as nodes and edges connect departures and arrivals together.
The weights on these edges are constant.
In a <em>time-dependent model</em>, a smaller graph is modeled in which vertices are physical stops and edges are transit connections between them.
The weights on these edges change as a function of time.
In both models, Dijkstra and Dijkstra-based algorithms can be used to calculate routes.</p>

        <p>In contrast to these two models, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://i11www.iti.kit.edu/extra/publications/dpsw-isftr-13.pdf"><em>Connection Scan Algorithm</em></a> <span class="references">[<a href="#ref-11">11</a>]</span>
takes an ordered array representation of <em>connections</em> as input.
A connection is the actual departure time at a stop and an arrival at the next stop.
These connections can be given a <abbr title='Internationalized Resource Identifier'>IRI</abbr>, and described using <abbr title='Resource Description Framework'>RDF</abbr>, using the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1486/paper_28.pdf">Linked Connections</a> <span class="references">[<a href="#ref-10">10</a>]</span> ontology.
For this base algorithm and its derivatives, a connection object is the smallest building block of a transit schedule.</p>

        <p>In our work, generated public transport networks and time schedules
can be serialized to both the <abbr title='General Transit Feed Specification'>GTFS</abbr> format, and <abbr title='Resource Description Framework'>RDF</abbr> datasets using the Linked Connections ontology.</p>

      </div>
</section>

    <section id="generating_research-question" inlist="" rel="schema:hasPart" resource="#generating_research-question">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Research Question</h3>

        <p>In order to generate public transport networks and schedules,
we start from the hypothesis that both
are correlated with the population distribution within the same area.
More populated areas are expected to have more nearby and more frequent access to public transport,
corresponding to the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">recurring demand satisfaction objective in public transit planning</a> <span class="references">[<a href="#ref-18">18</a>]</span>.
When we calculate the correlation
between the distribution of stops in an area and its population distribution,
we discover a positive correlation of 0.439 for Belgium and 0.459 for the Netherlands (<em>p</em>-values in both cases &lt; 0.00001),
thereby validating our hypothesis with a confidence of 99%.
Because of the continuous population variable and the binary variable indicating whether or not there is a stop,
the correlation is calculated using the <a href="https://github.com/PoDiGG/podigg-evaluate/blob/master/stats/correlation.r" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate/​blob/​master/​stats/​correlation.r">point-biserial correlation coefficient</a>.
For the calculation of these correlations, we ignored the population value outliers.
Following this conclusion, our mimicking algorithm will use such population distributions as input,
and derive public transport networks and trip instances.</p>

        <meta property="lsc:tests" resource="#generating_hypothesis" />

        <meta about="#generating_hypothesis" property="schema:name" content="Public transport networks and schedules are correlated with the population distribution within the same area." />

        <meta property="lsc:confirms" resource="#generating_hypothesis" />

        <p>The main objective of a mimicking algorithm is to create <em>realistic</em> data,
so that it can be used to by benchmarks to evaluate systems under realistic circumstances.
We will measure dataset realism in high-level by comparing the levels of structuredness
of real-world datasets and their synthetic variants
using the <em>coherence metric</em> introduced by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">Duan et al.</a> <span class="references">[<a href="#ref-9">9</a>]</span>.
Furthermore, we will measure the realism of different characteristics within public transport datasets,
such as the location of stops, density of the network of stops, length of routes or the frequency of connections.
We will quantify these aspects by measuring the distance of each aspect between real and synthetic datasets.
These dataset characteristics will be linked with potential evaluation metrics within <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
and tasks to evaluate them.
This generic coherence metric together with domain-specific metrics will provide a way to evaluate dataset realism.</p>

        <p>Based on this, we introduce the following research question for this work:</p>

        <div rel="schema:question">
          <blockquote id="generating_researchquestion" about="#generating_researchquestion" property="schema:name">
            <p>Can population distribution data be used to generate realistic synthetic public transport networks and scheduling?</p>
          </blockquote>
        </div>

        <p>We provide an answer to this question by first introducing an
algorithm for generating public transport networks and their scheduling based on population distributions in <a href="#generating_methodology">Section 2.5</a>.
After that, we validate the realism of datasets that were generated using an implementation of this algorithm in <a href="#generating_evaluation">Section 2.7</a>.</p>

      </div>
</section>

    <section id="generating_methodology" inlist="" rel="schema:hasPart" resource="#generating_methodology">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Method</h3>

        <p>In order to formulate an answer to our research question,
we designed a mimicking algorithm that generates realistic synthetic public transit feeds.
We based it on techniques from the domains of public transit planning, spatiotemporal and <abbr title='Resource Description Framework'>RDF</abbr> dataset generation.
We reuse the route design, frequencies setting and timetabling steps from the domain public transit planning,
but prepend this with a network generation phase.</p>

        <p><a href="#generating_fig:methodology:datamodel">Fig. 3</a> shows the model of the generated public transit feeds,
with connections being the primary data element.</p>

        <figure id="generating_fig:methodology:datamodel">
<img src="generating/img/datamodel.svg" alt="PoDiGG data model" class="figure-medium" />
<figcaption>
            <p><span class="label">Fig. 3:</span> The resources (rectangle), literals (dashed rectangle) and properties (arrows) used to model the generated public transport data.
Node and text colors indicate vocabularies.</p>
          </figcaption>
</figure>

        <p>We consider different properties in this model based on the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">independent, intra-record or inter-record dependency rules</a> <span class="references">[<a href="#ref-21">21</a>]</span>, as discussed in <a href="#generating_related-work">Section 2.2</a>.
The arrival time in a connection can be represented as a fully intra-record dependency,
because it depends on the time it departed and the stops it goes between.
The departure time in a connection is both an intra-record and inter-record dependency,
because it depends on the stop at which it departs,
but also on the arrival time of the connection before it in the trip.
Furthermore, the delay value can be seen as an inter-record dependency,
because it is influenced by the delay value of the previous connection in the trip.
Finally, the geospatial location of a stop depends on the location of its parent station,
so this is also an inter-record dependency.
All other unmentioned properties are independent.</p>

        <p>In order to generate data based on these dependency rules, our algorithm is subdivided in five steps:</p>

        <ol>
          <li><strong>Region</strong>: Creation of a two-dimensional area of cells annotated with population density information.</li>
          <li><strong>Stops</strong>: Placement of stops in the area.</li>
          <li><strong>Edges</strong>: Connecting stops using edges.</li>
          <li><strong>Routes</strong>: Generation of routes between stops by combining edges.</li>
          <li><strong>Trips</strong>: Scheduling of timely trips over routes by instantiating connections.</li>
        </ol>

        <p>These steps are not fully sequential, since stop generation is partially executed before and after edge generation.
The first three steps are required to generate a network,
step 4 corresponds to the route design step in public transit planning
and step 5 corresponds to both the frequencies setting and timetabling.
These steps are explained in the following subsections.</p>

        <h4 id="region">Region</h4>

        <p>In order to create networks, we sample geographic regions in which such networks exist as two-dimensional matrices.
The resolution is defined as a configurable number of cells per square of one latitude by one longitude.
Network edges are then represented as links between these cells.
Because our algorithm is population distribution-based, each cell contains a population density.
These values can either be based on real population information from countries,
or this can be generated based on certain statistical distributions.
For the remainder of this paper, we will reuse the population distribution from Belgium as a running example, as illustrated in <a href="#generating_fig:methodology:region">Fig. 4</a>.</p>

        <figure id="generating_fig:methodology:region">
<img src="generating/img/region.png" alt="Heatmap of the population distribution in Belgium" class="figure-medium" />
<figcaption>
            <p><span class="label">Fig. 4:</span> Heatmap of the population distribution in Belgium, which is illustrated for each cell
as a scale going from white (low), to red (medium) and black (high).
The actual placement of train stops are indicated as green points.</p>
          </figcaption>
</figure>

        <h4 id="stops">Stops</h4>

        <p>Stop generation is divided into two steps.
First, stops are placed based on population values,
then the edge generation step is initiated
after which the second phase of stop generation is executed where additional stops are created based on the generated edges.</p>

        <p><strong>Population-based</strong>
For the initial placement of stops, our algorithm only takes a population distribution as input.
The algorithm iteratively selects random cells in the two-dimensional area, and tags those cells as stops.
To make it <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">region-based</a> <span class="references">[<a href="#ref-20">20</a>]</span>,
the selection uses a weighted Zipf-like-distribution, where cells with high population values have a higher chance
of being picked than cells with lower values.
The shape of this Zipf curve can be scaled to allow for different stop distributions to be configured.
Furthermore, a minimum distance between stops can be configured, to avoid situations where all stops are placed in highly population areas.</p>

        <p><strong>Edge-based</strong>
Another stop generation phase exists after the edge generation
because real transit networks typically show line artifacts for stop placement.
<a href="#generating_fig:methodology:stopplacementgs">Subfig. 5.1</a> shows the actual train stops in Belgium, which clearly shows line structures.
Stop placement after the first generation phase results can be seen in <a href="#generating_fig:methodology:stopplacementp1">Subfig. 5.2</a>,
which does not show these line structures.
After the second stop generation phase, these line structures become more apparent as can be seen in <a href="#generating_fig:methodology:stopplacementp2">Subfig. 6.4</a>.</p>

        <figure id="generating_fig:methodology:stopplacement">

<figure id="generating_fig:methodology:stopplacementgs" class="subfigure">
<img src="generating/img/stops_gs.png" alt="Real stops" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 5.1:</span> Real stops with line structures.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp1" class="subfigure">
<img src="generating/img/stops_parameterized_1.png" alt="Generation phase step 1" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 5.2:</span> Synthetic stops after the first stop generation phase without line structures.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_parameterized_2.png" alt="Generation phase step 2" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 6.4:</span> Synthetic stops after the second stop generation phase with line structures.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 5:</span> Placement of train stops in Belgium, each dot represents one stop.</p>
          </figcaption>
</figure>

        <p>In this second stop generation phase,
edges are modified so that sufficiently populated areas will be included in paths formed by edges,
as illustrated by <a href="#generating_fig:methodology:stopsphase2">Fig. 6</a>.
Random edges will iteratively be selected, weighted by the edge length measured as
Euclidian distance.
(The Euclidian distance based on geographical coordinates is always used to calculate distances in this work.)
On each edge, a random cell is selected weighed by the population value in the cell.
Next, a weighed random point in a certain area around this point is selected.
This selected point is marked as a stop, the original edge is removed and two new edges are added,
marking the path between the two original edge nodes and the newly selected node.</p>

        <figure id="generating_fig:methodology:stopsphase2">

<figure id="generating_fig:methodology:stopsphase2_1" class="subfigure">
<img src="generating/img/stops_phase2_1.svg" alt="Real stops" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 6.1:</span> Selecting a weighted random point on the edge.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_2" class="subfigure">
<img src="generating/img/stops_phase2_2.svg" alt="Generation phase step 1" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 6.2:</span> Defining an area around the selected point.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_3" class="subfigure">
<img src="generating/img/stops_phase2_3.svg" alt="Generation phase step 2" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 6.3:</span> Choosing a random point within the area, weighted by population value.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_phase2_4.svg" alt="Generation phase step 2" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 6.4:</span> Modify edges so that the path includes this new point.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 6:</span> Illustration of the second phase of stop generation where edges are modified to include sufficiently populated areas in paths.</p>
          </figcaption>
</figure>

        <h4 id="edges">Edges</h4>
        <p>The next phase in public transit network generation connects the stops that were generated in the previous phase with edges.
In order to simulate real transit network structures, we split up this generation phase into three sequential steps.
In the first step, clusters of nearby stops are formed, to lay the foundation for short-distance routes.
Next, these local clusters are connected with each other, to be able to form long-distance routes.
Finally, a cleanup step is in place to avoid abnormal edge structures in the network.</p>

        <p><strong>Short-distance</strong>
The formation of clusters with nearby stations is done using agglomerative hierarchical clustering.
Initially, each stop is part of a seperate cluster, where each cluster always maintains its centroid.
The clustering step will iteratively try to merge two clusters with their centroid distance below a certain threshold.
This threshold will increase for each iteration, until a maximum value is reached.
The maximum distance value indicates the maximum inter-stop distance for forming local clusters.
When merging two clusters, an edge is added between the closest stations from the respective clusters.
The center location of the new cluster is also recalculated before the next iteration.</p>

        <p><strong>Long-distance</strong>
At this stage, we have several clusters of nearby stops.
Because all stops need to be reachable from all stops, these separate clusters also need to be connected.
This problem is related to the domain of route planning over public transit networks,
in which networks can be decomposed into smaller clusters of nearby stations to improve the efficiency of route planning.
Each cluster contains one or more <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1137/1.9781611974317.2"><a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611974317.2"><em>border stations</em></a></span> <span class="references">[<a href="#ref-25">25</a>]</span>, which are the only points
through which routes can be formed between different clusters.
We reuse this concept of border stations, by iteratively picking a random cluster,
identifying its closest cluster based on the minimal possible stop distance, and connecting their border stations using a new edge.
After that, the two clusters are merged.
The iteration will halt when all clusters are merged and there is only one connected graph.</p>

        <p><strong>Cleanup</strong>
The final cleanup step will make sure that the number of stops that are connected by only one edge are reduced.
In real train networks, the majority of stations are connected with at least more than one other stations.
The two earlier generation steps however generate a significant number of <em>loose stops</em>,
which are connected with only a single other stop with a direct edge.
In this step, these loose stops are identified, and an attempt is made to connect them to other nearby stops as shown in <a href="#generating_alg:methodology:loosestops">Algorithm 1</a>.
For each loose stop, this is done by first identifying the direction of the single edge of the loose stop on line 18.
This direction is scaled by the radius in which to look for stops, and defines the stepsize for the loop the starts on line 20.
This loop starts from the loose stop and iteratively moves the search position in the defined direction, until it finds a random stop in the radius,
or the search distance exceeds the average distance of between the stops in the neighbourhood of this loose stop.
This random stop from line 22 can be determined
by finding all stations that have a distance to the search point that is below the radius, and picking a random stop from this collection.
If such a stop is found, an edge is added from our loose stop to this stop.</p>

        <figure id="generating_alg:methodology:loosestops" class="algorithm numbered">
<pre><code>FUNCTION RemoveLooseStops(S, E, N, O, r)
</code><code>  INPUT:
</code><code>    Set of stops S
</code><code>    Set of edges E between the stops from S
</code><code>    Maximum number N of closest stations to consider
</code><code>    Maximum average distance O around a stop to be considered a loose station
</code><code>    Radius r in which to look for stops.
</code><code>FOREACH s in S with degree of 1 w.r.t. E DO
</code><code>    sx = x coordinate of s
</code><code>    sy = y coordinate of s
</code><code>    C = N closest stations to s in S excluding s
</code><code>    c = closest station to s in S excluding s
</code><code>    cx = x coordinate of c
</code><code>    cy = y coordinate of c
</code><code>    a = average distance between each pair of stops in C
</code><code>    IF a &lt;= O and C not empty THEN
</code><code>        dx= (sx - cx) * r
</code><code>        dy= (sy - cy) * r
</code><code>        ox = sx; oy = sy
</code><code>        WHILE distance between o and s &lt; a DO
</code><code>            ox += dx; oy += dy
</code><code>            s&#39; = random station around o with radius a * r
</code><code>            IF s&#39; exists
</code><code>                add edge between s and s&#39; to E and continue next for-loop iteration
</code></pre>
<figcaption>
            <p><span class="label">Algorithm 1:</span> Reduce the number of loose stops by adding additional edges.</p>
          </figcaption>
</figure>

        <p><a href="#generating_fig:methodology:edges">Fig. 7</a> shows an example of these three steps.
After this phase, a network with stops and edges is available, and the actual transit planning can commence.</p>

        <figure id="generating_fig:methodology:edges">
 
<figure id="generating_fig:methodology:edges1" class="subfigure">
<img src="generating/img/edges_1.svg" alt="Real stops" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 7.1:</span> Formation of local clusters.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:edges2" class="subfigure">
<img src="generating/img/edges_2.svg" alt="Generation phase step 1" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 7.2:</span> Connecting clusters through border stations.</p>
            </figcaption>
</figure>

<figure id="generating_fig:methodology:edges3" class="subfigure">
<img src="generating/img/edges_3.svg" alt="Generation phase step 2" class="figure-small" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 7.3:</span> Cleanup of loose stops.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 7:</span> Example of the different steps in the edges generation algorithm.</p>
          </figcaption>
</figure>

        <p><strong>Generator Objectives</strong>
The main guaranteed objective of the edge generator is that the stops form a single connected transit network graph.
This is to ensure that all stops in the network can be reached from any other stop using at least one path through the network.</p>

        <h4 id="routes">Routes</h4>

        <p>Given a network of stops and edges, this phase generates routes over the network.
This is done by creating short and long distance routes in two sequential steps.</p>

        <p><strong>Short-distance</strong>
The goal of the first step is to create short routes where vehicles deliver each passed stop.
This step makes sure that all edges are used in at least one route,
this ensures that each stop can at least be reached from each other stop with one or more transfers to another line.
The algorithm does this by first determining a subset of the largest stops in the network, based on the population value.
The shortest path from each large stop to each other large stop through the network is determined.
if this shortest path is shorter than a predetermined value in terms of the number of edges,
then this path is stored as a route, in which all passed stops are considered as actual stops in the route.
For each edge that has not yet been passed after this, a route is created by iteratively
adding unpassed edges to the route that are connected to the edge until an edge is found that has already been passed.</p>

        <p><strong>Long-distance</strong>
In the next step, longer routes are created, where the transport vehicle not necessarily halts at each passed stop.
This is done by iteratively picking two stops from the list of largest stops using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">network-based method</a> <span class="references">[<a href="#ref-20">20</a>]</span> with each stop having an equal chance to be selected.
A heuristical shortest path algorithm is used to determine a route between these stops.
This algorithm searches for edges in the geographical direction of the target stop.
This is done to limit the complexity of finding long paths through potentially large networks.
A random amount of the largest stops on the path are selected, where the amount
is a value between a minimum and maximum preconfigured route length.
This iteration ends when a predetermined number of routes are generated.</p>

        <p><strong>Generator Objectives</strong>
This algorithm takes into account the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives of route design</a> <span class="references">[<a href="#ref-18">18</a>]</span>, as discussed in <a href="#generating_related-work">Section 2.2</a>.
More specifically, by first focusing on the largest stops, a minimal level of <em>area coverage</em> and <em>demand satisfaction</em> is achieved,
because the largest stops correspond to highly populated areas, which therefore satisfies at least a large part of the population.
By determining the shortest path between these largest stops, the <em>route and trip directness</em> between these stops is optimal.
Finally, by not instantiating all possible routes over the network, the <em>total route length</em> is limited to a reasonable level.</p>

        <h4 id="generating_subsec-methodology-trips">Trips</h4>

        <p>A time-agnostic transit network with routes has been generated in the previous steps.
In this final phase, we temporally instantiate routes
by first determining starting times for trips,
after which the following stop times can be calculated based on route distances.
Instead of generating explicit timetables, as is done in typical transit scheduling methodologies,
we create fictional rides of vehicles.
In order to achieve realistic trip times, we approximate real trip time distributions,
with the possibility to encounter delays.</p>

        <p>As mentioned before in <a href="#generating_related-work">Section 2.2</a>, each consecutive pair of start and stop time in a trip over an edge corresponds to a connection.
A connection can therefore be represented as a pair of timestamps,
a link to the edge representing the departure and arrival stop,
a link to the trip it is part of,
and its index within this trip.</p>

        <p><strong>Trip Starting Times</strong>
The trips generator iteratively creates new connections until a predefined number is reached.
For each connection, a random route is selected with a larger chance of picking a long route.
Next, a random start time of the connection is determined.
This is done by first picking a random day within a certain range.
After that, a random hour of the day is determined using a preconfigured distribution.
This distribution is derived from the public logs of <a href="https://hello.irail.be" class="mandatory" data-link-text="https:/​/​hello.irail.be">iRail</a>,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">route planning <abbr title='Application Programming Interface'>API</abbr> in Belgium</a> <span class="references">[<a href="#ref-26">26</a>]</span>.
A seperate hourly distribution is used for weekdays and weekends, which is chosen depending on the random day that was determined.</p>

        <p><strong>Stop Times</strong>
Once the route and the starting time have been determined, different stop times across the trip can be calculated.
For this, we take into account the following factors:</p>

        <ul>
          <li>Maximum vehicle speed <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c9;</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span>, preconfigured constant.</li>
          <li>Vehicle acceleration <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c2;</mi></mrow><annotation encoding="application/x-tex">\varsigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.52778em;vertical-align:-0.09722em;"></span><span class="mord mathdefault" style="margin-right:0.07986em;">&#x3c2;</span></span></span></span>, preconfigured constant.</li>
          <li>Connection distance <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3b4;</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span></span></span></span>, Euclidian distance between stops in network.</li>
          <li>Stop size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c3;</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">&#x3c3;</span></span></span></span>, derived from population value.</li>
        </ul>

        <p>For each connection in the trip, the time it takes for a vehicle to move between the two stops over a certain distance is calculated
using the formula in <a href="#generating_math:methodology:distanceduration">Equation 3</a>.
<a href="#generating_math:methodology:timetomaxspeed">Equation 1</a> calculates the required time to reach maximum speed
and <a href="#generating_math:methodology:distancetomaxspeed">Equation 2</a> calculates the required distance to reach maximum speed.
This formula simulates the vehicle speeding up until its maximum speed, and slowing down again until it reaches its destination.
When the distance is too short, the vehicle will not reach its maximum speed,
and just speeds up as long as possible until is has to slow down again to stop in time.</p>

        <figure id="generating_math:methodology:timetomaxspeed" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>T</mi><mi>&#x3c9;</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>&#x3c9;</mi><mi mathvariant="normal">/</mi><mi>&#x3c2;</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    T_\omega &amp;= \omega / \varsigma
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">&#x3c9;</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07986em;">&#x3c2;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 1:</span> Time to reach maximum speed.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:methodology:distancetomaxspeed" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>&#x3b4;</mi><mi>&#x3c9;</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msubsup><mi>T</mi><mi>&#x3c9;</mi><mn>2</mn></msubsup><mo>&#x22c5;</mo><mi>&#x3c2;</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \delta_\omega &amp;= T_\omega^2 \cdot \varsigma
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em;"><span style="top:-3.147946em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em;"><span style="top:-3.147946em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.07986em;">&#x3c2;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 2:</span> Distance to reach maximum speed.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:methodology:distanceduration" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>T</mi><mi>&#x3c9;</mi></msub><mo>+</mo><mo>(</mo><mi>&#x3b4;</mi><mo>&#x2212;</mo><mn>2</mn><msub><mi>&#x3b4;</mi><mi>&#x3c9;</mi></msub><mo>)</mo><mi mathvariant="normal">/</mi><mi>&#x3c9;</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>&#xa0;if&#xa0;</mtext><msub><mi>&#x3b4;</mi><mi>&#x3c9;</mi></msub><mo>&lt;</mo><mi>&#x3b4;</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msqrt><mrow><mn>2</mn><mi>&#x3b4;</mi><mi mathvariant="normal">/</mi><mi>&#x3c2;</mi></mrow></msqrt></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>&#xa0;otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\begin{cases}
    2T_\omega + (\delta - 2 \delta_\omega) / \omega &amp;\text{ if } \delta_\omega &lt; \delta / 2 \\
    \sqrt{2\delta / \varsigma} &amp;\text{ otherwise}
\end{cases}
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.30003em;vertical-align:-1.400015em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.900015em;"><span style="top:-3.9000150000000002em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.03588em;">&#x3c9;</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07986em;">&#x3c2;</span></span></span><span style="top:-2.8950000000000005em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z'/></svg></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.30499999999999994em;"><span></span></span></span></span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&#xa0;if&#xa0;</span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">&#x3c9;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.03785em;">&#x3b4;</span><span class="mord">/</span><span class="mord">2</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&#xa0;otherwise</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.400015em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 3:</span> Duration for a vehicle to move between two stops.</p>
          </figcaption>
        </figure>

        <p>Not only the connection duration, but also the waiting times of the vehicle at each stop are important for determining the stop times.
These are calculated as a constant minimum waiting time together with a waiting time that increases for larger stop sizes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c3;</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">&#x3c3;</span></span></span></span>,
this increase is determined by a predefined growth factor.</p>

        <p><strong>Delays</strong>
Finally, each connection in the trip will have a certain chance to encounter a delay.
When a delay is applicable, a delay value is randomly chosen within a certain range.
Next to this, also a cause of the delay is determined from a preconfigured list.
These causes are based on the Traffic Element Events from the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>,
which contains a number of events that are not planned by the network operator such as strikes, bad weather or animal collisions.
Different types of delays can have a different impact factor of the delay value,
for instance, simple delays caused by rush hour would have a lower impact factor than a major train defect.
Delays are carried over to next connections in the trip, with again a chance of encountering additional delay.
Furthermore, these delay values can also be reduced when carried over to the next connection by a certain predetermined factor,
which simulates the attempt to reduce delays by letting vehicles drive faster.</p>

        <p><strong>Generator Objectives</strong>
For trip generation, we take into account several objectives from the
setting of frequencies and timetabling from <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transit planning</a> <span class="references">[<a href="#ref-18">18</a>]</span>.
By instantiating more long distance routes, we aim to increase <em>demand satisfaction</em> as much as possible,
because these routes deliver busy and populated areas, and the goal is to deliver these more frequently.
Furthermore, by taking into account realistic time distributions for trip instantiation, we also adhere to this objective.
Secondly, by ensuring waiting times at each stop that are longer for larger stations,
the <em>transfer coordination</em> objective is taken into account to some extent.</p>

      </div>
</section>

    <section id="generating_implementation" inlist="" rel="schema:hasPart" resource="#generating_implementation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Implementation</h3>

        <p>In this section, we discuss the implementation details of <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>, based on the generator algorithm introduced in <a href="#generating_methodology">Section 2.5</a>.
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is split up into two parts: the main <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> generator, which outputs <abbr title='General Transit Feed Specification'>GTFS</abbr> data, and PoDiGG-LC,
which depends on the main generator to output <abbr title='Resource Description Framework'>RDF</abbr> data.
Serialization in <abbr title='Resource Description Framework'>RDF</abbr> using existing ontologies, such as the <a href="http://vocab.gtfs.org/terms" class="mandatory" data-link-text="http:/​/​vocab.gtfs.org/​terms">GTFS</a>
and <a href="http://semweb.mmlab.be/ns/linkedconnections" class="mandatory" data-link-text="http:/​/​semweb.mmlab.be/​ns/​linkedconnections">Linked Connections ontologies</a>,
allows this inherently linked data to be used within <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
where it can for instance be used for benchmarking purposes.
Providing output in <abbr title='General Transit Feed Specification'>GTFS</abbr> will allow this data to be used directly within all systems
that are able to handle transit feeds, such as route planning systems.
The two generator parts will be explained hereafter, followed by a section on how the generator can be configured using various parameters.</p>

        <h4 id="podigg">PoDiGG</h4>

        <p>The main requirement of our system is the ability to generate realistic
public transport datasets using the mimicking algorithm that was introduced in <a href="#generating_methodology">Section 2.5</a>.
This means that given a population distribution of a certain region,
the system must be able to design a network of routes,
and determine timely trips over this network.</p>

        <p>PoDiGG is implemented to achieve this goal. It is written in JavaScript using Node.js,
and is available under an open license on <a href="https://github.com/PoDiGG/podigg" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg">GitHub</a>.
In order to make installation and usage more convenient,
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is available as a Node module on the <a href="https://www.npmjs.com/package/podigg" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg">NPM package manager</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg/​">Docker Hub</a> to easily run on any platform.
Every sub-generator that was explained in <a href="#generating_methodology">Section 2.5</a>, is implemented as a separate module.
This makes <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> highly modifiable and composable, because different implementations of sub-generators can easily be added and removed.
Furthermore, this flexible composition makes it possible to use real data instead of certain sub-generators.
This can be useful for instance when a certain public transport network is already available, and only the trips and connections need to be generated.</p>

        <p>We designed <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> to be highly configurable
to adjust the characteristics of the generated output across different levels,
and to define a certain <em>seed</em> parameter for producing deterministic output.</p>

        <p>All sub-generators store generated data in-memory, using list-based data structures directly corresponding to the <abbr title='General Transit Feed Specification'>GTFS</abbr> format.
This makes <abbr title='General Transit Feed Specification'>GTFS</abbr> serialization a simple and efficient process.
<a href="#generating_table:implementation:gtfsfiles">Table 1</a> shows the <abbr title='General Transit Feed Specification'>GTFS</abbr> files that are generated by the different <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> modules.
This table does not contain references to the region and edges generator,
because they are only used internally as prerequisites to the later steps.
All required files are created to have a valid <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.
Next to that, the optional file for exceptional service dates is created.
Furthermore, <code>delays.txt</code> is created, which is not part of the <abbr title='General Transit Feed Specification'>GTFS</abbr> specification.
It is an extension we provide in order to serialize delay information about each connection in a trip.
These delays are represented in a <abbr title='Comma Separated Values'>CSV</abbr> file containing columns for referring to a connection in a trip,
and contains delay values in milliseconds and a certain reason per connection arrival and departure,
as shown in <a href="#generating_listing:example:delays">Listing 2</a>.</p>

        <figure id="generating_table:implementation:gtfsfiles" class="table">

          <table>
            <thead>
              <tr>
                <th>File</th>
                <th>Generator</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong><code>agency.txt</code></strong></td>
                <td><em>Constant</em></td>
              </tr>
              <tr>
                <td><strong><code>stops.txt</code></strong></td>
                <td>Stops</td>
              </tr>
              <tr>
                <td><strong><code>routes.txt</code></strong></td>
                <td>Routes</td>
              </tr>
              <tr>
                <td><strong><code>trips.txt</code></strong></td>
                <td>Trips</td>
              </tr>
              <tr>
                <td><strong><code>stop_times.txt</code></strong></td>
                <td>Trips</td>
              </tr>
              <tr>
                <td><strong><code>calendar.txt</code></strong></td>
                <td>Trips</td>
              </tr>
              <tr>
                <td><code>calendar_dates.txt</code></td>
                <td>Trips</td>
              </tr>
              <tr>
                <td><code>delays.txt</code></td>
                <td>Trips</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 1:</span> The <abbr title='General Transit Feed Specification'>GTFS</abbr> files that are written by <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>, with their corresponding sub-generators that are responsible for generating the required data.
The files in bold refer to files that are required by the <abbr title='General Transit Feed Specification'>GTFS</abbr> specification.</p>
          </figcaption>
        </figure>

        <figure id="generating_listing:example:delays" class="listing">
<pre><code>trip_id,stop_sequence,delay_departure,delay_arrival,delay_departure_reason         ,delay_arrival_reason
</code><code>100_4  ,0            ,0              ,1405754      ,                               ,td:RepairWork
</code><code>100_6  ,0            ,0              ,1751671      ,                               ,td:BrokenDownTrain
</code><code>100_6  ,1            ,1751671        ,1553820      ,td:BrokenDownTrain             ,td:BrokenDownTrain
</code><code>100_7  ,0            ,2782295        ,0            ,td:TreeAndVegetationCuttingWork,</code></pre>
<figcaption>
            <p><span class="label">Listing 2:</span> Sample of a <code>delays.txt</code> file in a <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.</p>
          </figcaption>
</figure>

        <p>In order to easily observe the network structure in the generated datasets,
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> will always produce a figure accompanying the <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.
<a href="#generating_fig:generated_example">Fig. 8</a> shows an example of such a visualization.</p>

        <figure id="generating_fig:generated_example">
<img src="generating/img/generated_example.png" alt="Visualization of a generated public transport network based on Belgium's population distribution" class="figure-medium" />
<figcaption>
            <p><span class="label">Fig. 8:</span> Visualization of a generated public transport network based on Belgium’s population distribution.
Each route has a different color, and dark route colors indicate more frequent trips over them than light colors.
The population distribution is illustrated for each cell as a scale going from white (low), to red (medium) and black (high).
<a href="https://linkedsoftwaredependencies.org/raw/podigg/gen.png" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​podigg/​gen.png">Full image</a></p>
          </figcaption>
</figure>

        <p>Because the generation of large datasets can take a long time depending on the used parameters,
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> has a logging mechanism, which provides continuous feedback to the user about the current status and progress of the generator.</p>

        <p>Finally, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> provides the option to derive realistic public transit queries over the generated network,
aimed at testing the load of route planning systems.
This is done by iteratively selecting two random stops weighed by their size
and choosing a random starting time based on the same time distribution as discussed in <a href="#generating_subsec:methodology:trips"></a>.
This is serialized to a <a href="https://github.com/linkedconnections/benchmark-belgianrail#transit-schedules" class="mandatory" data-link-text="https:/​/​github.com/​linkedconnections/​benchmark-​belgianrail#transit-​schedules">JSON format</a>
that was introduced for benchmarking the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1486/paper_28.pdf">Linked Connections route planner</a> <span class="references">[<a href="#ref-10">10</a>]</span>.</p>

        <h4 id="podigg-lc">PoDiGG-LC</h4>

        <p>PoDiGG-LC is an extension of <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>, that outputs data in Turtle/RDF using the ontologies shown in <a href="#generating_fig:methodology:datamodel">Fig. 3</a>.
It is also implemented in JavaScript using Node.js, and available under an open license on <a href="https://github.com/PoDiGG/podigg-lc" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​lc">GitHub</a>.
PoDiGG-LC is also available as a Node module on <a href="https://www.npmjs.com/package/podigg-lc" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg-​lc">NPM</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg-lc/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg-​lc/​">Docker Hub</a>.
For this, we extended the <a href="https://github.com/PoDiGG/gtfs2lc" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​gtfs2lc">GTFS-LC tool</a> that is able
to convert <abbr title='General Transit Feed Specification'>GTFS</abbr> datasets to <abbr title='Resource Description Framework'>RDF</abbr> using the Linked Connections and <abbr title='General Transit Feed Specification'>GTFS</abbr> ontologies.
The original tool serializes a minimal subset of the <abbr title='General Transit Feed Specification'>GTFS</abbr> data, aimed at being used for Linked Connections route planning over connections.
Our extension also serializes trip, station and route instances, with their relevant interlinking.
Furthermore, our <abbr title='General Transit Feed Specification'>GTFS</abbr> extension for representing delays is also supported, and is serialized
using a new <a href="http://semweb.datasciencelab.be/ns/linked-connections-delay/" class="mandatory" data-link-text="http:/​/​semweb.datasciencelab.be/​ns/​linked-​connections-​delay/​">Linked Connections Delay ontology</a> that we created.</p>

        <h4 id="configuration">Configuration</h4>

        <p>PoDiGG accepts a wide range of parameters that can be used to configure properties of the different sub-generators.
<a href="#generating_table:implementation:params">Table 2</a> shows an overview of the parameters, grouped by each sub-generator.
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> and PoDiGG-LC accept these <a href="https://github.com/PoDiGG/podigg#parameters" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg#parameters">parameters</a>
either in a JSON configuration file or via environment variables.
Both <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> and PoDiGG-LC produce deterministic output for identical sets of parameters,
so that datasets can easily be reproduced given the configuration.
The <em>seed</em> parameter can be used to introduce pseudo-randomness into the output.</p>

        <figure id="generating_table:implementation:params" class="table">
<table>
    <tr>
        <th></th>
        <th>Name</th>
        <th>Default Value</th>
        <th>Description</th>
    </tr>

    <tr class="row-sep-above">
        <td></td>
        <td><code>seed</code></td>
        <td><code>1</code></td>
        <td>The random seed</td>
    </tr>

    <tr class="row-sep-above">
        <td rowspan="4" class="rotate">Region</td>
        <td><code>region_generator</code></td>
        <td><code>isolated</code></td>
        <td>Name of a region generator. (isolated, noisy or region)</td>
    </tr>
    <tr>
        <td><code>lat_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated latitudes</td>
    </tr>
    <tr>
        <td><code>lon_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated longitudes</td>
    </tr>
    <tr>
        <td><code>cells_per_latlon</code></td>
        <td><code>100</code></td>
        <td>How many cells go in 1 latitude/longitude</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="9" class="rotate">Stops</td>
        <td><code>stops</code></td>
        <td><code>600</code></td>
        <td>How many stops should be generated</td>
    </tr>
    <tr>
        <td><code>min_station_size</code></td>
        <td><code>0.01</code></td>
        <td>Minimum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>max_station_size</code></td>
        <td><code>30</code></td>
        <td>Maximum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells as stops</td>
    </tr>
    <tr>
        <td><code>min_interstop_distance</code></td>
        <td><code>1</code></td>
        <td>Minimum distance between stops in number of cells</td>
    </tr>
    <tr>
        <td><code>factor_stops_post_edges</code></td>
        <td><code>0.66</code></td>
        <td>Factor of stops to generate after edges</td>
    </tr>
    <tr>
        <td><code>edge_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer edges to generate stops on</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells around edges</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_radius</code></td>
        <td><code>2</code></td>
        <td>Radius in number of cells around an edge to select points</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Edges</td>
        <td><code>max_intracluster_distance</code></td>
        <td><code>100</code></td>
        <td>Maximum distance between stops in one cluster</td>
    </tr>
    <tr>
        <td><code>max_intracluster_distance_growthfactor</code></td>
        <td><code>0.1</code></td>
        <td>Power for clustering with more distant stops</td>
    </tr>
    <tr>
        <td><code>post_cluster_max_intracluster_distancefactor</code></td>
        <td><code>1.5</code></td>
        <td>Power for connecting a stop with multiple stops</td>
    </tr>
    <tr>
        <td><code>loosestations_neighbourcount</code></td>
        <td><code>3</code></td>
        <td>Neighbours around a loose station that should define its area</td>
    </tr>
    <tr>
        <td><code>loosestations_max_range_factor</code></td>
        <td><code>0.3</code></td>
        <td>Maximum loose station range relative to the total region size</td>
    </tr>
    <tr>
        <td><code>loosestations_max_iterations</code></td>
        <td><code>10</code></td>
        <td>Maximum iteration number to try to connect one loose station</td>
    </tr>
    <tr>
        <td><code>loosestations_search_radius_factor</code></td>
        <td><code>0.5</code></td>
        <td>Loose station neighbourhood size factor</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="5" class="rotate">Routes</td>
        <td><code>routes</code></td>
        <td><code>1000</code></td>
        <td>The number of routes to generate</td>
    </tr>
    <tr>
        <td><code>largest_stations_fraction</code></td>
        <td><code>0.05</code></td>
        <td>The fraction of stops to form routes between</td>
    </tr>
    <tr>
        <td><code>penalize_station_size_area</code></td>
        <td><code>10</code></td>
        <td>The area in which stop sizes should be penalized</td>
    </tr>
    <tr>
        <td><code>max_route_length</code></td>
        <td><code>10</code></td>
        <td>Maximum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>min_route_length</code></td>
        <td><code>4</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="15" class="rotate">Connections</td>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>connections</code></td>
        <td><code>30000</code></td>
        <td>Number of connections to generate</td>
    </tr>
    <tr>
        <td><code>stop_wait_min</code></td>
        <td><code>60000</code></td>
        <td>Minimum waiting time per stop</td>
    </tr>
    <tr>
        <td><code>stop_wait_size_factor</code></td>
        <td><code>60000</code></td>
        <td>Waiting time to add multiplied by station size</td>
    </tr>
    <tr>
        <td><code>route_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer routes for connections</td>
    </tr>
    <tr>
        <td><code>vehicle_max_speed</code></td>
        <td><code>160</code></td>
        <td>Maximum speed of a vehicle in km/h</td>
    </tr>
    <tr>
        <td><code>vehicle_speedup</code></td>
        <td><code>1000</code></td>
        <td>Vehicle speedup in km/(h$^2$)</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekdays</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekend days</td>
    </tr>
    <tr>
        <td><code>delay_chance</code></td>
        <td><code>0</code></td>
        <td>Chance for a connection delay</td>
    </tr>
    <tr>
        <td><code>delay_max</code></td>
        <td><code>3600000</code></td>
        <td>Maximum delay</td>
    </tr>
    <tr>
        <td><code>delay_choice_power</code></td>
        <td><code>1</code></td>
        <td>Power for selecting larger delays</td>
    </tr>
    <tr>
        <td><code>delay_reasons</code></td>
        <td><code>...<sup>2</sup></code></td>
        <td>Default reasons and chances for delays</td>
    </tr>
    <tr>
        <td><code>delay_reduction_duration_fraction</code></td>
        <td><code>0.1</code></td>
        <td>Maximum part of connection duration to subtract for delays</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Queryset</td>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large starting stations</td>
    </tr>
    <tr>
        <td><code>query_count</code></td>
        <td><code>100</code></td>
        <td>The number of queries to generate</td>
    </tr>
    <tr>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp</td>
    </tr>
    <tr>
        <td><code>max_time_before_departure</code></td>
        <td><code>3600000</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekday</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekend day</td>
    </tr>
</table>
<figcaption>
            <p><span class="label">Table 2:</span> Configuration parameters for the different sub-generators. Time values are represented in milliseconds.
<sup>1</sup> Time distributions are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">public route planning logs</a> <span class="references">[<a href="#ref-26">26</a>]</span>.
<sup>2</sup> Default delays are based on the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>.</p>
          </figcaption>
</figure>

      </div>
</section>

    <section id="generating_evaluation" inlist="" rel="schema:hasPart" resource="#generating_evaluation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Evaluation</h3>

        <p>In this section, we discuss our evaluation of <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>.
We first evaluate the realism of the generated datasets using a constant seed by comparing its coherence to real datasets,
followed by a more detailed realism evaluation of each sub-generator using distance functions.
Finally, we provide an indicative efficiency and scalability evaluation of the generator and discuss practical dataset sizes.
All scripts that were used for the following evaluation can be found on <a href="https://github.com/PoDiGG/podigg-evaluate" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate">GitHub</a>.
Our experiments were executed on a 64-bit Ubuntu 14.04 machine with 128 GB of memory and a 24-core 2.40 GHz <abbr title='Central Processing Unit'>CPU</abbr>.</p>

        <h4 id="generating_subsec:evaluation:coherence">Coherence</h4>

        <h5 id="metric">Metric</h5>
        <p>In order to determine how closely synthetic <abbr title='Resource Description Framework'>RDF</abbr> datasets resemble their real-world variants in terms of <em>structuredness</em>,
the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">coherence metric</a> <span class="references">[<a href="#ref-9">9</a>]</span> can be used.
In <abbr title='Resource Description Framework'>RDF</abbr> dataset generation, the goal is to reach a level of structuredness similar to real datasets.
As mentioned before in <a href="#generating_related-work">Section 2.2</a>, many synthetic datasets have a level of structuredness that is higher than their real-world counterparts.
Therefore, our coherence evaluation should indicate that our generator is not subject to the same problem.
We have implemented a <a href="https://github.com/PoDiGG/graph-coherence" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​graph-​coherence">command-line tool</a>
to calculate the coherence value for any given input dataset.</p>

        <h5 id="results">Results</h5>
        <p>When measuring the coherence of the Belgian railway, buses and Dutch railway datasets,
we discover high values for both the real-world datasets and the synthetic datasets, as can be seen in <a href="#generating_table:eval:coherence">Table 3</a>.
These nearly maximal values indicate that there is a very high level of structuredness in these real-world datasets.
Most instances have all the possible values, unlike most typical <abbr title='Resource Description Framework'>RDF</abbr> datasets,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf">which have values around or below 0.6</a> <span class="references">[<a href="#ref-9">9</a>]</span>.
That is because of the very specialized nature of this dataset, and the fact that they originate
from <abbr title='General Transit Feed Specification'>GTFS</abbr> datasets that have the characteristics of relational databases.
Only a very limited number of classes and predicates are used,
where almost all instances have the same set of attributes.
In fact, these very high coherence values for real-world datasets simplify the process of synthetic dataset generation,
as less attention needs to be given to factors that lead to lower levels of structuredness, such as optional attributes for instances.
When generating synthetic datasets using <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> with the same number of stops, routes and connections for the three gold standards,
we measure very similar coherence values, with differences ranging from 0.08% to 1.64%.
This confirms that <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is able to create datasets with the same high level of structuredness to real datasets of these types,
as it inherits the relational database characteristics from its GTFS-centric mimicking algorithm.</p>

        <figure id="generating_table:eval:coherence" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: left"> </th>
                <th style="text-align: right">Belgian railway</th>
                <th style="text-align: right">Belgian buses</th>
                <th style="text-align: right">Dutch railway</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: left"><strong>Real</strong></td>
                <td style="text-align: right">0.9845</td>
                <td style="text-align: right">0.9969</td>
                <td style="text-align: right">0.9862</td>
              </tr>
              <tr>
                <td style="text-align: left"><strong>Synthetic</strong></td>
                <td style="text-align: right">0.9879</td>
                <td style="text-align: right">0.9805</td>
                <td style="text-align: right">0.9870</td>
              </tr>
              <tr>
                <td style="text-align: left"><strong>Difference</strong></td>
                <td style="text-align: right">0.0034</td>
                <td style="text-align: right">0.0164</td>
                <td style="text-align: right">0.0008</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 3:</span> Coherence values for three gold standards compared to the values for equivalent synthetic variants.</p>
          </figcaption>
        </figure>

        <h4 id="generating_subsec:evaluation:distance">Distance to Gold Standards</h4>

        <p>While the coherence metric is useful to compare the level of structuredness between datasets,
it does not give any detailed information about how <em>real</em> synthetic datasets are in terms of their <em>distance</em> to the real datasets.
In this case, we are working with public transit feeds with a known structure,
so we can look at the different datasets aspects in more detail.
More specifically, we start from real geographical areas with their population distributions,
and consider the distance functions between stops, edges, routes and trips for the synthetic and gold standard datasets.
In order to check the applicability of <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> to different transport types and geographical areas,
we compare with the gold standard data of the Belgian railway, the Belgian buses and the Dutch railway.
The scripts that were used to derive these gold standards from real-world data
can be found on <a href="https://github.com/PoDiGG/population-density-generator" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​population-​density-​generator">GitHub</a>.</p>

        <p>In order to construct distance functions for the different generator elements, we consider several helper functions.
The function in <a href="#generating_math:eval:closest">Equation 4</a> is used to determine the closest element
in a set of elements <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span> to a given element <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>, given a distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>.
The function in <a href="#generating_math:eval:distance">Equation 5</a> calculates the distance between all elements in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span></span></span></span> and all elements in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>,
given a distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>.
The computational complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c7;</mi></mrow><annotation encoding="application/x-tex">\chi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">&#x3c7;</span></span></span></span> is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><mi>B</mi><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi>&#x3ba;</mi><mo>(</mo><mi>f</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|B\| \cdot \kappa(f))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">&#x3ba;</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>,
where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3ba;</mi><mo>(</mo><mi>f</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\kappa(f)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">&#x3ba;</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span></span></span></span> is the cost for one distance calculation for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>.
The complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">&#x394;</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">&#x394;</span></span></span></span> then becomes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><mi>A</mi><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi mathvariant="normal">&#x2225;</mi><mi>B</mi><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi>&#x3ba;</mi><mo>(</mo><mi>f</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|A\| \cdot \|B\| \cdot \kappa(f))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord mathdefault">A</span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">&#x2225;</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">&#x3ba;</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>.</p>

        <figure id="generating_math:eval:closest" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>&#x3c7;</mi><mo>(</mo><mi>a</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>f</mi><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mtext>arg&#xa0;min</mtext><mrow><mi>b</mi><mo>&#x2208;</mo><mi>B</mi></mrow></msub><mi>f</mi><mo>(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\chi(a, B, f) \coloneqq \text{arg min}_{b \in B} f(a, b)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">&#x3c7;</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord text"><span class="mord">arg&#xa0;min</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2419679999999999em;"><span style="top:-2.45586em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mrel mtight">&#x2208;</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.27151em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 4:</span> Function to determine the closest element in a set of elements.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:eval:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">&#x394;</mi><mo>(</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>f</mi><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><munder><mo>&#x2211;</mo><mrow><mi>a</mi><mo>&#x2208;</mo><mi>A</mi></mrow></munder><mrow><mi>f</mi><mo>(</mo><mi>a</mi><mo separator="true">,</mo><mi>&#x3c7;</mi><mo>(</mo><mi>a</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>f</mi><mo>)</mo><mo>)</mo></mrow><mo>+</mo><munder><mo>&#x2211;</mo><mrow><mi>b</mi><mo>&#x2208;</mo><mi>B</mi></mrow></munder><mrow><mi>f</mi><mo>(</mo><mi>b</mi><mo separator="true">,</mo><mi>&#x3c7;</mi><mo>(</mo><mi>b</mi><mo separator="true">,</mo><mi>A</mi><mo separator="true">,</mo><mi>f</mi><mo>)</mo><mo>)</mo></mrow></mrow><mrow><mi mathvariant="normal">&#x2225;</mi><mi>A</mi><mi mathvariant="normal">&#x2225;</mi><mo>+</mo><mi mathvariant="normal">&#x2225;</mi><mi>B</mi><mi mathvariant="normal">&#x2225;</mi></mrow></mfrac></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\Delta(A, B, f) \coloneqq \\
\dfrac{
      \sum\limits_{a \in A}{f(a, \chi(a, B, f))}
    + \sum\limits_{b \in B}{f(b, \chi(b, A, f))}
    }{\|A\| + \|B\|}
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.905488000000001em;vertical-align:-2.202744000000001em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.702744em;"><span style="top:-6.0322320000000005em;"><span class="pstrut" style="height:4.169488em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span></span></span><span style="top:-3.2027439999999996em;"><span class="pstrut" style="height:4.169488em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.1694880000000003em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">&#x2225;</span><span class="mord mathdefault">A</span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">&#x2225;</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">&#x2225;</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.419483em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.750005em;"><span style="top:-2.105664em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mrel mtight">&#x2208;</span><span class="mord mathdefault mtight">A</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.021706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">&#x3c7;</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mclose">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.750005em;"><span style="top:-2.097887em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mrel mtight">&#x2208;</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:1.029483em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">&#x3c7;</span><span class="mopen">(</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.202744000000001em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 5:</span> Function to calculate the distance between all elements in a set of elements.</p>
          </figcaption>
        </figure>

        <h5 id="stops-distance">Stops Distance</h5>
        <p>For measuring the distance between two sets of stops <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">S_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">S_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,
we introduce the distance function from <a href="#generating_math:stops:distance">Equation 6</a>.
This measures the distance between every possible pair of stops using the Euclidian distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>.
Assuming a constant execution time for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3ba;</mi><mo>(</mo><mi>d</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\kappa(d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">&#x3ba;</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span>,
the computational complexity for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>s</mtext></msub></mrow><annotation encoding="application/x-tex">\Delta_\text{s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>S</mi><mn>1</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>S</mi><mn>2</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|S_1\| \cdot \|S_2\|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mclose">)</span></span></span></span>.</p>

        <figure id="generating_math:stops:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>s</mtext></msub><mo>(</mo><msub><mi>S</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><mi mathvariant="normal">&#x394;</mi><mo>(</mo><msub><mi>S</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>S</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>d</mi><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \Delta_\text{s}(S_1, S_2) \coloneqq \Delta(S_1, S_2, d)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">&#x394;</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 6:</span> Function to calculate the distance between two stops.</p>
          </figcaption>
        </figure>

        <h5 id="edges-distance">Edges Distance</h5>
        <p>In order to measure the distance between two sets of edges <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">E_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">E_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,
we use the distance function from <a href="#generating_math:eval:edges:distance">Equation 7</a>,
which measures the distance between all pairs of edges using the distance function $d_\text{e}$.
This distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, which is introduced in <a href="#generating_math:eval:edge:distance">Equation 8</a>,
measures the Euclidian distance between the start and endpoints of each edge, and between the different edges,
weighed by the length of the edges.
The constant <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> in <a href="#generating_math:eval:edge:distance">Equation 8</a> is to ensure that the distance between two edges that have an equal length,
but exist at a different position, is not necessarily zero.
The computational cost of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> can be considered as a constant,
so the complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">\Delta_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> becomes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>E</mi><mn>1</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>E</mi><mn>2</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|E_1\| \cdot \|E_2\|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mclose">)</span></span></span></span>.</p>

        <figure id="generating_math:eval:edges:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>E</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>E</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><mi mathvariant="normal">&#x394;</mi><mo>(</mo><msub><mi>E</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>E</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mtext>e</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\Delta_\text{e}(E_1, E_2) \coloneqq \Delta(E_1, E_2, d_\text{e})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">&#x394;</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 7:</span> Function to calculate the distance between two sets of edges.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:eval:edge:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>d</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>e</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>min</mtext><mo fence="false">(</mo><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>1</mn><mtext>from</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>2</mn><mtext>from</mtext></msubsup><mo>)</mo><mo>+</mo><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>1</mn><mtext>to</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>2</mn><mtext>to</mtext></msubsup><mo>)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"/><mspace width="1em"/><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>1</mn><mtext>from</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>2</mn><mtext>to</mtext></msubsup><mo>)</mo><mo>+</mo><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>1</mn><mtext>to</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>2</mn><mtext>from</mtext></msubsup><mo>)</mo><mo fence="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>&#x22c5;</mo><mo>(</mo><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>1</mn><mtext>from</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>1</mn><mtext>to</mtext></msubsup><mo>)</mo><mo>&#x2212;</mo><mi>d</mi><mo>(</mo><msubsup><mi>e</mi><mn>2</mn><mtext>from</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>e</mi><mn>2</mn><mtext>to</mtext></msubsup><mo>)</mo><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    d_\text{e}(e_1, e_2) \coloneqq &amp; \text{min}\big(
          d(e_1^\text{from}, e_2^\text{from})
        + d(e_1^\text{to}, e_2^\text{to}), \\
    &amp;\quad\quad
          d(e_1^\text{from}, e_2^\text{to})
        + d(e_1^\text{to}, e_2^\text{from})\big) \\
    &amp;\cdot (d(e_1^\text{from}, e_1^\text{to}) - d(e_2^\text{from}, e_2^\text{to}) + 1)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.677323999999999em;vertical-align:-2.0886619999999994em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.588662em;"><span style="top:-4.689554em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span></span></span><span style="top:-3.130446em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-1.5713380000000008em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.0886619999999994em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.588662em;"><span style="top:-4.689554em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord">min</span></span><span class="mord"><span class="delimsizing size1">(</span></span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span style="top:-3.130446em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="delimsizing size1">)</span></span></span></span><span style="top:-1.5713380000000008em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">from</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8435559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">to</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.0886619999999994em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 8:</span> Function to calculate the distance of an edge.</p>
          </figcaption>
        </figure>

        <h5 id="routes-distance">Routes Distance</h5>
        <p>Similarly, the distance between two sets of routes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">R_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">R_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is measured in <a href="#generating_math:eval:routes:distance">Equation 9</a>
by applying <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">&#x394;</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">&#x394;</span></span></span></span> for the distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>r</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.
<a href="#generating_math:eval:route:distance">Equation 10</a> introduces this distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>r</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> between two routes,
which is calculated by considering the edges in each route and measuring the distance
between those two sets using the distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">\Delta_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> from <a href="#generating_math:eval:edges:distance">Equation 7</a>.
By considering the maximum amount of edges per route as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">e_\text{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,
the complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>r</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> becomes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msubsup><mi>e</mi><mtext>max</mtext><mn>2</mn></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e_\text{max}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>
This leads to a complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>R</mi><mn>1</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>R</mi><mn>2</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><msubsup><mi>e</mi><mtext>max</mtext><mn>2</mn></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|R_1\| \cdot \|R_2\| \cdot e_\text{max}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>r</mtext></msub></mrow><annotation encoding="application/x-tex">\Delta_\text{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p>

        <figure id="generating_math:eval:routes:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>r</mtext></msub><mo>(</mo><msub><mi>R</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><mi mathvariant="normal">&#x394;</mi><mo>(</mo><msub><mi>R</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>R</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mtext>r</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\Delta_\text{r}(R_1, R_2) \coloneqq \Delta(R_1, R_2, d_\text{r})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">&#x394;</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 9:</span> Function to calculate the distance between two sets of routes.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:eval:route:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>d</mi><mtext>r</mtext></msub><mo>(</mo><msub><mi>r</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub><mo>(</mo><msubsup><mi>r</mi><mn>1</mn><mtext>edges</mtext></msubsup><mo separator="true">,</mo><msubsup><mi>r</mi><mn>2</mn><mtext>edges</mtext></msubsup><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
d_\text{r}(r_1, r_2) \coloneqq \Delta_\text{e}(r_1^\text{edges}, r_2^\text{edges})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6270159999999998em;vertical-align:-0.563508em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0635079999999997em;"><span style="top:-3.096492em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999998em;"><span style="top:-2.433692em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.180908em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">edges</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.266308em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999998em;"><span style="top:-2.433692em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.180908em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">edges</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.266308em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.563508em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 10:</span> Function to calculate the distance between two routes.</p>
          </figcaption>
        </figure>

        <h5 id="connections-distance">Connections Distance</h5>
        <p>Finally, we measure the distance between two sets of connections <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">C_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">C_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
using the function from <a href="#generating_math:eval:connections:distance">Equation 11</a>.
The distance between two connections is measured using the function from <a href="#generating_math:eval:connection:distance">Equation 12</a>,
which is done by considering their respective temporal distance weighed by a constant <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>&#x3f5;</mi></msub></mrow><annotation encoding="application/x-tex">d_\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">&#x3f5;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> –when serializing time in milliseconds, we set <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>&#x3f5;</mi></msub></mrow><annotation encoding="application/x-tex">d_\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">&#x3f5;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>60000</mn></mrow><annotation encoding="application/x-tex">60000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span></span></span></span>.–,
and their geospatial distance using the edge distance function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.
The complexity of time calculation in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mtext>c</mtext></msub></mrow><annotation encoding="application/x-tex">d_\text{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> can be considered being constant,
which makes it overall complexity <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msubsup><mi>e</mi><mtext>max</mtext><mn>2</mn></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(e_\text{max}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.
For <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>c</mtext></msub></mrow><annotation encoding="application/x-tex">\Delta_\text{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, this leads to a complexity of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>C</mi><mn>1</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><mi mathvariant="normal">&#x2225;</mi><msub><mi>C</mi><mn>2</mn></msub><mi mathvariant="normal">&#x2225;</mi><mo>&#x22c5;</mo><msubsup><mi>e</mi><mtext>max</mtext><mn>2</mn></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\|C_1\| \cdot \|C_2\| \cdot e_\text{max}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">&#x2225;</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">&#x2225;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.</p>

        <figure id="generating_math:eval:connections:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="normal">&#x394;</mi><mtext>c</mtext></msub><mo>(</mo><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo>)</mo><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><mi mathvariant="normal">&#x394;</mi><mo>(</mo><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mtext>c</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\Delta_\text{c}(C_1, C_2) \coloneqq \Delta(C_1, C_2, d_\text{c})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">&#x394;</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 11:</span> Function to calculate the distance between two sets of connections.</p>
          </figcaption>
        </figure>

        <figure id="generating_math:eval:connection:distance" class="equation">
          <p><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>d</mi><mtext>c</mtext></msub><mo>(</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><mo>(</mo><mo>(</mo><msubsup><mi>c</mi><mn>1</mn><mtext>departureTime</mtext></msubsup><mo>&#x2212;</mo><msubsup><mi>c</mi><mn>2</mn><mtext>departureTime</mtext></msubsup><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>+</mo><mo>(</mo><msubsup><mi>c</mi><mn>1</mn><mtext>arrivalTime</mtext></msubsup><mo>&#x2212;</mo><msubsup><mi>c</mi><mn>2</mn><mtext>arrivalTime</mtext></msubsup><mo>)</mo><mi mathvariant="normal">/</mi><msub><mi>d</mi><mi>&#x3f5;</mi></msub><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>+</mo><msub><mi>d</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    d_\text{c}(c_1, c_2) &amp;\coloneqq ((c_1^\text{departureTime} - c_2^\text{departureTime}) \\
    &amp;+ (c_1^\text{arrivalTime} - c_2^\text{arrivalTime}) / d_\epsilon) \\
    &amp;+ d_\text{e}(c_1, c_2)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.6861239999999995em;vertical-align:-2.0930619999999998em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5930619999999998em;"><span style="top:-4.6260460000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.0669380000000004em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-1.5669380000000004em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.0930619999999998em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5930619999999998em;"><span style="top:-4.6260460000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999998em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.180908em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">departureTime</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.266308em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999998em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.180908em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">departureTime</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.266308em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.0669380000000004em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">arrivalTime</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">arrivalTime</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">&#x3f5;</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-1.5669380000000004em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.0930619999999998em;"><span></span></span></span></span></span></span></span></span></span></span></p>
          <figcaption>
            <p><span class="label">Equation 12:</span> Function to calculate the distance between two connections.</p>
          </figcaption>
        </figure>

        <h5 id="computability">Computability</h5>
        <p>When using the introduced functions for calculating the distance between stops, edges, routes or connections,
execution times can become long for a large number of elements because of their large complexity.
When applying these distance functions for realistic numbers of stops, edges, routes and connections,
several optimizations should be done in order to calculate these distances in a reasonable time.
A major contributor for these high complexities is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c7;</mi></mrow><annotation encoding="application/x-tex">\chi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">&#x3c7;</span></span></span></span> for finding the closest element from a set of elements to a given element,
as introduced in <a href="#generating_math:eval:closest">Equation 4</a>.
In practice, we only observed extreme execution times for the respective distance between routes and connections.
For routes, we implemented an optimization, with the same worst-case complexity,
that indexes routes based on their geospatial position, and performs radial search around each route
when the closest one from a set of other routes should be found.
For connections, we consider the linear time dimension when performing binary search for finding the closest connection from a set of elements.</p>

        <h5 id="metrics">Metrics</h5>
        <p>In order to measure the realism of each generator phase, we introduce a <em>realism</em> factor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3c1;</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">&#x3c1;</span></span></span></span> for each phase.
These values are calculated by measuring the distance from randomly generated elements to the gold standard,
divided by the distance from the actually generated elements to the gold standard,
as shown below for respectively stops, edges, routes and connections.
We consider these randomly generated elements having the lowest possible level of realism,
so we use these as a weighting factor in our realism values.</p>

        <span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>&#x3c1;</mi><mtext>s</mtext></msub><mo>(</mo><msub><mi>S</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>S</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>S</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mi mathvariant="normal">&#x394;</mi><mtext>s</mtext></msub><mo>(</mo><msub><mi>S</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>S</mi><mtext>gs</mtext></msub><mo>)</mo><mi mathvariant="normal">/</mi><msub><mi mathvariant="normal">&#x394;</mi><mtext>s</mtext></msub><mo>(</mo><msub><mi>S</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>S</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>&#x3c1;</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>E</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>E</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>E</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>E</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>E</mi><mtext>gs</mtext></msub><mo>)</mo><mi mathvariant="normal">/</mi><msub><mi mathvariant="normal">&#x394;</mi><mtext>e</mtext></msub><mo>(</mo><msub><mi>E</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>E</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>&#x3c1;</mi><mtext>r</mtext></msub><mo>(</mo><msub><mi>R</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>R</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>R</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mi mathvariant="normal">&#x394;</mi><mtext>r</mtext></msub><mo>(</mo><msub><mi>R</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>R</mi><mtext>gs</mtext></msub><mo>)</mo><mi mathvariant="normal">/</mi><msub><mi mathvariant="normal">&#x394;</mi><mtext>r</mtext></msub><mo>(</mo><msub><mi>R</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>R</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>&#x3c1;</mi><mtext>c</mtext></msub><mo>(</mo><msub><mi>C</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo><mo>:</mo></mo><mspace width="-0.06666666666666667em"/><mo>=</mo><msub><mi mathvariant="normal">&#x394;</mi><mtext>c</mtext></msub><mo>(</mo><msub><mi>C</mi><mtext>rand</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>gs</mtext></msub><mo>)</mo><mi mathvariant="normal">/</mi><msub><mi mathvariant="normal">&#x394;</mi><mtext>c</mtext></msub><mo>(</mo><msub><mi>C</mi><mtext>gen</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>gs</mtext></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \rho_\text{s}(S_\text{rand}, S_\text{gen}, S_\text{gs})
    &amp;\coloneqq \Delta_\text{s}(S_\text{rand}, S_\text{gs}) / \Delta_\text{s}(S_\text{gen}, S_\text{gs})\\
    \rho_\text{e}(E_\text{rand}, E_\text{gen}, E_\text{gs})
    &amp;\coloneqq \Delta_\text{e}(E_\text{rand}, E_\text{gs}) / \Delta_\text{e}(E_\text{gen}, E_\text{gs})\\
    \rho_\text{r}(R_\text{rand}, R_\text{gen}, R_\text{gs})
    &amp;\coloneqq \Delta_\text{r}(R_\text{rand}, R_\text{gs}) / \Delta_\text{r}(R_\text{gen}, R_\text{gs})\\
    \rho_\text{c}(C_\text{rand}, C_\text{gen}, C_\text{gs})
    &amp;\coloneqq \Delta_\text{c}(C_\text{rand}, C_\text{gs}) / \Delta_\text{c}(C_\text{gen}, C_\text{gs})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6em;vertical-align:-2.7500000000000004em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.7500000000000004em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mop" style="position:relative;top:-0.03472em;">:</span></span><span class="mrel"><span class="mspace" style="margin-right:-0.06666666666666667em;"></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">rand</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord">&#x394;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gen</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">gs</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:2.7500000000000004em;"><span></span></span></span></span></span></span></span></span></span></span></span>

        <h5 id="results-1">Results</h5>
        <p>We measured these realism values with gold standards for the Belgian railway, the Belgian buses and the Dutch railway.
In each case, we used an optimal set of <a href="https://github.com/PoDiGG/podigg-evaluate/blob/master/bin/evaluate.js" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate/​blob/​master/​bin/​evaluate.js">parameters</a>
to achieve the most realistic generated output.
<a href="#generating_table:eval:realism">Table 4</a> shows the realism values for the three cases,
which are visualized in <a href="#generating_fig:realism:stops">Fig. 9</a>, <a href="#generating_fig:realism:edges">Fig. 10</a>, <a href="#generating_fig:realism:routes">Fig. 11</a> and <a href="#generating_fig:realism:connections">Fig. 12</a>.
Each value is larger than 1, showing that the generator at least produces data that is closer to the gold standard,
and is therefore more realistic.
The realism for edges is in each case very large, showing that our algorithm produces edges that are very similar to
actual the edge placement in public transport networks according to our distance function.
Next, the realism of stops is lower, but still sufficiently high to consider it as realistic.
Finally, the values for routes and connections show that these sub-generators produce output that is closer
to the gold standard than the random function according to our distance function.
Routes achieve the best level of realism for the Belgian railway case.
For this same case, the connections are however only slightly closer to the gold standard than random placement,
while for the other cases the realism is more significant.
All of these realism values show that <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is able to produce realistic data for different regions and different transport types.</p>

        <figure id="generating_table:eval:realism" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: left"> </th>
                <th style="text-align: right">Belgian railway</th>
                <th style="text-align: right">Belgian buses</th>
                <th style="text-align: right">Dutch railway</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: left"><strong>Stops</strong></td>
                <td style="text-align: right">5.5490</td>
                <td style="text-align: right">297.0888</td>
                <td style="text-align: right">4.0017</td>
              </tr>
              <tr>
                <td style="text-align: left"><strong>Edges</strong></td>
                <td style="text-align: right">147.4209</td>
                <td style="text-align: right">1633.4693</td>
                <td style="text-align: right">318.4131</td>
              </tr>
              <tr>
                <td style="text-align: left"><strong>Routes</strong></td>
                <td style="text-align: right">2.2420</td>
                <td style="text-align: right">0.0164</td>
                <td style="text-align: right">1.3095</td>
              </tr>
              <tr>
                <td style="text-align: left"><strong>Connections</strong></td>
                <td style="text-align: right">1.0451</td>
                <td style="text-align: right">1.5006</td>
                <td style="text-align: right">1.3017</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 4:</span> Realism values for the three gold standards in case of the different sub-generators,
respectively calculated for the stops <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>&#x3c1;</mi><mtext>s</mtext></msub></mrow><annotation encoding="application/x-tex">\rho_\text{s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">s</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, edges <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>&#x3c1;</mi><mtext>e</mtext></msub></mrow><annotation encoding="application/x-tex">\rho_\text{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">e</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, routes <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>&#x3c1;</mi><mtext>r</mtext></msub></mrow><annotation encoding="application/x-tex">\rho_\text{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">r</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and connections <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>&#x3c1;</mi><mtext>c</mtext></msub></mrow><annotation encoding="application/x-tex">\rho_\text{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">&#x3c1;</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">c</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p>
          </figcaption>
        </figure>

        <figure id="generating_fig:realism:stops">

<figure id="generating_fig:realism:stops:rand" class="subfigure">
<img src="generating/img/realism/train_be/stops_random.png" alt="Stops Random" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 9.1:</span> Random</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gen" class="subfigure">
<img src="generating/img/realism/train_be/stops_parameterized.png" alt="Stops Generated" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 9.2:</span> Generated</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gs" class="subfigure">
<img src="generating/img/realism/train_be/stops_gs.png" alt="Stops Gold standard" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 9.3:</span> Gold standard</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 9:</span> Stops for the Belgian railway case.</p>
          </figcaption>
</figure>

        <figure id="generating_fig:realism:edges">

<figure id="generating_fig:realism:edges:rand" class="subfigure">
<img src="generating/img/realism/train_be/edges_random.png" alt="Edges Random" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 10.1:</span> Random</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gen" class="subfigure">
<img src="generating/img/realism/train_be/edges_parameterized.png" alt="Edges Generated" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 10.2:</span> Generated</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gs" class="subfigure">
<img src="generating/img/realism/train_be/edges_gs.png" alt="Edges Gold standard" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 10.3:</span> Gold standard</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 10:</span> Edges for the Belgian railway case.</p>
          </figcaption>
</figure>

        <figure id="generating_fig:realism:routes">

<figure id="generating_fig:realism:routes:rand" class="subfigure">
<img src="generating/img/realism/train_be/routes_random.png" alt="Routes Random" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 11.1:</span> Random</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gen" class="subfigure">
<img src="generating/img/realism/train_be/routes_parameterized.png" alt="Routes Generated" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 11.2:</span> Generated</p>
            </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gs" class="subfigure">
<img src="generating/img/realism/train_be/routes_gs.png" alt="Routes Gold standard" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 11.3:</span> Gold standard</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 11:</span> Routes for the Belgian railway case.</p>
          </figcaption>
</figure>

        <figure id="generating_fig:realism:connections">
<img src="generating/img/realism/train_be/connections_distr.svg" alt="Hourly distribution" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 12:</span> Connections per hour for the Belgian railway case.</p>
          </figcaption>
</figure>

        <h4 id="generating_subsec:evaluation:performance">Performance</h4>

        <h5 id="metrics-1">Metrics</h5>
        <p>While performance is not the main focus of this work,
we provide an indicative performance evaluation in this section in order to discover the bottlenecks and limitations
of our current implementation that could be further investigated and resolved in future work.
We measure the impact of different parameters on the execution times of the generator.
The three main parameters for increasing the output dataset size are the number of stops, routes and connections.
Because the number of edges is implicitly derived from the number of stops in order to reach a connected network,
this can not be configured directly.
In this section, we start from a set of parameters that produces realistic output data that is similar to the Belgian railway case.
We let the value for each of these parameters increase to see the evolution of the execution times and memory usage.</p>

        <h5 id="results-2">Results</h5>
        <p><a href="#generating_fig:performance:times">Fig. 13</a> shows a linear increase in execution times when increasing the routes or connections.
The execution times for stops do however increase much faster, which is caused by the higher complexity of networks that are formed for many stops.
The used algorithms for producing this network graph proves to be the main bottleneck when generating large networks.
Networks with a limited size can however be generated quickly, for any number of routes and connections.
The memory usage results from <a href="#generating_fig:performance:mem">Fig. 14</a> also show a linear increase,
but now the increase for routes and connections is higher than for the stops parameter.
These figures show that stops generation is a more <abbr title='Central Processing Unit'>CPU</abbr> intensive process than routes and connections generation.
These last two are able to make better usage of the available memory for speeding up the process.</p>

        <figure id="generating_fig:performance:times">
<img src="generating/img/performance/times.svg" alt="Execution times" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 13:</span> Execution times when increasing the number of stops, routes or connections.</p>
          </figcaption>
</figure>

        <figure id="generating_fig:performance:mem">
<img src="generating/img/performance/mem.svg" alt="Memory usage" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 14:</span> Memory usage when increasing the number of stops, routes or connections.</p>
          </figcaption>
</figure>

        <h4 id="dataset-size">Dataset size</h4>

        <p>An important aspect of dataset generation is its ability to output various dataset sizes.
In <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>, different options are available for tweaking these sizes.
Increasing the time range parameter within the generator increases the number of connections
while the number of stops and routes will remain the same.
When enlarging the geographical area over the same period of time, the opposite is true.
As a rule of thumb, based on the number of triples per connection, stops and routes,
the total number of generated triples per dataset is approximately <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>7</mn><mo>&#x22c5;</mo><mtext mathvariant="italic">#connections</mtext><mo>+</mo><mn>6</mn><mo>&#x22c5;</mo><mtext mathvariant="italic">#stops</mtext><mo>+</mo><mtext mathvariant="italic">#routes</mtext></mrow><annotation encoding="application/x-tex">7 \cdot \textit{\#connections} + 6 \cdot \textit{\#stops} + \textit{\#routes}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord textit">#connections</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x22c5;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord textit">#stops</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord textit">#routes</span></span></span></span></span>.
For the Belgian railway case, containing 30,011 connections over a period of 9 months,
with 583 stops and 362 routes, this would theoretically result in 213,937 triples.
In practice, we reach 235,700 triples when running with these parameters, which is slightly higher because of the other triples that are not
taken into account for this simplified formula, such as the ones for trips, stations and delays.</p>

      </div>
</section>

    <section id="generating_discussion" inlist="" rel="schema:hasPart" resource="#generating_discussion">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Discussion</h3>

        <p>In this section, we discuss the main characteristics, the usage within benchmarks and the limitations of this work.
Finally, we mention several <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> use cases.</p>

        <h4 id="characteristics">Characteristics</h4>

        <p>Our main research question on how to generate realistic synthetic public transport networks
has been answered by the introduction of the mimicking algorithm from <a href="#generating_methodology">Section 2.5</a>,
based on commonly used practises in transit network design.
This is based on the accepted hypothesis that the population distribution
of an area is correlated with its transport network design and scheduling.
We measured the realism of the generated datasets using the coherence metric in <a href="#generating_subsec:evaluation:coherence">Subsection 2.7.1</a>
and more fine-grained realism metrics for different public transport aspects in <a href="#generating_subsec:evaluation:distance">Subsection 2.7.2</a>.</p>

        <p>PoDiGG, our implementation of the algorithm, accepts a wide range of parameters to configure the mimicking algorithm.
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> and PoDiGG-LC are able to output the mimicked data respectively as <abbr title='General Transit Feed Specification'>GTFS</abbr> and <abbr title='Resource Description Framework'>RDF</abbr> datasets,
together with a visualization of the generated transit network.
Our system can be used without requiring any extensive setup or advanced programming skills,
as it consists of simple command lines tools that can be invoked with a number of optional parameters to configure the generator.
Our system is proven to be generalizable to other transport types,
as we evaluated <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> for the bus and train transport type,
and the Belgium and Netherlands geospatial regions in <a href="#generating_subsec:evaluation:distance">Subsection 2.7.2</a>.</p>

        <h4 id="usage-within-benchmarks">Usage within Benchmarks</h4>

        <p>A~synthetic dataset generator,
which is one of the main contributions of this work,
forms an essential aspect of benchmarks for <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.sti-innsbruck.at/sites/default/files/fileadmin/documents/articles/p27-angles.pdf">(RDF) data management systems</a> <span class="references">[<a href="#ref-22">22</a>, <a href="#ref-27">27</a>]</span>.
Prescribing a concrete benchmark that includes the evaluation of tasks is out of scope.
However,
to provide a guideline on how our dataset generator can be used as part of a benchmark,
we relate the primary elements of public transport datasets to <em>choke points</em> in data management systems,
i.e., key technical challenges in these system.
Below, we list choke points related to <em>storage</em> and <em>querying</em> within data management systems and route planning systems.
For each choke point, we introduce example tasks to evaluate them in the context of public transport datasets.
The querying choke points are inspired by the choke points identified by <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">Petzka et. al. for faceted browsing</a> <span class="references">[<a href="#ref-28">28</a>]</span>.</p>

        <ol>
          <li>Storage of entities.
            <ol>
              <li>Storage of stops, stations, connections, routes, trips and delays.</li>
            </ol>
          </li>
          <li>Storage of links between entities.
            <ol>
              <li>Storage of stops per station.</li>
              <li>Storage of connections for stops.</li>
              <li>Storage of the next connection for each connection.</li>
              <li>Storage of connections per trip.</li>
              <li>Storage of trips per route.</li>
              <li>Storage of a connection per delay.</li>
            </ol>
          </li>
          <li>Storage of literals.
            <ol>
              <li>Storage of latitude, longitude, platform code and code of stops.</li>
              <li>Storage of latitude, longitude and label of stations.</li>
              <li>Storage of delay durations.</li>
              <li>Storage of the start and end time of connections.</li>
            </ol>
          </li>
          <li>Storage of sequences.
            <ol>
              <li>Storage of sequences of connections.</li>
            </ol>
          </li>
          <li>Find instances by property value.
            <ol>
              <li>Find latitude, longitude, platform code or code by stop.</li>
              <li>Find station by stop.</li>
              <li>Find country by station.</li>
              <li>Find latitude, longitude, or label by station.</li>
              <li>Find delay by connection.</li>
              <li>Find next connection by connection.</li>
              <li>Find trip by connection.</li>
              <li>Find route by connection.</li>
              <li>Find route by trip.</li>
            </ol>
          </li>
          <li>Find instances by inverse property value.
            <ol>
              <li><em>Inverse of examples above.</em></li>
            </ol>
          </li>
          <li>Find instances by a combination of properties values.
            <ol>
              <li>Find stops by geospatial location.</li>
              <li>Find stations by geospatial location.</li>
            </ol>
          </li>
          <li>Find instances for a certain property path with a certain value.
            <ol>
              <li>Find the delay value of the connection after a given connection.</li>
              <li>Find the delay values of all connections after a given connection.</li>
            </ol>
          </li>
          <li>Find instances by inverse property path with a certain value.
            <ol>
              <li>Find stops that are part of a certain trip that passes by the stop at the given geospatial location.</li>
            </ol>
          </li>
          <li>Find instances by class, including subclasses.
            <ol>
              <li>Find delays of a certain class.</li>
            </ol>
          </li>
          <li>Find instances with a numerical value within a certain interval.
            <ol>
              <li>Find stops by latitude or longitude range.</li>
              <li>Find stations by latitude or longitude range.</li>
              <li>Find delays with durations within a certain range.</li>
            </ol>
          </li>
          <li>Find instances with a combination of numerical values within a certain interval.
            <ol>
              <li>Find stops by geospatial range.</li>
              <li>Find stations by geospatial range.</li>
            </ol>
          </li>
          <li>Find instances with a numerical interval by a certain value for a certain property path.
            <ol>
              <li>Find connections that pass by stops in a given geospatial range.</li>
            </ol>
          </li>
          <li>Find instances with a numerical interval by a certain value.
            <ol>
              <li>Find connections that occur at a certain time.</li>
            </ol>
          </li>
          <li>Find instances with a numerical interval by a certain value for a certain property path.
            <ol>
              <li>Find trips that occur at a certain time.</li>
            </ol>
          </li>
          <li>Find instances with a numerical interval by a certain interval.
            <ol>
              <li>Find connections that occur during a certain time interval.</li>
            </ol>
          </li>
          <li>Find instances with a numerical interval by a certain interval for a certain property path.
            <ol>
              <li>Find trips that occur during a certain time interval.</li>
            </ol>
          </li>
          <li>Find instances with numerical intervals by intervals with property paths.
            <ol>
              <li>Find connections that occur during a certain time interval with stations that have stops in a given geospatial range.</li>
              <li>Find trips that occur during a certain time interval with stops in a given geospatial range.</li>
              <li>Plan a route that gets me from stop A to stop B starting at a certain time.</li>
            </ol>
          </li>
        </ol>

        <p>This list of choke points and tasks can be used
as a basis for benchmarking spatiotemporal data management systems 
using public transport datasets.
For example, <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries can be developed based on these tasks
and executed by systems using a public transport dataset.
For the benchmarking with these tasks, it is essential that the used datasets are realistic,
as discussed in <a href="#generating_subsec:evaluation:distance">Subsection 2.7.2</a>.
Otherwise, certain choke points may not resemble the real world.
For example, if an unrealistic dataset would contain only a single trip that goes over all stops,
then finding a route between two given stops could be unrealistically simple.</p>

        <h4 id="limitations-and-future-work">Limitations and Future Work</h4>

        <p>In this section, we discuss the limitations of the current mimicking algorithm and its implementation,
together with further research opportunities.</p>

        <h5 id="memory-usage">Memory Usage</h5>
        <p>The sequential steps in the presented mimicking algorithm require persistence of the intermediary data that is generated in each step.
Currently, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is implemented in such a way that all data is kept in-memory for the duration of the generation, until it is serialized.
When large datasets need to be generated, this requires a larger amount of memory to be allocated to the generator.
Especially for large amounts of routes or connections, where 100 million connections already require almost 10GB of memory to be allocated.
While performance was not the primary concern in this work, in future work, improvements could be made in the future.
A first possible solution would be to use a memory-mapped database for intermediary data,
so that not all data must remain in memory at all times.
An alternative solution would be to modify the mimicking process to a streaming algorithm,
so that only small parts of data need to be kept in memory for datasets of any size.
Considering the complexity of transit networks, a pure streaming algorithm might not be feasible,
because route design requires knowledge of the whole network.
The generation of connections however, could be adapted so that it works as a streaming algorithm.</p>

        <h5 id="realism">Realism</h5>
        <p>We aimed to produce realistic transit feeds by reusing the methodologies learned in public transit planning.
Our current evaluation compares generated output to real datasets, as no similar generators currently exist.
When similar generation algorithms are introduced in the future, this evaluation can be extended to compare their levels of realism.
Our results showed that all sub-generators, except for the trips generator, produced output with a high realism value.
The trips are still closer to real data than a random generator, but this can be further improved in future work.
This can be done by for instance taking into account <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">network capacities</a> <span class="references">[<a href="#ref-18">18</a>]</span>
on certain edges when instantiating routes as trips,
because we currently assume infinite edge capacities, which can result in a large amount of connections over an edge at the same time,
which may not be realistic for certain networks.
Alternatively, we could include other factors in the generation algorithm,
such as the location of certain points of interest, such as shopping areas, schools and tourist spots.
In the future, a study could be done to identify and measure the impact of certain points of interest on transit networks,
which could be used as additional input to the generation algorithm to further improve the level of realism.
Next to this, in order to improve <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transfer coordination</a> <span class="references">[<a href="#ref-18">18</a>]</span>,
possible transfers between trips should be taken into account when generating stop times.
Limiting the network capacity will also lead to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">natural congestion of networks</a> <span class="references">[<a href="#ref-20">20</a>]</span>,
which should also be taken into account for improving the realism.
Furthermore, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">total vehicle fleet size</a> <span class="references">[<a href="#ref-18">18</a>]</span> should be considered,
because we currently assume an infinite number of available vehicles.
It is more realistic to have a limited availability of vehicles in a network,
with the last position of each vehicle being of importance when choosing the next trip for that vehicle.</p>

        <h5 id="alternative-implementations">Alternative Implementations</h5>
        <p>An alternative way of implementing this generator would be to define declarative dependency rules for public transport networks,
based on the work by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">Pengyue et. al.</a> <span class="references">[<a href="#ref-21">21</a>]</span>. This would require a semantic extension to the engine
so that is aware of the relevant ontologies and that it can serialize to one or more <abbr title='Resource Description Framework'>RDF</abbr> formats.
Alternatively, machine learning techniques could be used
to automatically learn the structure and characteristics of real datasets
and create <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/MIC.2008.55">similar realistic synthetic datasets</a> <span class="references">[<a href="#ref-29">29</a>]</span>,
or to <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.08764">create variants of existing datasets</a> <span class="references">[<a href="#ref-30">30</a>]</span>.
The downside of machine learning techniques is however that it is typically more difficult to tweak parameters of automatically learned models
when specific characteristics of the output need to be changed, when compared to a manually implemented algorithm.
Sensitivity analysis could help to determine the impact of such parameters in order to understand the learned models better.</p>

        <h5 id="streaming-extension">Streaming Extension</h5>
        <p>Finally, the temporal aspect of public transport networks is useful for the domain of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf">RDF stream processing</a> <span class="references">[<a href="#ref-31">31</a>]</span>.
Instead of producing single static datasets as output, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> could be adapted to produce <abbr title='Resource Description Framework'>RDF</abbr> streams of connections and delays,
where information about stops and routes are part of the background knowledge.
Such an extension can become part of a benchmark, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf">CityBench</a> <span class="references">[<a href="#ref-32">32</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://iswc2012.semanticweb.org/sites/default/files/76500294.pdf">LSBench</a> <span class="references">[<a href="#ref-17">17</a>]</span>,
for assessing the performance of <abbr title='Resource Description Framework'>RDF</abbr> stream processing systems with temporal and geospatial capabilities.</p>

        <h4 id="podigg-in-use">PoDiGG In Use</h4>

        <p>PoDiGG and PoDiGG-LC have been developed for usage within the <abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> platform.
This platform is being developed within the <abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> project and aims to provide
an environment for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> systems for Big Linked Data.
The platform provides several default dataset generators, including <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>,
which can be used to benchmark systems.</p>

        <p>PoDiGG, and its generated datasets are being used in the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf">ESWC Mighty Storage Challenge 2017 and 2018</a> <span class="references">[<a href="#ref-33">33</a>]</span>.
The first task of this challenge consists of <abbr title='Resource Description Framework'>RDF</abbr> data ingestion into triple stores,
and querying over this data.
Because of the temporal aspect of public transport data in the form of connections,
<abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> datasets are fragmented by connection departure time, and transformed to a data stream that can be inserted.
In task 4 of this challenge, the efficiency of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">faceted browsing solutions is benchmarked</a> <span class="references">[<a href="#ref-28">28</a>]</span>.
In this work, a list of choke points are identified regarding <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries on triple stores,
which includes points such as the selection of subclasses and property-path transitions.
Because of the geographical property of public transport data, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> datasets are being used for this benchmark.</p>

        <p>Finally, <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> is being used for creating virtual transit networks of variable size
for the purposes of benchmarking route planning frameworks, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1486/paper_28.pdf">Linked Connections</a> <span class="references">[<a href="#ref-10">10</a>]</span>.</p>

      </div>
</section>

    <section id="generating_conclusions" inlist="" rel="schema:hasPart" resource="#generating_conclusions">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Conclusions</h3>

        <p>In this article, we introduced a mimicking algorithm for public transport data,
based on steps that are used in real-world transit planning.
Our method splits this process into several sub-generators and uses population distributions of an area as input.
As part of this article, we introduced <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr>, a reusable framework that accepts a wide range of parameters to configure the generation algorithm.</p>

        <p>Results show that the structuredness of generated datasets are similar to real public transport datasets.
Furthermore, we introduced several functions for measuring the realism of
synthetic public transport datasets compared to a gold standard on several levels, based on distance functions.
The realism was confirmed for different regions and transport types.
Finally, the execution times and memory usages were measured when increasing the most important parameters,
which showed a linear increase for each parameter, showing that the generator is able to scale to large dataset outputs.</p>

        <p>The public transport mimicking algorithm we introduced, with <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> and PoDiGG-LC as implementations,
is essential for properly benchmarking the efficiency and performance
of public transport route planning systems under a wide range of realistic, but synthetic circumstances.
Flexible configuration allows datasets of any size to be created
and various characteristics to be tweaked to achieve highly specialized datasets for testing specific use cases.
In general, our dataset generator can be used for the benchmarking of geospatial and temporal <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
and therefore lowers the barrier towards more efficient and performant systems.</p>
      </div>
</section>

    <div class="subfooter">
  <section id="generating_acknowledgements" inlist="" rel="schema:hasPart" resource="#generating_acknowledgements">
<div datatype="rdf:HTML" property="schema:description">
          <h3 property="schema:name" class="no-label-increment">Acknowledgements</h3>

          <p>We wish to thank Henning Petzka for his help with discovering issues and providing useful suggestions for the <abbr title='POpulation DIstribution-based GTFS Generator'>PoDiGG</abbr> implementation.
The described research activities were funded by the H2020 project <abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> (#688227).</p>

        </div>
</section>

</div>
  </section>
  
  <section class="sub-paper">
    <h2 id="storing">Storing Evolving Data</h2>

    <section class="sub-preface">
<div datatype="rdf:HTML" property="schema:description">
        <p>In this chapter, we tackle the second challenge of this PhD, which is:
“Indexing evolving data involves a <em>trade-off</em> between <em>storage size</em> and <em>lookup efficiency</em>”.
As <em>evolving</em> knowledge graphs add a temporal dimension to regular knowledge graphs,
new storage and querying techniques are required.
A naive way to handle this temporal dimension would be to store each knowledge graph version as a separately materialized knowledge graph.
This can however introduce a tremendous storage overhead when consecutive versions are similar to each other.
Furthermore, querying over such a naive storage method would require going through <em>all</em> of these versions,
which does not scale well with many versions.</p>

        <p>The focus of our work in this chapter is to come up with a Web-friendly trade-off between storage size and lookup efficiency,
so that evolving knowledge graphs can be published on the Web without requiring high-end machines.
We introduce a new indexing technique for evolving data,
that focuses on querying in a <em>stream-based</em> manner.
This allows results to be sent to the client from the moment that they are found,
which reduces waiting time compared to batch-based querying.
Streaming is especially useful when the number of results is very large,
and is memory friendly for machines with limited capabilities.</p>

        <p>This chapter is based on the research question:
“How can we store <abbr title='Resource Description Framework'>RDF</abbr> archives to enable efficient versioned triple pattern queries with offsets?”
We focus on triple pattern queries, as these are the fundamental building blocks
for more complex <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries over knowledge graphs.
For example, the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments interface</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> exposes triple pattern access to datasets,
which is sufficient for evaluating complex <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries on top of this.
We answer our research question by introducing a storage technique
that introduces various temporal indexes next to the typical indexes that are required for knowledge graphs.
These indexes are essential for achieving efficient querying for different kinds of versioned queries.
We extensively evaluate this approach based on our implementation <em>OSTRICH</em>.
Our results show that our method achieves a trade-off between storage size and lookup efficiency
that is useful for hosting evolving knowledge graphs on the Web.
Concretely, at the cost of an increase in storage size and ingestion time,
query execution time is significantly reduced.
As storage is typically cheap, and ingestion can happen offline, this trade-off is acceptable in a Web environment.</p>

      </div>
</section>

    <p class="published-as">Ruben Taelman, Miel Vander Sande, Joachim Van Herwegen, Erik Mannens, and Ruben Verborgh. 2019. <strong><a href="https://rdfostrich.github.io/article-jws2018-ostrich/">Triple Storage for Random-Access Versioned Querying of <abbr title='Resource Description Framework'>RDF</abbr> Archives</a></strong>. Journal of Web Semantics 54 (January 2019), 4–28.</p>

    <section id="storing_abstract" inlist="" rel="schema:hasPart" resource="#storing_abstract">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name" class="no-label-increment">Abstract</h3>

        <!-- Context      -->
        <p>When publishing Linked Open Datasets on the Web,
most attention is typically directed to their latest version.
Nevertheless, useful information is present in or between previous versions.
<!-- Need         -->
In order to exploit this historical information in dataset analysis,
we can maintain history in <abbr title='Resource Description Framework'>RDF</abbr> archives.
Existing approaches either require much storage space,
or they expose an insufficiently expressive or efficient interface
with respect to querying demands.
<!-- Task         -->
In this article, we introduce an <abbr title='Resource Description Framework'>RDF</abbr> archive indexing technique that is able to store datasets
with a low storage overhead,
by compressing consecutive versions and adding metadata for reducing lookup times.
<!-- Object       -->
We introduce algorithms based on this technique for efficiently evaluating
queries <em>at</em> a certain version, <em>between</em> any two versions, and <em>for</em> versions.
Using the <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> <abbr title='Resource Description Framework'>RDF</abbr> archiving benchmark,
we evaluate our implementation, called <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
<!-- Findings     -->
Results show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> introduces a new trade-off regarding storage space, ingestion time, and querying efficiency.
By processing and storing more metadata during ingestion time,
it significantly lowers the average lookup time for versioning queries.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> performs better for many smaller dataset versions
than for few larger dataset versions.
Furthermore, it enables efficient offsets in query result streams,
which facilitates random access in results.
<!-- Conclusion   -->
Our storage technique reduces query evaluation time for versioned queries
through a preprocessing step during ingestion,
which only in some cases increases storage space when compared to other approaches.
This allows data owners to store and query multiple versions of their dataset efficiently,
<!-- Perspectives -->
lowering the barrier to historical dataset publication and analysis.</p>

      </div>
</section>

    <section id="storing_introduction" inlist="" rel="schema:hasPart" resource="#storing_introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Introduction</h3>

        <p>In the area of data analysis,
there is an ongoing need for maintaining the history of datasets.
Such archives can be used for looking up data at certain points in time,
for requesting evolving changes,
or for checking the temporal validity of these data <span class="references">[<a href="#ref-35">35</a>]</span>.
With the continuously increasing number of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/DesignIssues/LinkedData.html">Linked Open Datasets</a> <span class="references">[<a href="#ref-4">4</a>]</span>,
archiving has become an issue for <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF</a> <span class="references">[<a href="#ref-2">2</a>]</span> data as well.
While the <abbr title='Resource Description Framework'>RDF</abbr> data model itself is atemporal, Linked Datasets typically <a property="schema:citation http://purl.org/spar/cito/cites" href="http://events.linkeddata.org/ldow2010/papers/ldow2010_paper12.pdf">change over time</a> <span class="references">[<a href="#ref-36">36</a>]</span> on
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://export.arxiv.org/pdf/1504.01891">dataset, schema, and/or instance level</a> <span class="references">[<a href="#ref-37">37</a>]</span>.
Such changes can include additions,
modifications, or deletions of complete datasets, ontologies, and separate facts.
While some evolving datasets, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.cis.upenn.edu/~zives/research/dbpedia.pdf">DBpedia</a> <span class="references">[<a href="#ref-38">38</a>]</span>,
are published as separate dumps per version,
more direct and efficient access to prior versions is desired.</p>

        <p>Consequently,
<abbr title='Resource Description Framework'>RDF</abbr> archiving systems emerged that, for instance, support query engines that use the standard <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>.
In 2015, however, <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">a survey on archiving Linked Open Data</a> <span class="references">[<a href="#ref-35">35</a>]</span> illustrated the need for improved versioning capabilities,
as current approaches have scalability issues at Web-scale.
They either perform well for versioned query evaluation—at the cost of large storage space requirements—or
require less storage space—at the cost of slower query evaluation.
Furthermore, no existing solution performs well for all versioned query types, namely querying <em>at</em>, <em>between</em>, and <em>for</em> different versions.
An efficient <abbr title='Resource Description Framework'>RDF</abbr> archive solution should have a scalable <em>storage model</em>,
efficient <em>compression</em>, and <em>indexing methods</em> that enable expressive versioned querying <span class="references">[<a href="#ref-35">35</a>]</span>.</p>

        <p>In this article,
we argue that supporting both <abbr title='Resource Description Framework'>RDF</abbr> archiving and <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> at once is difficult to scale due to their combined complexity.
Instead, we propose an elementary but efficient versioned <em>triple pattern</em> index.
Since triple patterns are the basic element of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr>,
such indexes can serve as an entry point for query engines.
Our solution is applicable as:
(a) an alternative index with efficient triple-pattern-based access for existing engines, in order to improve the efficiency of more expressive <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries; and
(b) a data source for the Web-friendly <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> (TPF) interface, i.e.,
a Web <abbr title='Application Programming Interface'>API</abbr> that provides access to <abbr title='Resource Description Framework'>RDF</abbr> datasets by triple pattern and partitions the results in pages.
We focus on the performance-critical features of <em>stream-based results</em>, query result <em>offsets</em>, and <em>cardinality estimation</em>.
Stream-based results allow more memory-efficient processing when query results are plentiful.
The capability to efficiently offset (and limit) a large stream reduces processing time if only a subset is needed.
Cardinality estimation is essential for efficient <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">query planning</a></span> <span class="references">[<a href="#ref-34">34</a>, <a href="#ref-39">39</a>]</span> in many query engines.</p>

        <p>Concretely,
this work introduces a storage technique with the following contributions:</p>

        <ul>
          <li>a scalable versioned and compressed <abbr title='Resource Description Framework'>RDF</abbr> <em>index</em> with <em>offset</em> support and result <em>streaming</em>;</li>
          <li>efficient <em>query algorithms</em> to evaluate triple pattern queries and perform cardinality estimation <em>at</em>, <em>between</em>, and <em>for</em> different versions, with optional <em>offsets</em>;</li>
          <li>an open-source <em>implementation</em> of this approach called OSTRICH;</li>
          <li>an extensive <em>evaluation</em> of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> compared to other approaches using an existing <abbr title='Resource Description Framework'>RDF</abbr> archiving benchmark.</li>
        </ul>

        <p>The main novelty of this work is the combination of efficient offset-enabled queries over a new index structure for <abbr title='Resource Description Framework'>RDF</abbr> archives.
We do not aim to compete with existing versioned <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engines—full access to the language can instead be leveraged by different engines,
or by using alternative <abbr title='Resource Description Framework'>RDF</abbr> publication and querying methods such as the <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> interface-based <abbr title='Triple Pattern Fragments'>TPF</abbr> approach.
Optional versioning capabilities are possible for <abbr title='Triple Pattern Fragments'>TPF</abbr> by using <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">VTPF</a> <span class="references">[<a href="#ref-40">40</a>]</span>,
or <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/jod2017.pdf">datetime content-negotiation</a> <span class="references">[<a href="#ref-41">41</a>]</span> through <a property="schema:citation http://purl.org/spar/cito/cites" href="https://arxiv.org/pdf/0911.1112.pdf">Memento</a> <span class="references">[<a href="#ref-42">42</a>]</span>.</p>

        <p>This article is structured as follows.
In the following section, we start by introducing the related work and our problem statement in <a href="#storing_problem-statement">Section 3.3</a>.
Next, in <a href="#storing_fundamentals">Section 3.4</a>, we introduce the basic concepts of our approach,
followed by our storage approach in <a href="#storing_storage">Section 3.5</a>, our ingestion algorithms in <a href="#storing_ingestions">Section 3.6</a>,
and the accompanying querying algorithms in <a href="#storing_querying">Section 3.7</a>.
After that, we present and discuss the evaluation of our implementation in <a href="#storing_evaluation">Section 3.8</a>.
Finally, we present our conclusions in <a href="#storing_conclusions">Section 3.9</a>.</p>

      </div>
</section>

    <section id="storing_related-work" inlist="" rel="schema:hasPart" resource="#storing_related-work">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Related Work</h3>

        <p>In this section, we discuss existing solutions and techniques for indexing and compression in <abbr title='Resource Description Framework'>RDF</abbr> storage, without archiving support.
Then, we compare different <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions.
Finally, we discuss suitable benchmarks and different query types for <abbr title='Resource Description Framework'>RDF</abbr> archives.
This section does not contain an exhaustive list of all relevant solutions and techniques,
instead, only those that are most relevant to this work are mentioned.</p>

        <h4 id="general-rdf-indexing-and-compression">General <abbr title='Resource Description Framework'>RDF</abbr> Indexing and Compression</h4>

        <p>RDF storage systems typically use indexing and compression techniques
for reducing query times and storage space.
These systems can either be based on existing database technologies,
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1007/978-3-642-04329-1_21">relational databases</a> <span class="references">[<a href="#ref-43">43</a>]</span> or <a property="schema:citation http://purl.org/spar/cito/cites" href="https://cindy.informatik.uni-bremen.de/cosy/staff/dylla/publications/Paper/cosy:sparq-ki-ws06.pdf">document stores</a> <span class="references">[<a href="#ref-44">44</a>]</span>,
or on techniques tailored to <abbr title='Resource Description Framework'>RDF</abbr>.
These technologies can even be combined, such as approaches that detect <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.www2015.it/documents/proceedings/proceedings/p864.pdf"><em>emergent schemas</em></a> <span class="references">[<a href="#ref-45">45</a>, <a href="#ref-46">46</a>]</span>
in <abbr title='Resource Description Framework'>RDF</abbr> datasets, which allow parts of the data to be stored in relational databases
in order to increase compression and improve the efficiency of query evaluation.
These emergent schemas are recently being exploited as <em>characteristics sets</em>
in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.cs.uoi.gr/~nikos/ICDE17.pdf">native <abbr title='Resource Description Framework'>RDF</abbr> approaches</a> <span class="references">[<a href="#ref-47">47</a>, <a href="#ref-48">48</a>]</span>.
For the remainder of this article, we focus the RDF-specific techniques that have direct relevance to our approach.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://sites.fas.harvard.edu/~cs265/papers/neumann-2008.pdf">RDF-3X</a> <span class="references">[<a href="#ref-39">39</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> storage technique that is based
on a clustered B+Tree with 18 indexes in which triples are sorted lexicographically.
Given that a triple consists of
a subject (S), predicate (P) and object (O),
it includes six indexes for different triple component orders (SPO, SOP, OSP, OPS, PSO and POS),
six aggregated indexes (SP, SO, PS, PO, OS, and OP),
and three one-valued indexes (S, P, and O).
A dictionary is used to compress common triple components.
When evaluating <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries, optimal indexes can be selected based on the query’s triple patterns.
Furthermore, the store allows update operations.
In our storage approach, we will reuse the concept of multiple indexes
and encoding triple components in a dictionary.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://people.csail.mit.edu/tdanford/6830papers/weiss-hexastore.pdf">Hexastore</a> <span class="references">[<a href="#ref-49">49</a>]</span> is a similar approach as it uses six different sorted lists,
one for each possible triple component order.
Also, it uses dictionary encoding to compress common triple components.
An alternative is <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.vldb.org/pvldb/vol6/p517-yuan.pdf">Triplebit</a> <span class="references">[<a href="#ref-50">50</a>]</span>, which is based on a two-dimensional storage matrix.
Columns correspond to predicates, and rows to subjects and objects.
This sparse matrix is compressed and dictionary-encoded to reduce storage requirements.
Furthermore, it uses auxiliary index structures to improve index selection during query evaluation.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://arxiv.org/pdf/1105.4004.pdf">K2-Triples</a> <span class="references">[<a href="#ref-51">51</a>]</span> is another <abbr title='Resource Description Framework'>RDF</abbr> storage technique that uses <em>k2-tree</em>
structures to the data, which results in high compression rates.
These structures allow <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries to be evaluated in memory without decompressing the structures.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/chapter/10.1007/978-3-319-23826-5_11">RDFCSA</a> <span class="references">[<a href="#ref-52">52</a>]</span> is a compact <abbr title='Resource Description Framework'>RDF</abbr> storage technique.
It is a <em>self-index</em> that stores the data together with its index, which results in less storage space than raw storage.
Furthermore, it is built on the concept of <em>compressed suffix arrays</em>,
which compresses text while still allowing efficient pattern-based search on it.
RDFCSA requires about twice the storage space compared to K2-Triples, but it is faster for most queries.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-53">53</a>]</span> is a binary <abbr title='Resource Description Framework'>RDF</abbr> representation that is highly compressed
and provides indexing structures that enable efficient querying.
It consists of three main components:</p>

        <dl>
          <dt>Header</dt>
          <dd>metadata describing the dataset</dd>
          <dt>Dictionary</dt>
          <dd>mapping between triple components and unique IDs for reducing storage requirements of triples</dd>
          <dt>Storage</dt>
          <dd>actual triples based on the IDs of the triple components</dd>
        </dl>

        <p>The dictionary component encodes triple components in four subsets.
The first subset consists of triple components that exist both as subject and objects.
The second and third subset respectively consists of the non-common subject and object component.
The last subset consists of the predicate components.
The storage part encodes triple components using the dictionary,
compacts the triples in a sorted predicate and object adjacency list,
and stores these adjacency list in a bitmap structure that efficiently
indicates the borders of these consecutive adjacency list.
By default, <abbr title='Header Dictionary Triples'>HDT</abbr> only stores triples in the SPO-order.
When querying is required, enhanced triple indexes are constructed
to allow any triple pattern to be resolved efficiently based on the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007%2F978-3-642-30284-8_36.pdf">HDT-FoQ</a> <span class="references">[<a href="#ref-54">54</a>]</span> approach.
<abbr title='Header Dictionary Triples'>HDT</abbr> archives are read-only, which leads to high efficiency and compressibility,
but makes them unsuitable for cases where datasets change frequently.
Its fast triple pattern queries and high compression rate make it
an appropriate backend storage method for <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">TPF</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> servers.
Approaches like <a property="schema:citation http://purl.org/spar/cito/cites" href="http://lodlaundromat.org/pdf/lodlaundry.pdf">LOD Laundromat</a> <span class="references">[<a href="#ref-55">55</a>]</span> combine <abbr title='Header Dictionary Triples'>HDT</abbr> and <abbr title='Triple Pattern Fragments'>TPF</abbr> for hosting and publishing
650K+ Linked Datasets containing 38B+ triples, proving its usefulness at large scale.
Because of these reasons, we will reuse <abbr title='Header Dictionary Triples'>HDT</abbr> snapshots as part of our storage solution.</p>

        <h4 id="storing_related-work-archiving">RDF Archiving</h4>

        <p>Linked Open Datasets typically <a property="schema:citation http://purl.org/spar/cito/cites" href="http://events.linkeddata.org/ldow2010/papers/ldow2010_paper12.pdf">change over time</a> <span class="references">[<a href="#ref-36">36</a>]</span>,
creating a need for <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">maintaining the history of the datasets</a> <span class="references">[<a href="#ref-35">35</a>]</span>.
Hence, <abbr title='Resource Description Framework'>RDF</abbr> archiving has been an active area of research over the last couple of years.
In the domain of non-RDF graph databases, several graph database extensions exist.
These extensions are either <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/datablend/fluxgraph">wrapper-based</a> <span class="references">[<a href="#ref-56">56</a>, <a href="#ref-57">57</a>]</span>, which leads to sub-optimal querying due to the lack of indexing,
or they are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">changing the graph model</a> <span class="references">[<a href="#ref-58">58</a>, <a href="#ref-59">59</a>]</span>, which complicates the writing of queries.
Furthermore, none of the existing non-RDF graph stores offer native versioning capabilities at the time of writing.
We therefore only discuss <abbr title='Resource Description Framework'>RDF</abbr> archiving for the remainder of this section.</p>

        <p>Fernández et al. formally define an <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><em>RDF archive</em></a> <span class="references">[<a href="#ref-60">60</a>]</span> as follows:
<em>An <abbr title='Resource Description Framework'>RDF</abbr> archive graph A is a set of version-annotated triples.</em>
Where a <em>version-annotated triple</em> <em>(s, p, o):[i]</em> is defined as <em>an <abbr title='Resource Description Framework'>RDF</abbr> triple (s, p, o) with a label i ∈ N representing the version in which this triple holds.</em>
The set of all <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF triples</a> <span class="references">[<a href="#ref-2">2</a>]</span> is defined as <em>(U ∪ B) × U × (U ∪ B ∪ L)</em>,
where <em>U</em>, <em>B</em>, and <em>L</em>, respectively represent the disjoint, infinite sets of URIs, blank nodes, and literals.
Furthermore,
<em>an <abbr title='Resource Description Framework'>RDF</abbr> version of an <abbr title='Resource Description Framework'>RDF</abbr> archive A at snapshot i is the <abbr title='Resource Description Framework'>RDF</abbr> graph A(i) = {(s, p, o)|(s, p, o):[i] ∈ A}.</em>
For the remainder of this article, we use the notation <em>V<sub>i</sub></em> to refer to the <abbr title='Resource Description Framework'>RDF</abbr> version <em>A(i)</em>.</p>

        <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://export.arxiv.org/pdf/1504.01891">DIACHRON data model</a> <span class="references">[<a href="#ref-37">37</a>]</span> introduces the concept of <em>diachronic datasets</em>,
i.e., datasets that contain diachronic entities, which are semantic entities that evolve over time.
This data model formally defines a diachronic dataset as a set of dataset versions together with metadata annotations about this dataset.
Each dataset version is defined as a set of records (i.e., tuples or triples), an associated schema,
temporal information about this version and metadata specific to this version.
Domain data must be reified in order to store it in the DIACHRON model.
Due to the simplicity of <abbr title='Resource Description Framework'>RDF</abbr> archive model compared to the domain-specific DIACHRON data model,
we will use the model of Fernández et al. for the remainder of this document.</p>

        <p>Systems for archiving Linked Open Data are categorized 
into <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">three non-orthogonal storage strategies</a> <span class="references">[<a href="#ref-35">35</a>]</span>:</p>

        <ul>
          <li>The <strong>Independent Copies (IC)</strong> approach creates separate instantiations of datasets for
each change or set of changes.</li>
          <li>The <strong>Change-Based (CB)</strong> approach instead only stores change sets between versions.</li>
          <li>The <strong>Timestamp-Based (TB)</strong> approach stores the temporal validity of facts.</li>
        </ul>

        <p>In the following sections, we discuss several existing <abbr title='Resource Description Framework'>RDF</abbr> archiving systems, which use either pure <abbr title='Independent Copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> or <abbr title='Timestamp-based'>TB</abbr>, or hybrid IC/CB.
<a href="#storing_rdf-archive-systems">Table 5</a> shows an overview of the discussed systems.</p>

        <figure id="storing_rdf-archive-systems" class="table">

          <table>
            <thead>
              <tr>
                <th>Name</th>
                <th>IC</th>
                <th>CB</th>
                <th>TB</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://core.ac.uk/display/15108588">SemVersion</a> <span class="references">[<a href="#ref-61">61</a>]</span></td>
                <td>✓</td>
                <td> </td>
                <td> </td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/b5e3/220451f41a55d4eb3790414bf46dad2175b4.pdf">Cassidy et. al.</a> <span class="references">[<a href="#ref-62">62</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-01.pdf">R&amp;WBase</a> <span class="references">[<a href="#ref-63">63</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/187e/60acfcc687b21c2a8887626b1e28d19f03aa.pdf">R43ples</a> <span class="references">[<a href="#ref-64">64</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/ed73/3deb78dfffd53279ec3eb768477646204236.pdf">Hauptman et. al.</a> <span class="references">[<a href="#ref-65">65</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/R22.pdf">X-RDF-3X</a> <span class="references">[<a href="#ref-66">66</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf">RDF-TX</a> <span class="references">[<a href="#ref-67">67</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="http://lbd.udc.es/Repository/Publications/Drafts/1521360121882_dcc16_150.pdf">v-RDFCSA</a> <span class="references">[<a href="#ref-68">68</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_02.pdf">Dydra</a> <span class="references">[<a href="#ref-69">69</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">TailR</a> <span class="references">[<a href="#ref-70">70</a>]</span></td>
                <td>✓</td>
                <td>✓</td>
                <td> </td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 5:</span> Overview of <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions with their corresponding storage strategy:
Individual copies (IC), Change-based (CB), or Timestamp-based (TB).</p>
          </figcaption>
        </figure>

        <h5 id="independent-copies-approaches">Independent copies approaches</h5>
        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://core.ac.uk/display/15108588">SemVersion</a> <span class="references">[<a href="#ref-61">61</a>]</span> was one of the first works to look into tracking different versions of <abbr title='Resource Description Framework'>RDF</abbr> graphs.
SemVersion is based on Concurrent Versions System (CVS) concepts to maintain different versions of ontologies,
such as diff, branching and merging.
Their approach consists of a separation of language-specific features with ontology versioning from general features together with <abbr title='Resource Description Framework'>RDF</abbr> versioning.
Unfortunately, the implementation details on triple storage and retrieval are unknown.</p>

        <h5 id="change-based-approaches">Change-based approaches</h5>
        <p>Based on the Theory of Patches from the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://darcs.net">Darcs software management system</a> <span class="references">[<a href="#ref-71">71</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/b5e3/220451f41a55d4eb3790414bf46dad2175b4.pdf">Cassidy et. al.</a> <span class="references">[<a href="#ref-62">62</a>]</span> propose to store changes to graphs as a series of patches, which makes it a <abbr title='Change-based'>CB</abbr> approach.
They describe operations on versioned graphs such as reverse, revert and merge.
An implementation of their approach is provided using the Redland python library and MySQL
by representing each patch as named graphs and serializing them in <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/trig/">TriG</a> <span class="references">[<a href="#ref-72">72</a>]</span>.
Furthermore, a preliminary evaluation shows that their implementation is significantly slower
than a native <abbr title='Resource Description Framework'>RDF</abbr> store. They suggest a native implementation of the approach to avoid some of the overhead.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.worldscientific.com/doi/abs/10.1142/S0218194012500040">Im et. al.</a> <span class="references">[<a href="#ref-73">73</a>]</span> propose a <abbr title='Change-based'>CB</abbr> patching system based on a relational database.
In their approach, they use a storage scheme called <em>aggregated deltas</em>
which associates the latest version with each of the previous ones.
While aggregated deltas result in fast delta queries, they introduce much storage overhead.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-01.pdf">R&amp;WBase</a> <span class="references">[<a href="#ref-63">63</a>]</span> is a <abbr title='Change-based'>CB</abbr> versioning system that adds an additional versioning layer to existing quad-stores.
It adds the functionality of tagging, branching and merging for datasets.
The graph element is used to represent the additions and deletions of patches,
which are respectively the even and uneven graph IDs.
Queries are resolved by looking at the highest even graph number of triples.</p>

        <p>Graube et. al. introduce <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/187e/60acfcc687b21c2a8887626b1e28d19f03aa.pdf">R43ples</a> <span class="references">[<a href="#ref-64">64</a>]</span> which stores change sets as separate named graphs, making it a <abbr title='Change-based'>CB</abbr> system.
It supports the same versioning features as R&amp;WBase and introduces new <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> keywords for these, such as REVISION, BRANCH and TAG.
As reconstructing a version requires combining all change sets that came before,
queries at a certain version are only usable for medium-sized datasets.</p>

        <h5 id="timestamp-based-approaches">Timestamp-based approaches</h5>
        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/ed73/3deb78dfffd53279ec3eb768477646204236.pdf">Hauptman et. al. introduce a similar delta-based storage approach</a> <span class="references">[<a href="#ref-65">65</a>]</span>
by storing each triple in a different named graph as a <abbr title='Timestamp-based'>TB</abbr> storage approach.
The identifying graph of each triple is used in a commit graph for <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation at a certain version.
Their implementation is based on <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_28">Sesame</a> <span class="references">[<a href="#ref-74">74</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.taylorfrancis.com/books/e/9780429102455/chapters/10.1201/b16859-17">Blazegraph</a> <span class="references">[<a href="#ref-75">75</a>]</span> and is slower than snapshot-based approaches, but uses less disk space.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/R22.pdf">X-RDF-3X</a> <span class="references">[<a href="#ref-66">66</a>]</span> is an extension of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://sites.fas.harvard.edu/~cs265/papers/neumann-2008.pdf">RDF-3X</a> <span class="references">[<a href="#ref-39">39</a>]</span> which adds versioning support using the <abbr title='Timestamp-based'>TB</abbr> approach.
On storage-level, each triple is annotated with a creation and deletion timestamp.
This enables time-travel queries where only triples valid at the given time are returned.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf">RDF-TX</a> <span class="references">[<a href="#ref-67">67</a>]</span> is an in-memory query engine that supports a temporal <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> querying extension.
The system is based on compressed multi-version B+Trees that outperforms similar systems such as X-RDF-3X in terms of querying efficiency.
The required storage space after indexing is similar to that of X-RDF-3X.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://lbd.udc.es/Repository/Publications/Drafts/1521360121882_dcc16_150.pdf">v-RDFCSA</a> <span class="references">[<a href="#ref-68">68</a>]</span> is a self-indexing <abbr title='Resource Description Framework'>RDF</abbr> archive mechanism,
based on the <abbr title='Resource Description Framework'>RDF</abbr> self-index <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/chapter/10.1007/978-3-319-23826-5_11">RDFCSA</a> <span class="references">[<a href="#ref-52">52</a>]</span>,
that enables versioning queries on top of compressed <abbr title='Resource Description Framework'>RDF</abbr> archives as a <abbr title='Timestamp-based'>TB</abbr> approach.
They evaluate their approach using the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">BEAR</a> <span class="references">[<a href="#ref-60">60</a>]</span> benchmark
and show that they can reduce storage space requirements 60 times compared to raw storage.
Furthermore, they reduce query evaluation times more than an order of magnitude compared to state of the art solutions.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_02.pdf">Dydra</a> <span class="references">[<a href="#ref-69">69</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> graph storage platform with dataset versioning support.
They introduce the REVISION keyword, which is similar to the GRAPH <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> keyword for referring to different dataset versions.
Their implementation is based on B+Trees that are indexed in six ways  GSPO, GPOS, GOSP, SPOG, POSG, OSPG.
Each B+Tree value indicates the revisions in which a particular quad exists, which makes it a <abbr title='Timestamp-based'>TB</abbr> approach.</p>

        <h5 id="hybrid-approaches">Hybrid approaches</h5>
        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">TailR</a> <span class="references">[<a href="#ref-70">70</a>]</span> is an <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> archive for Linked Data pages based
on the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://arxiv.org/pdf/0911.1112.pdf">Memento protocol</a> <span class="references">[<a href="#ref-42">42</a>]</span> for retrieving prior versions of certain <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> resources.
It is a hybrid CB/IC approach as it starts by storing a dataset snapshot,
after which only deltas are stored for each consecutive version, as shown in <a href="#storing_regular-delta-chain">Fig. 15</a>.
When the chain becomes too long, or other conditions are fulfilled,
a new snapshot is created for the next version to avoid long version reconstruction times.</p>

        <p>Results show that this is an effective way of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">reducing version reconstruction times</a> <span class="references">[<a href="#ref-70">70</a>]</span>,
in particular for many versions.
Within the delta chain, however, an increase in version reconstruction times can still be observed.
Furthermore, it requires more storage space than pure delta-based approaches.</p>

        <p>The authors’ implementation is based on a relational database system.
Evaluation shows that resource lookup times for any version ranges between
1 and 50 ms for 10 versions containing around 500K triples.
In total, these versions require ~64MB of storage space.</p>

        <figure id="storing_regular-delta-chain">
<img src="storing/img/regular-delta-chain.svg" alt="[regular delta chain]" />
<figcaption>
            <p><span class="label">Fig. 15:</span> Delta chain in which deltas are relative to the previous delta, as is done in <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">TailR</a> <span class="references">[<a href="#ref-70">70</a>]</span>.</p>
          </figcaption>
</figure>

        <h4 id="related-work-benchmarks">RDF Archiving Benchmarks</h4>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">BEAR</a> <span class="references">[<a href="#ref-60">60</a>]</span> is a benchmark for <abbr title='Resource Description Framework'>RDF</abbr> archive systems.
The <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> benchmark is based on three real-world datasets from different domains:</p>

        <dl>
          <dt>BEAR-A</dt>
          <dd>58 weekly snapshots from the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://events.linkeddata.org/ldow2010/papers/ldow2010_paper12.pdf">Dynamic Linked Data Observatory</a> <span class="references">[<a href="#ref-36">36</a>]</span>. This is the main dataset from the article on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">BEAR</a> <span class="references">[<a href="#ref-60">60</a>]</span>.</dd>
          <dt>BEAR-B</dt>
          <dd>The 100 most volatile resources from <a property="schema:citation http://purl.org/spar/cito/cites" href="http://jens-lehmann.org/files/2012/program_el_dbpedia_live.pdf">DBpedia Live</a> <span class="references">[<a href="#ref-76">76</a>]</span> over the course of three months
as three different granularities: instant, hour and day.</dd>
          <dt>BEAR-C</dt>
          <dd>Dataset descriptions from the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://aic.ai.wu.ac.at/~polleres/publications/neum-etal-2016JDIQ.pdf">Open Data Portal Watch</a> <span class="references">[<a href="#ref-77">77</a>]</span> project over the course of 32 weeks.</dd>
        </dl>

        <p>The 58 versions of BEAR-A contain between 30M and 66M triples per version, with an average change ratio of 31%.
BEAR-A provides triple pattern queries for three different versioned query types for both result sets with a low and a high cardinality.
The queries are selected in such a way that they will be evaluated over triples of a certain dynamicity,
which requires the benchmarked systems to handle this dynamicity well.
BEAR-B provides a small collection of triple pattern queries corresponding to the real-world usage of DBpedia.
Finally, BEAR-C provides 10 complex <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries that were created with the help of Open Data experts.</p>

        <p>BEAR provides baseline <abbr title='Resource Description Framework'>RDF</abbr> archive implementations based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-53">53</a>]</span> and
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://ieeexplore.ieee.org/iel5/4236/22924/01067737.pdf">Jena’s</a> <span class="references">[<a href="#ref-78">78</a>]</span> <a href="https://jena.apache.org/documentation/tdb/">TDB store</a>
for the <abbr title='Independent Copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr>, and <abbr title='Timestamp-based'>TB</abbr> approaches, but also hybrid IC/CB and TB/CB approaches.
The hybrid approaches are based on snapshots followed by delta chains, as implemented by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">TailR</a> <span class="references">[<a href="#ref-70">70</a>]</span>.
Due to <abbr title='Header Dictionary Triples'>HDT</abbr> not supporting quads, the <abbr title='Timestamp-based'>TB</abbr> and TB/CB approaches could not be implemented in the <abbr title='Header Dictionary Triples'>HDT</abbr> baseline implementations.</p>

        <p>Results show that <abbr title='Independent Copies'>IC</abbr> for both Jena and <abbr title='Header Dictionary Triples'>HDT</abbr> requires more storage space than the compressed deltas for the three datasets.
<abbr title='Change-based'>CB</abbr> results in less storage space for both approaches for BEAR-A and BEAR-B, but not for BEAR-C because that dataset is so dynamic that
the deltas require more storage space than they would in with <abbr title='Independent Copies'>IC</abbr>.
Jena-TB results in the least storage space of Jena-based approaches,
however,
it fails for BEAR-B-instant because of the large amount of versions
as Jena is less efficient for many graphs.</p>

        <p>The hybrid approaches are evaluated with different delta chain lengths and expectedly show
that shorter delta chains lead to results similar to <abbr title='Independent Copies'>IC</abbr>, and longer delta chains lead are similar to <abbr title='Change-based'>CB</abbr> or <abbr title='Timestamp-based'>TB</abbr>.
The queries for BEAR-A and BEAR-B show that
<abbr title='Independent Copies'>IC</abbr> results in constant evaluation times for any version,
<abbr title='Change-based'>CB</abbr> times increase for each following version,
and <abbr title='Timestamp-based'>TB</abbr> also result in constant times.
The HDT-based approaches outperform Jena in all cases because of its compressed nature.
The IC/CB hybrid approaches similarly show increasing evaluation times for each version,
with a drop each time a new snapshot is created.
The IC/TB hybrid Jena approach has slowly increasing evaluation times for each version,
but they are significantly lower than the regular <abbr title='Timestamp-based'>TB</abbr> approach.</p>

        <p>The queries of BEAR-C currently can not be solved by the archiving strategies in a straightforward way,
but they are designed to help foster the development of future <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions.
While queries of BEAR-A and BEAR-B are just triple pattern queries and therefore do not cover the full <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> spectrum,
they provide the basis for more complex queries, as is proven by the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">TPF framework</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>,
which makes them sufficient for benchmarking.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_03.pdf">EvoGen</a> <span class="references">[<a href="#ref-79">79</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> archive systems benchmark that is based on the synthetic <a property="schema:citation http://purl.org/spar/cito/cites" href="http://swat.cse.lehigh.edu/pubs/guo05a.pdf">LUBM dataset generator</a> <span class="references">[<a href="#ref-6">6</a>]</span>.
It is an extension of the <abbr title='Lehigh University Benchmark'>LUBM</abbr> generator with additional classes and properties for introducing dataset evolution on schema-level.
EvoGen enables the user to tweak parameters of the dataset and query generation process,
for example to change the dataset dynamicity and the number of versions.</p>

        <p>While EvoGen offers more flexibility than <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> in terms of configurability.
<abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> provides real-world datasets and baseline implementations which lowers the barrier towards its usage.
Hence, we will use the <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> dataset in this work for benchmarking our system.</p>

        <h4 id="query-atoms">Query atoms</h4>

        <p>The query atoms that will be introduced in this section are based on
the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF data model</a> <span class="references">[<a href="#ref-2">2</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>.
In these models, a <em>triple pattern</em> is defined as <em>(U ∪ V) × (U ∪ V) × (U ∪ L ∪ V)</em>, with <em>V</em> being the infinite set of variables.
A set of triple patterns is called a <em>Basic Graph Pattern</em>, which forms the basis of a <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query.
The evaluation of a <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query <em>Q</em> on an <abbr title='Resource Description Framework'>RDF</abbr> graph <em>G</em> containing <abbr title='Resource Description Framework'>RDF</abbr> triples,
produces a bag of solution mappings <em>[[Q]]<sub>G</sub></em>.</p>

        <p>To cover the retrieval demands in <abbr title='Resource Description Framework'>RDF</abbr> archiving,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">five foundational query types were introduced</a> <span class="references">[<a href="#ref-60">60</a>]</span>,
which are referred to as <em>query atoms</em>:</p>

        <ol>
          <li><strong>Version materialization (VM)</strong> retrieves data using a query <em>Q</em> targeted at a single version <em>V<sub>i</sub></em>.
Formally: <em>VM(Q, V<sub>i</sub>) = [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were present in the library yesterday?</em></li>
          <li><strong>Delta materialization (DM)</strong> retrieves query <em>Q</em>’s result change sets between two versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em>DM(Q, V<sub>i</sub>, V<sub>j</sub>)=(Ω<sup>+</sup>, Ω<sup>−</sup>). With Ω<sup>+</sup> = [[Q]]<sub>V<sub>i</sub></sub> \ [[Q]]<sub>V<sub>j</sub></sub> and Ω<sup>−</sup> = [[Q]]<sub>V<sub>j</sub></sub> \ [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were returned or taken from the library between yesterday and now?</em></li>
          <li><strong>Version query (VQ)</strong> annotates query <em>Q</em>’s results with the versions (of <abbr title='Resource Description Framework'>RDF</abbr> archive A) in which they are valid.
Formally: <em>VQ(Q, A) = {(Ω, W) | W = {A(i) | Ω=[[Q]]<sub>A(i)</sub>, i ∈ N} ∧ Ω ≠ ∅}</em>.
Example: <em>At what times was book X present in the library?</em></li>
          <li><strong>Cross-version join (CV)</strong> joins the results of two queries (<em>Q1</em> and <em>Q2</em>) between versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em>VM(Q1, V<sub>i</sub>) ⨝ VM(Q2, V<sub>j</sub>)</em>.
Example: <em>What books were present in the library yesterday and today?</em></li>
          <li><strong>Change materialization (CM)</strong> returns a list of versions in which a given query <em>Q</em> produces
consecutively different results.
Formally: <em>{(i, j) | i,j ∈ ℕ, i &lt; j, DM(Q, A(i), A(j)) = (Ω<sup>+</sup>, Ω<sup>−</sup>), Ω<sup>+</sup> ∪ Ω<sup>−</sup> ≠ ∅, ∄ k ∈ ℕ : i &lt; k &lt; j}</em>.
Example: <em>At what times was book X returned or taken from the library?</em></li>
        </ol>

        <p>There exists a correspondence between these query atoms
and the independent copies (IC), change-based (CB), and timestamp-based (TB) storage strategies.</p>

        <p>Namely, <abbr title='Version Materialization'>VM</abbr> queries are efficient in storage solutions that are based on <abbr title='Independent Copies'>IC</abbr>, because there is indexing on version.
On the other hand, IC-based solutions may introduce a large amount of overhead in terms of storage space because each version is stored separately.
Furthermore, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> queries are less efficient for <abbr title='Independent Copies'>IC</abbr> solutions.
That is because <abbr title='Delta Materialization'>DM</abbr> queries require two fully-materialized versions to be compared on-the-fly,
and <abbr title='Version Query'>VQ</abbr> requires <em>all</em> versions to be queried at the same time.</p>

        <p>DM queries can be efficient in <abbr title='Change-based'>CB</abbr> solutions if the query version ranges correspond to the stored delta ranges.
In all other cases, as well as for <abbr title='Version Materialization'>VM</abbr> and <abbr title='Version Query'>VQ</abbr> queries, the desired versions must be materialized on-the-fly,
which will take increasingly more time for longer delta chains.
<abbr title='Change-based'>CB</abbr> solutions do however typically require less storage space than <abbr title='Version Materialization'>VM</abbr> if there is sufficient overlap between each consecutive version.</p>

        <p>Finally, <abbr title='Version Query'>VQ</abbr> queries perform well for <abbr title='Timestamp-based'>TB</abbr> solutions because the timestamp annotation directly corresponds to VQ’s result format.
<abbr title='Version Materialization'>VM</abbr> and <abbr title='Delta Materialization'>DM</abbr> queries in this case are typically less efficient than for <abbr title='Independent Copies'>IC</abbr> approaches, due to the missing version index.
Furthermore, <abbr title='Timestamp-based'>TB</abbr> solutions can require less storage space compared to <abbr title='Version Materialization'>VM</abbr> if the change ratio of the dataset is not too large.</p>

        <p>In summary, <abbr title='Independent Copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches can perform well for certain query types, but they can be slow for others.
On the other hand, this efficiency typically comes at the cost of a large storage overhead, as is the case for IC-based approaches.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://export.arxiv.org/pdf/1504.01891">DIACHRON QL</a> <span class="references">[<a href="#ref-37">37</a>]</span> is a <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query language extension
based on the DIACHRON data model that provides functionality similar to these query atoms
in order to query specific versions, changesets, or all versions.</p>

      </div>
</section>

    <section id="storing_problem-statement" inlist="" rel="schema:hasPart" resource="#storing_problem-statement">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Problem statement</h3>

        <p>As mentioned in <a href="#storing_introduction">Section 3.1</a>, no <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions exist that allow
efficient triple pattern querying <em>at</em>, <em>between</em>, and <em>for</em> different versions,
in combination with a scalable <em>storage model</em> and efficient <em>compression</em>.
In the context of query engines, streams are typically used to return query results,
on which offsets and limits can be applied to reduce processing time if only a subset is needed.
Offsets are used to skip a certain amount of elements,
while limits are used to restrict the number of elements to a given amount.
As such, <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions should also allow query results to be returned as offsettable streams.
The ability to achieve such stream subsets is limited in existing solutions.</p>

        <p>This leads us to the following research question:</p>

        <div rel="schema:question">
          <blockquote id="storing_researchquestion" about="#storing_researchquestion" property="schema:name">
            <p>How can we store <abbr title='Resource Description Framework'>RDF</abbr> archives to enable efficient <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> triple pattern queries with offsets?</p>
          </blockquote>
        </div>

        <p>The focus of this article is evaluating version materialization (VM), delta materialization (DM), and version (VQ) queries efficiently,
as <abbr title='Cross-version Join'>CV</abbr> and <abbr title='Change Materialization'>CM</abbr> queries can be expressed in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">terms of the other ones</a> <span class="references">[<a href="#ref-80">80</a>]</span>.
In total, our research question indentifies the following requirements:</p>

        <ul>
          <li>an efficient <abbr title='Resource Description Framework'>RDF</abbr> archive storage technique;</li>
          <li>VM, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> triple pattern querying algorithms on top of this storage technique;</li>
          <li>efficient offsetting of the <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr>, and <abbr title='Version Query'>VQ</abbr> query result streams.</li>
        </ul>

        <p>In this work, we lower query evaluation times by processing and storing more metadata during ingestion time.
Instead of processing metadata during every lookup, this happens only once per version.
This will increase ingestion times, but will improve the efficiency of performance-critical features
within query engines and Linked Data interfaces, such as querying with offsets.
To this end, we introduce the following hypotheses:</p>

        <div rel="lsc:tests">
          <ol>
            <li id="storing_hypothesis-qualitative-querying" about="#storing_hypothesis-qualitative-querying" property="schema:name">Our approach shows no influence of the selected versions on the querying efficiency of <abbr title='Version Materialization'>VM</abbr> and <abbr title='Delta Materialization'>DM</abbr> triple pattern queries.</li>
            <li id="storing_hypothesis-qualitative-ic-storage" about="#storing_hypothesis-qualitative-ic-storage" property="schema:name">Our approach requires <em>less</em> storage space than state-of-the-art IC-based approaches.</li>
            <li id="storing_hypothesis-qualitative-ic-querying" about="#storing_hypothesis-qualitative-ic-querying" property="schema:name">For our approach, querying is <em>slower</em> for <abbr title='Version Materialization'>VM</abbr> and <em>equal</em> or <em>faster</em> for <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> than in state-of-the-art IC-based approaches.</li>
            <li id="storing_hypothesis-qualitative-cb-storage" about="#storing_hypothesis-qualitative-cb-storage" property="schema:name">Our approach requires <em>more</em> storage space than state-of-the-art CB-based approaches.</li>
            <li id="storing_hypothesis-qualitative-cb-querying" about="#storing_hypothesis-qualitative-cb-querying" property="schema:name">For our approach, querying is <em>equal</em> or <em>faster</em> than in state-of-the-art CB-based approaches.</li>
            <li id="storing_hypothesis-qualitative-ingestion" about="#storing_hypothesis-qualitative-ingestion" property="schema:name">Our approach reduces average query time compared to other non-IC approaches at the cost of increased ingestion time.</li>
          </ol>
        </div>

      </div>
</section>

    <section id="storing_fundamentals" inlist="" rel="schema:hasPart" resource="#storing_fundamentals">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Overview of Approaches</h3>

        <p>In this section, we lay the groundwork for the following sections.
We introduce fundamental concepts
that are required in our storage approach and its accompanying querying algorithms,
which will be explained in <a href="#storing_storage">Section 3.5</a> and <a href="#storing_querying">Section 3.7</a>, respectively.</p>

        <p>To combine smart use of storage space with efficient processing of <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr>, and <abbr title='Version Query'>VQ</abbr> triple pattern queries,
we employ a hybrid approach between the individual copies (IC), change-based (CB), and timestamp-based (TB) storage techniques (as discussed in <a href="#storing_related-work">Section 3.2</a>).
In summary, intermittent <em>fully materialized snapshots</em> are followed by <em>delta chains</em>.
Each delta chain is stored in <em>six tree-based indexes</em>, where values are dictionary-encoded and timestamped
to reduce storage requirements and lookup times.
These six indexes correspond to the combinations for storing three triple component orders
separately for additions and deletions.
The indexes for the three different triple component orders
ensure that any triple pattern query can be resolved quickly.
The additions and deletions are stored separately
because access patterns to additions and deletions in deltas differ between <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr>, and <abbr title='Version Query'>VQ</abbr> queries.
To efficiently support inter-delta <abbr title='Delta Materialization'>DM</abbr> queries, each addition and deletion value contains a <em>local change</em> flag
that indicates if the change is not relative to the snapshot.
Finally, in order to provide cardinality estimation for any triple pattern,
we store an additional count data structure.</p>

        <p>In the following sections, we discuss the most important distinguishing features of our approach.
We elaborate on the novel hybrid IC/CB/TB storage technique that our approach is based on,
the reason for using multiple indexes,
having local change metadata,
and methods for storing addition and deletion counts.</p>

        <h4 id="storing_snapshot-delta-chain">Snapshot and Delta Chain</h4>

        <p>Our storage technique is partially based on a hybrid IC/CB approach similar to <a href="#storing_regular-delta-chain">Fig. 15</a>.
To avoid increasing reconstruction times,
we construct the delta chain in an <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.worldscientific.com/doi/abs/10.1142/S0218194012500040">aggregated deltas</a> <span class="references">[<a href="#ref-73">73</a>]</span> fashion:
each delta is <em>independent</em> of a preceding delta and relative to the closest preceding snapshot in the chain, as shown in <a href="#storing_alternative-delta-chain">Fig. 16</a>.
Hence, for any version, reconstruction only requires at most one delta and one snapshot.
Although this does increase possible redundancies within delta chains,
due to each delta <em>inheriting</em> the changes of its preceding delta,
the overhead can be compensated with compression, which we discuss in <a href="#storing_storage">Section 3.5</a>.</p>

        <figure id="storing_alternative-delta-chain">
<img src="storing/img/alternative-delta-chain.svg" alt="[alternative delta chain]" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 16:</span> Delta chain in which deltas are relative to the snapshot at the start of the chain, as part of our approach.</p>
          </figcaption>
</figure>

        <h4 id="storing_indexes">Multiple Indexes</h4>

        <p>Our storage approach consists of six different indexes that are used for separately storing additions and deletions
in three different triple component orders, namely: <code>SPO</code>, <code>POS</code> and <code>OSP</code>.
These indexes are B+Trees, thereby, the starting triple for any triple pattern can be found in logarithmic time.
Consequently, the next triples can be found by iterating through the links between each tree leaf.
<a href="#triple-pattern-index-mapping"></a> shows an overview of which triple patterns can be mapped to which index.
In contrast to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://sites.fas.harvard.edu/~cs265/papers/neumann-2008.pdf">other approaches</a> <span class="references">[<a href="#ref-39">39</a>, <a href="#ref-49">49</a>]</span> that ensure certain triple orders,
we use three indexes instead of all six possible component orders,
because we only aim to reduce the iteration scope of the lookup tree for any triple pattern.
For each possible triple pattern,
we now have an index that locates the first triple component in logarithmic time,
and identifies the terminating element of the result stream without necessarily having iterate to the last value of the tree.
For some scenarios, it might be beneficial to ensure the order of triples in the result stream,
so that more efficient stream joining algorithms can be used, such as sort-merge join.
If this would be needed, <code>OPS</code>, <code>PSO</code> and <code>SOP</code> indexes could optionally be added
so that all possible triple orders would be available.</p>

        <figure id="storing_triple-pattern-index-mapping" class="table">

          <table>
            <thead>
              <tr>
                <th>Triple pattern</th>
                <th><code>SPO</code></th>
                <th><code>SP?</code></th>
                <th><code>S?O</code></th>
                <th><code>S??</code></th>
                <th><code>?PO</code></th>
                <th><code>?P?</code></th>
                <th><code>??O</code></th>
                <th><code>???</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>OSTRICH</strong></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>OSP</code></td>
                <td><code>SPO</code></td>
                <td><code>POS</code></td>
                <td><code>POS</code></td>
                <td><code>OSP</code></td>
                <td><code>SPO</code></td>
              </tr>
              <tr>
                <td><strong>HDT-FoQ</strong></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>OPS</code></td>
                <td><code>PSO</code></td>
                <td><code>OPS</code></td>
                <td><code>SPO</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 6:</span> Overview of which triple patterns are queried inside which index in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='HDT Focus on Querying'>HDT-FoQ</abbr>.</p>
          </figcaption>
        </figure>

        <p>Our approach could also act as a dedicated <abbr title='Resource Description Framework'>RDF</abbr> archiving solution
without (necessarily efficient) querying capabilities.
In this case, only a single index would be required, such as <code>SPO</code>, which would reduce the required storage space even further.
If querying would become required afterwards,
the auxiliary <code>OSP</code> and <code>POS</code> indexes could still be derived from this main index
during a one-time, pre-querying processing phase.</p>

        <p>This technique is similar to the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007%2F978-3-642-30284-8_36.pdf">HDT-FoQ</a> <span class="references">[<a href="#ref-54">54</a>]</span> extension for <abbr title='Header Dictionary Triples'>HDT</abbr> that adds additional indexes to a basic <abbr title='Header Dictionary Triples'>HDT</abbr> file
to enable faster querying for any triple pattern.
The main difference is that <abbr title='HDT Focus on Querying'>HDT-FoQ</abbr> uses the indexes <code>OSP</code>, <code>PSO</code> and <code>OPS</code>,
with a different triple pattern to index mapping as shown in <a href="#storing_triple-pattern-index-mapping">Table 6</a>.
We chose our indexes in order to achieve a more balanced distribution from triple patterns to index,
which could lead to improved load balancing between indexes when queries are parallelized.
<abbr title='HDT Focus on Querying'>HDT-FoQ</abbr> uses <code>SPO</code> for five triple pattern groups, <code>OPS</code> for two and <code>PSO</code> for only a single group.
Our approach uses <code>SPO</code> for 4 groups, <code>POS</code> for two and <code>OSP</code> for two.
Future work is needed to evaluate the distribution for real-world queries.
Additionally, the mapping from patterns <code>S?O</code> to index <code>SPO</code> in <abbr title='HDT Focus on Querying'>HDT-FoQ</abbr> will lead to suboptimal query evaluation
when a large number of distinct predicates is present.</p>

        <h4 id="storing_local-changes">Local Changes</h4>

        <p>A delta chain can contain multiple instances of the same triple,
since it could be added in one version and removed in the next.
Triples that revert a previous addition or deletion within the same delta chain, are called <em>local changes</em>,
and are important for query evaluation.
Determining the locality of changes can be costly,
thus we pre-calculate this information during ingestion time and store it for each versioned triple,
so that this does not have to happen during query-time.</p>

        <p>When evaluating version materialization queries by combining a delta with its snapshot,
all local changes should be filtered out.
For example, a triple <code>A</code> that was deleted in version 1, but re-added in version 2,
is cancelled out when materializing against version 2.
For delta materialization, these local changes should be taken into account,
because triple <code>A</code> should be marked as a deletion between versions 0 and 1,
but as an addition between versions 1 and 2.
Finally, for version queries, this information is also required
so that the version ranges for each triple can be determined.</p>

        <h4 id="storing_addition-deletion-counts">Addition and Deletion counts</h4>

        <p>Parts of our querying algorithms depend on the ability to efficiently count
the <em>exact</em> number of additions or deletions in a delta.
Instead of naively counting triples by iterating over all of them,
we propose two separate approaches for enabling efficient addition and deletion counting in deltas.</p>

        <p>For additions, we store an additional mapping from triple pattern and version to number of additions
so that counts can happen in constant time by just looking them up in the map.
For deletions, we store additional metadata in the main deletions tree.
Both of these approaches will be further explained in <a href="#storing_storage">Section 3.5</a>.</p>

      </div>
</section>

    <section id="storing_storage" inlist="" rel="schema:hasPart" resource="#storing_storage">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Hybrid Multiversion Storage</h3>

        <p>In this section, we introduce our hybrid IC/CB/TB storage approach for storing multiple versions of an <abbr title='Resource Description Framework'>RDF</abbr> dataset.
<a href="#storing_storage-overview">Fig. 17</a> shows an overview of the main components.
Our approach consists of an initial dataset snapshot—stored in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-53">53</a>]</span>—followed by a delta chain (similar to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf">TailR</a> <span class="references">[<a href="#ref-70">70</a>]</span>).
The delta chain uses multiple compressed B+Trees for a TB-storage strategy (similar to <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_02.pdf">Dydra</a> <span class="references">[<a href="#ref-69">69</a>]</span>),
applies dictionary-encoding to triples, and
stores additional metadata to improve lookup times.
In this section, we discuss each component in more detail.
In the next section, we describe two ingestion algorithms based on this storage structure.</p>

        <figure id="storing_storage-overview">
<img src="storing/img/storage-overview.svg" alt="[storage overview]" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 17:</span> Overview of the main components of our hybrid IC/CB/TB storage approach.</p>
          </figcaption>
</figure>

        <p>Throughout this section, we will use the example <abbr title='Resource Description Framework'>RDF</abbr> archive from <a href="#storing_example-archive">Table 7</a>
to illustrate the different storage components with.</p>

        <figure id="storing_example-archive" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right">Version</th>
                <th>Triple</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right">0</td>
                <td><code>:Bob foaf:name "Bobby"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">1</td>
                <td><code>:Alice foaf:name "Alice"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">1</td>
                <td><code>:Bob foaf:name "Bobby"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">2</td>
                <td><code>:Bob foaf:name "Bob"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">3</td>
                <td><code>:Alice foaf:name "Alice"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">3</td>
                <td><code>:Bob foaf:name "Bob"</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 7:</span> Example of a small <abbr title='Resource Description Framework'>RDF</abbr> archive with 4 versions.
We assume that the following URI prefixes: <code>: http://example.org</code>, <code>foaf: http://xmlns.com/foaf/0.1/</code></p>
          </figcaption>
        </figure>

        <h4 id="storing_snapshot-storage">Snapshot storage</h4>

        <p>As mentioned before, the start of each delta chain is a fully materialized snapshot.
In order to provide sufficient efficiency for <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> querying with respect to all versions in the chain,
we assume the following requirements for the snapshot storage:</p>

        <ul>
          <li>Any triple pattern query <em>must</em> be resolvable as triple streams.</li>
          <li>Offsets <em>must</em> be applicable to the result stream of any triple pattern query.</li>
          <li>Cardinality estimation for all triple pattern queries <em>must</em> be possible.</li>
        </ul>

        <p>These requirements are needed for ensuring the efficiency of the querying algorithms that will be introduced in <a href="#querying">Chapter 4</a>.
For the implementation of snapshots,
existing techniques such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-53">53</a>]</span> fulfill all the requirements.
Therefore,
we do not introduce a new snapshot approach, but use <abbr title='Header Dictionary Triples'>HDT</abbr> in our implementation.
This will be explained further in <a href="#storing_implementation">Subsection 3.8.1</a>.</p>

        <h4 id="storing_dictionary">Delta Chain Dictionary</h4>

        <p>A common technique in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">RDF indexes</a> <span class="references">[<a href="#ref-53">53</a>, <a href="#ref-39">39</a>, <a href="#ref-50">50</a>]</span> is to use a dictionary for mapping triple components to numerical IDs.
This is done for three main reasons:
1) reduce storage space if triple components are stored multiple times;
2) reducing I/O overhead when retrieving data; and
3) simplify and optimize querying.
As our storage approach essentially stores each triple three or six times,
a dictionary can definitely reduce storage space requirements.</p>

        <p>Each delta chain consists of two dictionaries, one for the snapshot and one for the deltas.
The snapshot dictionary consists of triple components that already existed in the snapshot.
All other triple components are stored in the delta dictionary.
This dictionary is shared between the additions and deletions,
as the dictionary ignores whether or not the triple is an addition or deletion.
How this distinction is made will be explained in <a href="#storing_delta-storage">Subsection 3.5.3</a>.
The snapshot dictionary can be optimized and sorted, as it will not change over time.
The delta dictionary is volatile, as each new version can introduce new mappings.</p>

        <p>During triple encoding (i.e., ingestion), the snapshot dictionary will always first be probed for existence of the triple component.
If there is a match, that ID is used for storing the delta’s triple component.
To identify the appropriate dictionary for triple decoding,
a reserved bit is used where <code>1</code> indicates snapshot dictionary
and <code>0</code> indicates the delta dictionary.
The text-based dictionary values can be compressed to reduce storage space further, as they are likely to contain many redundancies.</p>

        <p><a href="#storing_example-delta-storage-dict">Table 8</a> contains example encodings of the triple components.</p>

        <figure id="storing_example-delta-storage-dict" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right"><code>:Bob</code></th>
                <th style="text-align: right"><code>foaf:name</code></th>
                <th style="text-align: right"><code>"Bobby"</code></th>
                <th style="text-align: right"><code>:Alice</code></th>
                <th style="text-align: right"><code>"Alice"</code></th>
                <th style="text-align: right"><code>"Bob"</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right"><code>S0</code></td>
                <td style="text-align: right"><code>S1</code></td>
                <td style="text-align: right"><code>S2</code></td>
                <td style="text-align: right"><code>D0</code></td>
                <td style="text-align: right"><code>D1</code></td>
                <td style="text-align: right"><code>D2</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 8:</span> Example encoding of the triple components from <a href="#storing_example-archive">Table 7</a>.
Instead of the reserved bit, IDs prefixed with <code>S</code> belong to the snapshot dictionary
and those prefixed with <code>D</code> belong to the delta dictionary.</p>
          </figcaption>
        </figure>

        <h4 id="storing_delta-storage">Delta Storage</h4>

        <p>In order to cope with the newly introduced redundancies in our delta chain structure,
we introduce a delta storage method similar to the <abbr title='Timestamp-based'>TB</abbr> storage strategy,
which is able to compress redundancies within consecutive deltas.
In contrast to a regular <abbr title='Timestamp-based'>TB</abbr> approach, which stores plain timestamped triples,
we store timestamped triples annotated with a flag for addition or deletion.
An overview of this storage technique is shown in <a href="#storing_delta-storage-overview">Fig. 18</a>,
which will be explained in detail hereafter.</p>

        <figure id="storing_delta-storage-overview">
<img src="storing/img/delta-storage-overview.svg" alt="[delta storage overview]" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 18:</span> Overview of the components for storing a delta chain.
The value structure for the addition and deletion trees are indicated with the dashed nodes.</p>
          </figcaption>
</figure>

        <p>The additions and deletions of deltas require different metadata in our querying algorithms,
which will be explained in <a href="#storing_querying">Section 3.7</a>.
Additions and deletions are respectively stored in separate stores,
which hold all additions and deletions from the complete delta chain.
Each store uses B+Tree data structures,
where a key corresponds to a triple and the value contains version information.
The version information consists of a mapping from version to a local change flag as mentioned in <a href="#storing_local-changes">Subsection 3.4.3</a> and,
in case of deletions, also the relative position of the triple inside the delta.
Even though triples can exist in multiple deltas in the same chain,
they will only be stored once.
Each addition and deletion store uses three trees with a different triple component order (SPO, POS and OSP),
as discussed in <a href="#storing_indexes">Subsection 3.4.2</a>.</p>

        <p>The relative position of each triple inside the delta to the deletion trees speeds up the process
of patching a snapshot’s triple pattern subset for any given offset.
In fact, seven relative positions are stored for each deleted triple: one for each possible triple pattern (<code>SP?</code>, <code>S?O</code>, <code>S??</code>, <code>?PO</code>, <code>?P?</code>, <code>??O</code>, <code>???</code>),
except for <code>SPO</code> since this position will always be 0 as each triple is stored only once.
This position information serves two purposes:
1) it allows the querying algorithm to exploit offset capabilities of the snapshot store
to resolve offsets for any triple pattern against any version;
and 2) it allows deletion counts for any triple pattern and version to be determined efficiently.
The use of the relative position and the local change flag during querying will be further explained in <a href="#querying">Chapter 4</a>.</p>

        <figure id="storing_example-delta-storage" class="table">

          <table>
            <thead>
              <tr>
                <th>+</th>
                <th style="text-align: right">V</th>
                <th>L</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><code>D0 S1 D1</code></td>
                <td style="text-align: right">1</td>
                <td>F</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">3</td>
                <td>F</td>
              </tr>
              <tr>
                <td><code>S0 S1 D2</code></td>
                <td style="text-align: right">2</td>
                <td>F</td>
              </tr>
            </tbody>
          </table>

          <table>
            <thead>
              <tr>
                <th>-</th>
                <th style="text-align: right">V</th>
                <th>L</th>
                <th style="text-align: right"><code>SP?</code></th>
                <th style="text-align: right"><code>S?O</code></th>
                <th style="text-align: right"><code>S??</code></th>
                <th style="text-align: right"><code>?PO</code></th>
                <th style="text-align: right"><code>?P?</code></th>
                <th style="text-align: right"><code>??O</code></th>
                <th style="text-align: right"><code>???</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><code>D0 S1 D1</code></td>
                <td style="text-align: right">2</td>
                <td>T</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
              </tr>
              <tr>
                <td><code>S0 S1 S2</code></td>
                <td style="text-align: right">2</td>
                <td>F</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">1</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">1</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">3</td>
                <td>F</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 9:</span> Addition and deletion tree contents based on the example from <a href="#storing_example-archive">Table 7</a> using the dictionary encoding from <a href="#storing_example-delta-storage-dict">Table 8</a>.
Column <code>+</code> and <code>-</code> respectively represent the keys of the addition and deletion trees, which contains triples based on the encoded triple components.
The remaining columns represent the values, i.e., a mapping from version (<code>V</code>) to the local change flag (<code>L</code>).
For the deletion trees, values also include the relative positions for all essential triple patterns.</p>
          </figcaption>
        </figure>

        <p><a href="#storing_example-delta-storage">Table 9</a> represent
the addition and deletion tree contents when the triples from the example in <a href="#storing_example-archive">Table 7</a> are stored.
The local change flag is enabled for <code>D0 S1 D1</code> in the deletions tree for version 2, as it was previously added in version 1.
The relative positions in the deletion tree for <code>S0 S1 S2</code> is not the same for versions 2 and 3,
because in version 2, the triple <code>D0 S1 D1</code> also exists as a deletion, and when sorted, this comes before <code>S0 S1 S2</code> for triple patterns <code>?P?</code> and <code>???</code>.</p>

        <h4 id="storing_addition-counts">Addition Counts</h4>

        <p>As mentioned before in <a href="#storing_addition-deletion-counts">Subsection 3.4.4</a>,
in order to make the counting of matching addition triples for any triple pattern for any version more efficient,
we propose to store an additional mapping from triple pattern and version to the number of matching additions.
Furthermore, for being able to retrieve the total number of additions across all versions,
we also propose to store this value for all triple patterns.
This mapping must be calculated during ingestion time, so that counts during lookup time for any triple pattern
at any version can be derived in constant time.
For many triples and versions, the number of possible triple patterns can become very large,
which can result in a large mapping store.
To cope with this, we propose to only store the elements where their counts are larger than a certain threshold.
Elements that are not stored will have to be counted during lookup time.
This is however not a problem for reasonably low thresholds,
because the iteration scope in our indexes can be limited efficiently, as mentioned in <a href="#storing_addition-deletion-counts">Subsection 3.4.4</a>.
The count threshold introduces a trade-off between the storage requirements and the required triple counting during lookups.</p>

        <h4 id="storing_deletion-counts">Deletion Counts</h4>

        <p>As mentioned in <a href="#storing_delta-storage">Subsection 3.5.3</a>, each deletion is annotated with its relative position in all deletions for that version.
This position is exploited to perform deletion counting for any triple pattern and version.
We look up the largest possible triple (sorted alphabetically) for the given triple pattern in the deletions tree,
which can be done in logarithmic time by navigating in the tree to the largest possible match for the given triple pattern.
If this does not result in a match for the triple pattern, no matches exist for the given triple pattern, and the count is zero.
Otherwise, we take one plus the relative position of the matched deletion for the given triple pattern.
Because we have queried the largest possible triple for that triple pattern in the given version,
this will be the last deletion in the list, so this position corresponds to the total number of deletions in that case.</p>

        <p>For example, when we want to determine the deletion count for <code>? foaf:name ?</code> (encoded: <code>? S1 ?</code>) in version 2
using the deletion tree contents from <a href="#storing_example-delta-storage">Table 9</a>,
we will find <code>S0 S1 S2</code> as largest triple in version 2.
This triple has relative position <code>1</code> for <code>?P?</code>, so the total deletion count is <code>2</code> for this pattern.
This is correct, as we have indeed two triples matching this pattern, namely <code>D0 S1 D1</code> and <code>S0 S1 S2</code>.</p>

        <h4 id="storing_metadata">Metadata</h4>

        <p>Querying algorithms have to be able to detect the total number of versions across all delta chains.
Therefore,
we must store metadata regarding the delta chain version ranges.
Assuming that version identifiers are numerical, a mapping can be maintained from version ID to delta chain.
Additionally, a counter of the total number of versions must be maintained for when the last version must be identified.</p>

      </div>
</section>

    <section id="storing_ingestions" inlist="" rel="schema:hasPart" resource="#storing_ingestions">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Changeset Ingestion Algorithms</h3>

        <p>In this section, we discuss two ingestion algorithms: a memory-intensive batch algorithm and a memory-efficient streaming algorithm.
These algorithms both take a changeset—containing additions and deletions—as input,
and append it as a new version to the store.
Note that the ingested changesets are regular changesets: they are relative to one another according to <a href="#storing_regular-delta-chain">Fig. 15</a>.
Furthermore, we assume that the ingested changesets are <em>valid</em> changesets:
they don’t contain impossible triple sequences such as a triple that is removed in two versions without having an addition in between.
During ingestion, they will be transformed to the alternative delta chain structure as shown in <a href="#storing_alternative-delta-chain">Fig. 16</a>.
Within the scope of this article, we only discuss ingestion of deltas in a single delta chain following a snapshot.</p>

        <p>Next to ingesting the added and removed triples,
an ingestion algorithm for our storage approach must be able to calculate
the appropriate metadata for the store as discussed in <a href="#storing_delta-storage">Subsection 3.5.3</a>.
More specifically, an ingestion algorithm has the following requirements:</p>
        <ul>
    <li>addition triples must be stored in all addition trees;</li>
    <li>additions and deletions must be annotated with their version;</li>
    <li>additions and deletions must be annotated with being a local change or not;</li>
    <li>deletions must be annotated with their relative position for all triple patterns.</li>
</ul>

        <h4 id="storing_batch-ingestion">Batch Ingestion</h4>

        <p>Our first algorithm to ingest data into the store naively loads everything in memory,
and inserts the data accordingly.
The advantage of this algorithm is its simplicity and the possibility to do straightforward optimizations during ingestion.
The main disadvantage is the high memory consumption requirement for large versions.</p>

        <p>Before we discuss the actual batch ingestion algorithm,
we first introduce an in-memory changeset merging algorithm,
which is required for the batch ingestion.
<a href="#storing_algorithm-ingestion-batch-merge">Algorithm 2</a> contains the pseudocode of this algorithm.
First, all contents of the original changeset are copied into the new changeset (line 3).
After that, we iterate over all triples of the second changeset (line 4).
If the changeset already contained the given triple (line 5), the local change flag is negated.
Otherwise, the triple is added to the new changeset, and the local change flag is set to <code>false</code> (line 9,10).
Finally, in both cases the addition flag of the triple in the new changeset is copied from the second changeset (line 12).</p>

        <figure id="storing_algorithm-ingestion-batch-merge" class="algorithm numbered">
<pre><code>mergeChangesets(changesetOriginal, changesetIngest) {
</code><code>  changesetNew = new Changeset()
</code><code>  changesetNew.addAll(changesetOriginal)
</code><code>  for (triple : changesetIngest.getTriples()) {
</code><code>    if (changesetOriginal.contains(triple)) {
</code><code>      localChange = !changesetOriginal.isLocalChange(triple)
</code><code>      changesetNew.setLocalChange(triple, localChange)
</code><code>    } else {
</code><code>      changesetNew.add(triple)
</code><code>      changesetNew.setLocalChange(triple, false)
</code><code>    }
</code><code>    changesetNew.setAddition(triple, changesetIngest.isAddition(triple))
</code><code>  }
</code><code>  return changesetNew
</code><code>}</code></pre>
<figcaption>
            <p><span class="label">Algorithm 2:</span> In-memory changeset merging algorithm</p>
          </figcaption>
</figure>

        <p>Because our querying algorithms require the relative position of each deletion within a changeset to be stored,
we have to calculate these positions during ingestion.
We do this using the helper function <code>calculatePositions(triple)</code>.
This function depends on external mappings that persist over the duration of the ingestion phase
that map from triple to a counter for each possible triple pattern.
When this helper function is called for a certain triple,
we increment the counters for the seven possible triple patterns of the triple.
For the triple itself, we do not maintain a counter, as its value is always 1.
Finally, the function returns a mapping for the current counter values of the seven triple patterns.</p>

        <p>The batch ingestion algorithm starts by reading a complete changeset stream in-memory, sorting it in SPO order,
and encoding all triple components using the dictionary.
After that, it loads the changeset from the previous version in memory,
which is required for merging it together with the new changeset using the algorithm from <a href="#storing_algorithm-ingestion-batch-merge">Algorithm 2</a>.
After that, we have the new changeset loaded in memory.
Now, we load each added triple into the addition trees, together with their version and local change flag.
After that, we load each deleted triple into the deletion trees
with their version, local change flag and relative positions.
These positions are calculated using <code>calculatePositions(triple)</code>.
For the sake of completeness, we included the batch algorithm in pseudo-code in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-algorithms" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​algorithms">Appendix D</a>.</p>

        <p>Even though this algorithm is straightforward,
it can require a large amount of memory for large changesets and long delta chains.
The theoretical time complexity of this algorithm is <code>O(P + N log(N))</code> (<code>O(P + N)</code> if the new changeset is already sorted),
with <code>P</code> the number of triples in the previous changeset,
and <code>N</code> the number of triples in the new changeset.</p>

        <h4 id="storing_streaming-ingestion">Streaming Ingestion</h4>

        <p>Because of the unbounded memory requirements of the <a href="#storing_batch-ingestion">batch ingestion algorithm</a>,
we introduce a more complex streaming ingestion algorithm.
Just like the batch algorithm, it takes a changeset stream as input,
with the additional requirement that the stream’s values must be sorted in SPO-order.
This way the algorithm can assume a consistent order and act as a sort-merge join operation.
Just as for the batch algorithm, we included this algorithm in pseudo-code in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-algorithms" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​algorithms">Appendix D</a>.</p>

        <p>In summary, the algorithm performs a sort-merge join over three streams in SPO-order:
1) the stream of <em>input</em> changeset elements that are encoded using the dictionary when each element is read,
2) the existing <em>deletions</em> over all versions
and 3) the existing <em>additions</em> over all versions.
The algorithm iterates over all streams together, until all of them are finished.
The smallest triple (string-based) over all stream heads is handled in each iteration,
and can be categorized in seven different cases where these stream heads are indicated by <em>input</em>, <em>deletion</em> and <em>addition</em>, respectively:</p>

        <ol>
<li>
            <p><strong><em>Deletion</em> is strictly smaller than both <em>input</em> and <em>addition</em>.</strong>
<br />
The current deletion is the smallest element.
The unchanged deletion information can be copied to the new version.
New relative positions must be calculated in this and all other cases where deletions are added.</p>
          </li>
<li>
            <p><strong><em>Addition</em> is strictly smaller than both <em>input</em> and <em>deletion</em>.</strong>
<br />
Similar to the previous case, the current addition is now the smallest element,
and its information can be copied to the new version.</p>
          </li>
<li>
            <p><strong><em>Input</em> is strictly smaller than both <em>addition</em> and <em>deletion</em>.</strong>
<br />
A triple is added or removed that was not present before,
so it can respectively be added as a non-local change addition or a non-local change deletion.</p>
          </li>
<li>
            <p><strong><em>Input</em> and <em>deletion</em> are equal, but strictly smaller than <em>addition</em>.</strong>
<br />
In this case, the new triple already existed in the previous version as a deletion.
If the new triple is an addition, it must be added as a local change.</p>
          </li>
<li>
            <p><strong><em>Input</em> and <em>addition</em> are equal, but strictly smaller than <em>deletion</em>.</strong>
<br />
Similar as in the previous case, the new triple now already existed as an addition.
So the triple must be deleted as a local change if the new triple is a deletion.</p>
          </li>
<li>
            <p><strong><em>Addition</em> and <em>deletion</em> are equal, but strictly smaller than <em>input</em>.</strong>
<br />
The triple existed as both an addition and deletion at some point.
In this case, we copy over the one that existed at the latest version, as it will still apply in the new version.</p>
          </li>
<li>
            <p><strong><em>Addition</em>,  <em>deletion</em>, and <em>input</em> are equal.</strong>
<br />
Finally, the triple already existed as both an addition and deletion,
and is equal to our new triple.
This means that if the triple was an addition in the previous version, it becomes a deletion, and the other way around,
and the local change flag can be inherited.</p>
          </li>
</ol>

        <p>The theoretical memory requirement for this algorithm is much lower than the <a href="#storing_batch-ingestion">batch variant</a>.
That is because it only has to load at least three triples, i.e., the heads of each stream, in memory, instead of the complete new changeset.
Furthermore, we still need to maintain the relative position counters for the deletions in all triple patterns.
While these counters could also become large, a smart implementation could perform memory-mapping
to avoid storing everything in memory.
The lower memory requirements come at the cost of a higher logical complexity, but an equal time complexity (assuming sorted changesets).</p>

      </div>
</section>

    <section id="storing_querying" inlist="" rel="schema:hasPart" resource="#storing_querying">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Versioned Query Algorithms</h3>

        <p>In this section, we introduce algorithms for performing <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> triple pattern queries
based on the storage structure introduced in <a href="#storing_storage">Section 3.5</a>.
Each of these querying algorithms are based on result streams, enabling efficient offsets and limits,
by exploiting the index structure from <a href="#storing_storage">Section 3.5</a>.
Furthermore, we provide algorithms to provide count estimates for each query.</p>

        <h4 id="version-materialization">Version Materialization</h4>

        <p>Version Materialization (VM) is the most straightforward versioned query type,
it allows you to query against a certain dataset version.
In the following, we start by introducing our <abbr title='Version Materialization'>VM</abbr> querying algorithm,
after we give a simple example of this algorithm.
After that, we prove the correctness of our <abbr title='Version Materialization'>VM</abbr> algorithm and introduce a corresponding algorithm to provide count estimation for <abbr title='Version Materialization'>VM</abbr> query results.</p>

        <h5 id="query">Query</h5>

        <p><a href="#storing_algorithm-querying-vm">Algorithm 3</a> introduces an algorithm for <abbr title='Version Materialization'>VM</abbr> triple pattern queries based on our storage structure.
It starts by determining the snapshot on which the given version is based (line 2).
After that, this snapshot is queried for the given triple pattern and offset.
If the given version is equal to the snapshot version, the snapshot iterator can be returned directly (line 3).
In all other cases, this snapshot offset could only be an estimation,
and the actual snapshot offset can be larger if deletions were introduced before the actual offset.</p>

        <p>Our algorithm returns a stream where triples originating from the snapshot always
come before the triples that were added in later additions.
Because of that, the mechanism for determining the correct offset in the
snapshot, additions and deletions streams can be split up into two cases.
The given offset lies within the range of either snapshot minus deletion triples or within the range of addition triples.
At this point, the additions and deletions streams are initialized to the start position for the given triple pattern and version.</p>

        <figure id="storing_algorithm-querying-vm" class="algorithm numbered">
<pre><code>queryVm(store, tp, version, originalOffset) {
</code><code>  snapshot = store.getSnapshot(version).query(tp, originalOffset)
</code><code>  if (snapshot.getVersion() = version) {
</code><code>    return snapshot
</code><code>  }
</code><code>  
</code><code>  additions = store.getAdditionsStream(tp, version)
</code><code>  deletions = store.getDeletionStream(tp, version)
</code><code>  offset = 0
</code><code>  
</code><code>  if (originalOffset &lt; snapshot.count(tp) - deletions.exactCount(tp)) {
</code><code>    do {
</code><code>      snapshot.offset(originalOffset + offset)
</code><code>      offsetTriple = snapshot.peek()
</code><code>      deletions = store.getDeletionsStream(tp, version, offsetTriple)
</code><code>      offset = deletions.getOffset(tp)
</code><code>    } while (snapshot.getCurrentOffset() != originalOffset + offset)
</code><code>  }
</code><code>  else {
</code><code>    snapshot.offset(snapshot.count(tp))
</code><code>    additions.offset(originalOffset - snapshot.count(tp)
</code><code>        + deletions.exactCount(tp))
</code><code>  }
</code><code>  
</code><code>  return PatchedSnapshotIterator(snapshot, deletions, additions)
</code><code>}</code></pre>
<figcaption>
            <p><span class="label">Algorithm 3:</span> Version Materialization algorithm for triple patterns that produces a triple stream with an offset in a given version.</p>
          </figcaption>
</figure>

        <p>In the first case, when the offset lies within the snapshot and deletions range (line 11),
we enter a loop that converges to the actual snapshot offset based on the deletions
for the given triple pattern in the given version.
This loop starts by determining the triple at the current offset position in the snapshot (line 13, 14).
We then query the deletions tree for the given triple pattern and version (line 15),
filter out local changes, and use the snapshot triple as offset.
This triple-based offset is done by navigating through the tree to the smallest triple before or equal to the offset triple.
We store an additional offset value (line 16), which corresponds to the current numerical offset inside the deletions stream.
As long as the current snapshot offset is different from the sum of the original offset and the additional offset,
we continue iterating this loop (line 17), which will continuously increase this additional offset value.</p>

        <p>In the second case (line 19), the given offset lies within the additions range.
Now, we terminate the snapshot stream by offsetting it after its last element (line 20),
and we relatively offset the additions stream (line 21).
This offset is calculated as the original offset subtracted with the number of snapshot triples incremented with the number of deletions.</p>

        <p>Finally, we return a simple iterator starting from the three streams (line 25).
This iterator performs a sort-merge join operation that removes each triple from the snapshot that also appears in the deletion stream,
which can be done efficiently because of the consistent <code>SPO</code>-ordering.
Once the snapshot and deletion streams have finished,
the iterator will start emitting addition triples at the end of the stream.
For all streams, local changes are filtered out because locally changed triples
are cancelled out for the given version as explained in <a href="#storing_local-changes">Subsection 3.4.3</a>,
so they should not be returned in materialized versions.</p>

        <h5 id="example">Example</h5>

        <p>We can use the deletion’s position in the delta as offset in the snapshot
because this position represents the number of deletions that came before that triple inside the snapshot given a consistent triple order.
<a href="#storing_query-vm-example">Table 10</a> shows simplified storage contents where triples are represented as a single letter,
and there is only a single snapshot and delta.
In the following paragraphs, we explain the offset convergence loop of the algorithm in function of this data for different offsets.</p>

        <figure id="storing_query-vm-example" class="table">

          <table>
            <thead>
              <tr>
                <th>Snapshot</th>
                <th>A</th>
                <th>B</th>
                <th>C</th>
                <th>D</th>
                <th>E</th>
                <th>F</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Deletions</td>
                <td> </td>
                <td>B</td>
                <td> </td>
                <td>D</td>
                <td>E</td>
                <td> </td>
              </tr>
              <tr>
                <td>Positions</td>
                <td> </td>
                <td>0</td>
                <td> </td>
                <td>1</td>
                <td>2</td>
                <td> </td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 10:</span> Simplified storage contents example where triples are represented as a single letter.
The snapshot contains six elements, and the next version contains three deletions.
Each deletion is annotated with its position.</p>
          </figcaption>
        </figure>

        <h6 id="offset-0"><em>Offset 0</em></h6>
        <p>For offset zero, the snapshot is first queried for this offset,
which results in a stream starting from <code>A</code>.
Next, the deletions are queried with offset <code>A</code>, which results in no match,
so the final snapshot stream starts from <code>A</code>.</p>

        <h6 id="offset-1"><em>Offset 1</em></h6>
        <p>For an offset of one, the snapshot stream initially starts from <code>B</code>.
After that, the deletions stream is offset to <code>B</code>, which results in a match.
The original offset (1), is increased with the position of <code>B</code> (0) and the constant 1,
which results in a new snapshot offset of 2.
We now apply this new snapshot offset.
As the snapshot offset has changed, we enter a second iteration of the loop.
Now, the head of the snapshot stream is <code>C</code>.
We offset the deletions stream to <code>C</code>, which again results in <code>B</code>.
As this offset results in the same snapshot offset,
we stop iterating and use the snapshot stream with offset 2 starting from <code>C</code>.</p>

        <h6 id="offset-2"><em>Offset 2</em></h6>
        <p>For offset 2, the snapshot stream initially starts from <code>C</code>.
After querying the deletions stream, we find <code>B</code>, with position 0.
We update the snapshot offset to 2 + 0 + 1 = 3,
which results in the snapshot stream with head <code>D</code>.
Querying the deletions stream results in <code>D</code> with position 1.
We now update the snapshot offset to 2 + 1 + 1 = 4, resulting in a stream with head <code>E</code>.
We query the deletions again, resulting in <code>E</code> with position 2.
Finally, we update the snapshot offset to 2 + 2 + 1 = 5 with stream head <code>F</code>.
Querying the deletions results in the same <code>E</code> element,
so we use this last offset in our final snapshot stream.</p>

        <h5 id="estimated-count">Estimated count</h5>

        <p>In order to provide an estimated count for <abbr title='Version Materialization'>VM</abbr> triple pattern queries,
we introduce a straightforward algorithm that depends on the efficiency of the snapshot to provide count estimations for a given triple pattern.
Based on the snapshot count for a given triple pattern, the number of deletions for that version and triple pattern
are subtracted and the number of additions are added.
These last two can be resolved efficiently, as we precalculate
and store expensive addition and deletion counts as explained in <a href="#storing_addition-counts">Subsection 3.5.4</a> and <a href="#storing_deletion-counts">Subsection 3.5.5</a>.</p>

        <h5 id="correctness">Correctness</h5>

        <p>In this section, we provide a proof that <a href="#storing_algorithm-querying-vm">Algorithm 3</a> results in the correct stream offset
for any given version and triple pattern. We do this by first introducing a set of notations,
followed by several lemmas and corollaries, which lead up to our final theorem proof.</p>

        <p><strong>Notations</strong>:</p>

        <p>We will make use of bracket notation to indicate lists (ordered sets):</p>

        <ul>
          <li><code>A[i]</code> is the element at position <code>i</code> from the list <code>A</code>.</li>
          <li><code>A + B</code> is the concatenation of list <code>A</code> followed by list <code>B</code>.</li>
        </ul>

        <p>Furthermore, we will use the following definitions:</p>

        <ul>
          <li><code>snapshot(tp, version)</code> is the ordered list of triples matching the given triple pattern <code>tp</code> in the corresponding snapshot, from here on shortened to <code>snapshot</code>.</li>
          <li><code>additions(version)</code> and <code>deletions(version)</code> are the corresponding ordered additions and deletions for the given version, from here on shortened to <code>additions</code> and <code>deletions</code>.</li>
          <li><code>originalOffset</code> is how much the versioned list should be shifted, from here on shortened to <code>ori</code>.</li>
          <li><code>PatchedSnapshotIterator(snapshot, deletions, additions)</code> is a function that returns the list <code>snapshot\deletions + additions</code>.</li>
        </ul>

        <p>The following definitions correspond to elements from the loop on lines 12-17:</p>

        <ul>
          <li><code>deletions(x)</code> is the ordered list <code>{d | d ∈ deletions, d ≥ x}</code>, with <code>x</code> a triple.</li>
          <li><code>offset(x) = |deletions| - |deletions(x)|</code>, with <code>x</code> a triple.</li>
          <li><code>t(i)</code> is the triple generated at line 13-14 for iteration <code>i</code>.</li>
          <li><code>off(i)</code> is the offset generated at line 16 for iteration <code>i</code>.</li>
        </ul>

        <p><strong>Lemma 1</strong>: <code>off(n) ≥ off(n-1)</code><br />
<em>Proof</em>:<br />
We prove this by induction over the iterations of the loop.
For <code>n=1</code> this follows from line 9 and <code>∀ x offset(x) ≥ 0.</code></p>

        <p>For <code>n+1</code> we know by induction that <code>off(n) ≥ off(n-1)</code>.
Since <code>snapshot</code> is ordered, <code>snapshot[ori + off(n)] ≥ snapshot[ori + off(n-1)]</code>.
From lines 13-14 follows that <code>t(n) = snapshot[ori + off(n-1)]</code>,
together this gives <code>t(n+1) ≥ t(n)</code>.</p>

        <p>From this, we get:</p>

        <ul>
          <li><code>{d | d ∈ deletions, d ≥ t(n+1)} ⊆ {d | d ∈ deletions, d ≥ t(n)}</code></li>
          <li><code>deletions(t(n+1)) ⊆ deletions(t(n))</code></li>
          <li><code>|deletions(t(n+1))| ≤ |deletions(t(n))|</code></li>
          <li><code>|deletions| - |deletions(t(n+1))| ≥ |deletions| - |deletions(t(n))|</code></li>
          <li><code>offset(t(n+1)) ≥ offset(t(n))</code></li>
        </ul>

        <p>Together with lines 15-16 this gives us <code>off(n+1) ≥ off(n)</code>.</p>

        <p><strong>Corollary 1</strong>: The loop on lines 12-17 always terminates.<br />
<em>Proof</em>:<br />
Following the definitions, the end condition of the loop is <code>ori + off(n) = ori + off(n+1)</code>.
From Lemma 1 we know that <code>off</code> is a non-decreasing function.
Since <code>deletions</code> is a finite list of triples, there is an upper limit for <code>off</code> (<code>|deletions|</code>),
causing <code>off</code> to stop increasing at some point which triggers the end condition.</p>

        <p><strong>Corollary 2</strong>: When the loop on lines 12-17 terminates, <code>offset = |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> and <code>ori + offset &lt; |snapshot|</code><br />
<em>Proof</em>:<br />
The first part follows from the definition of <code>deletions</code> and <code>offset</code>.
The second part follows from <code>offset ≤ |deletions|</code> and line 11.</p>

        <p><strong>Theorem 1</strong>: queryVm returns a sublist of <code>(snapshot\deletions + additions)</code>, starting at the given offset.<br />
<em>Proof</em>:<br />
If the given version is equal to a snapshot, there are no additions or deletions so this follows directly from lines 2-4.</p>

        <p>Following the definition of <code>deletions</code>, <code>∀ x ∈ deletions: x ∈ snapshot</code> and thus <code>|snapshot\deletions| = |snapshot| - |deletions|</code>.</p>

        <p>Due to the ordered nature of <code>snapshot</code> and <code>deletions</code>, if <code>ori &lt; |snapshot\deletions|</code>, version<code>[ori] = snapshot[ori + |D|]</code> with <code>D = {d | d ∈ deletions, d &lt; snapshot[ori + |D|]}</code>.
Due to <code>|snapshot\deletions| = |snapshot| - |deletions|</code>, this corresponds to the if-statement on line 11.
From Corollary 1 we know that the loop terminates
and from Corollary 2 and line 13 that snapshot points to the element at position
<code>ori + |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> which,
together with <code>additions</code> starting at index 0 and line 25,
returns the requested result.</p>

        <p>If <code>ori ≥ |snapshot\deletions|</code>, <code>version[ori] = additions[ori - |snapshot\deletions|]</code>.
From lines 20-22 follows that <code>snapshot</code> gets emptied and <code>additions</code> gets shifted for the remaining required elements <code>(ori - |snapshot\deletions|)</code>, which then also returns the requested result on line 25.</p>

        <h4 id="delta-materialization">Delta Materialization</h4>

        <p>The goal of delta materialization (DM) queries is to query the triple differences between two versions.
Furthermore, each triple in the result stream is annotated with either being an addition or deletion between the given version range.
Within the scope of this work, we limit ourselves to delta materialization within a single snapshot and delta chain.
Because of this, we distinguish between two different cases for our <abbr title='Delta Materialization'>DM</abbr> algorithm
in which we can query triple patterns between a start and end version,
the start version of the query can either correspond to the snapshot version or it can come after that.
Furthermore, we introduce an equivalent algorithm for estimating the number of results for these queries.</p>

        <h5 id="query-1">Query</h5>

        <p>For the first query case, where the start version corresponds to the snapshot version,
the algorithm is straightforward.
Since we always store our deltas relative to the snapshot,
filtering the delta of the given end version based on the given triple pattern directly corresponds to the desired result stream.
Furthermore, we filter out local changes, as we are only interested in actual change with respect to the snapshot.</p>

        <p>For the second case, the start version does not correspond to the snapshot version.
The algorithm iterates over the triple pattern iteration scope of the addition and deletion trees in a sort-merge join-like operation,
and only emits the triples that have a different addition/deletion flag for the two versions.</p>

        <h5 id="estimated-count-1">Estimated count</h5>

        <p>For the first case, the start version corresponds to the snapshot version.
The estimated number of results is then the number of snapshot triples for the pattern summed up with the exact umber of deletions and additions for the pattern.</p>

        <p>In the second case the start version does not correspond to the snapshot version.
We estimate the total count as the sum of the additions and deletions for the given triple pattern in both versions.
This may only be a rough estimate, but will always be an upper bound, as the triples that were changed twice within the version range and negate each other
are also counted.
For exact counting, this number of negated triples should be subtracted.</p>

        <h4 id="version-query">Version Query</h4>

        <p>For version querying (VQ), the final query atom, we have to retrieve all triples across all versions,
annotated with the versions in which they exist.
In this work, we again focus on version queries for a single snapshot and delta chain.
For multiple snapshots and delta chains, the following algorithms can simply be applied once for each snapshot and delta chain.
In the following sections, we introduce an algorithm for performing triple pattern version queries
and an algorithm for estimating the total number of matching triples for the former queries.</p>

        <h5 id="query-2">Query</h5>

        <p>Our version querying algorithm is again based on a sort-merge join-like operation.
We start by iterating over the snapshot for the given triple pattern.
Each snapshot triple is queried within the deletion tree.
If such a deletion value can be found, the versions annotation contains all versions except for the versions
for which the given triple was deleted with respect to the given snapshot.
If no such deletion value was found, the triple was never deleted,
so the versions annotation simply contains all versions of the store.
Result stream offsetting can happen efficiently as long as the snapshot allows efficient offsets.
When the snapshot iterator is finished, we iterate over the addition tree in a similar way.
Each addition triple is again queried within the deletions tree
and the versions annotation can equivalently be derived.</p>

        <h5 id="estimated-count-2">Estimated count</h5>

        <p>Calculating the number of unique triples matching any triple pattern version query is trivial.
We simply retrieve the count for the given triple pattern in the given snapshot
and add the number of additions for the given triple pattern over all versions.
The number of deletions should not be taken into account here,
as this information is only required for determining the version annotation in the version query results.</p>

      </div>
</section>

    <section id="storing_evaluation" inlist="" rel="schema:hasPart" resource="#storing_evaluation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Evaluation</h3>

        <p>In this section, we evaluate our proposed storage technique and querying algorithms.
We start by introducing <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, an implementation of our proposed solution.
After that, we describe the setup of our experiments, followed by presenting our results.
Finally, we discuss these results.</p>

        <h4 id="storing_implementation">Implementation</h4>

        <p>OSTRICH stands for <em>Offset-enabled STore for TRIple CHangesets</em>,
and it is a software implementation of the storage and querying techniques described in this article
It is implemented in C/C++ and available on <a href="https://zenodo.org/record/883008" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883008">GitHub</a> under an open license.
In the scope of this work, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> currently supports a single snapshot and delta chain.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> uses <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-53">53</a>]</span> as snapshot technology as it conforms to all the <a href="#storing_snapshot-storage">requirements</a> for our approach.
Furthermore, for our indexes we use <a href="http://fallabs.com/kyotocabinet/" class="mandatory" data-link-text="http:/​/​fallabs.com/​kyotocabinet/​">Kyoto Cabinet</a>,
which provides a highly efficient memory-mapped B+Tree implementation with compression support.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> immediately generates the main <code>SPO</code> index and the auxiliary <code>OSP</code> and <code>POS</code> indexes.
In future work, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> could be modified to only generate the main index and delay auxiliary index generation to a later stage.
Memory-mapping is required so that not all data must be loaded in-memory when queries are evaluated,
which would not always be possible for large datasets.
For our delta dictionary, we extend HDT’s dictionary implementation with adjustments to make it work with unsorted triple components.
We compress this delta dictionary with <a href="http://www.gzip.org/">gzip</a>, which requires decompression during querying and ingestion.
Finally, for storing our addition counts, we use the Hash Database of Kyoto Cabinet, which is also memory-mapped.</p>

        <p>We provide a developer-friendly C/C++ <abbr title='Application Programming Interface'>API</abbr> for ingesting and querying data based on an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store.
Additionally, we provide command-line tools for ingesting data into an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store,
or evaluating <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> or <abbr title='Version Query'>VQ</abbr> triple pattern queries for any given limit and offset against a store.
Furthermore, we implemented <a href="https://zenodo.org/record/883010" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883010">Node JavaScript bindings</a> that
expose the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> <abbr title='Application Programming Interface'>API</abbr> for ingesting and querying to JavaScript applications.
We used these bindings to <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">expose an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store</a>
containing a dataset with 30M triples in 10 versions using <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">TPF</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>, with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">VTPF feature</a> <span class="references">[<a href="#ref-40">40</a>]</span>.</p>

        <h4 id="experimental-setup">Experimental Setup</h4>

        <p>As mentioned before in <a href="#storing_related-work-benchmarks"></a>, we evaluate our approach using the <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> benchmark.
We chose for this benchmark because it provides a complete set of tools and data for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> versioning systems,
containing datasets, queries and easy-to-use engines to compare with.</p>

        <p>We extended the existing <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> implementation for the evaluation of offsets.
We did this by implementing custom offset features into each of the <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> approaches.
Only for <abbr title='Version Materialization'>VM</abbr> queries in HDT-IC an efficient implementation (HDT-IC+) could be made because of HDT’s native offset capabilities.
In all other cases, naive offsets had to be implemented by iterating over the result stream
until a number of elements equal to the desired offset were consumed.
This modified implementation is available on <a href="https://github.com/rdfostrich/bear/tree/ostrich-eval-journal" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​bear/​tree/​ostrich-​eval-​journal">GitHub</a>.
To test the scalability of our approach for datasets with few and large versions, we use the BEAR-A benchmark.
We use the ten first versions of the BEAR-A dataset, which contains 30M to 66M triples per version.
This dataset was compiled from the <a href="http://swse.deri.org/dyldo/">Dynamic Linked Data Observatory</a>.
To test for datasets with many smaller versions, we use BEAR-B with the daily and hourly granularities.
The daily dataset contains 89 versions and the hourly dataset contains 1,299 versions,
both of them have around 48K triples per version.
We did not evaluate BEAR-B-instant, because <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires increasingly
more time for each new version ingestion, as will be shown in the next section.
As BEAR-B-hourly with 1,299 versions already takes more than three days to ingest,
the 21,046 versions from BEAR-B-instant would require too much time to ingest.
Our experiments were executed on a 64-bit
Ubuntu 14.04 machine with 128 GB of memory and a
24-core 2.40 GHz <abbr title='Central Processing Unit'>CPU</abbr>.</p>

        <p>For BEAR-A, we use all 7 of the provided querysets, each containing at most 50 triple pattern queries,
once with a high result cardinality and once with a low result cardinality.
These querysets correspond to all possible triple pattern materializations, except for triple patterns where each component is blank.
For BEAR-B, only two querysets are provided, those that correspond to <code>?P?</code> and <code>?PO</code> queries.
The number of BEAR-B queries is more limited, but they are derived from real-world DBpedia queries
which makes them useful for testing real-world applicability.
All of these queries are evaluated as <abbr title='Version Materialization'>VM</abbr> queries on all versions,
as <abbr title='Delta Materialization'>DM</abbr> between the first version and all other versions,
and as <abbr title='Version Query'>VQ</abbr>.</p>

        <p>For a complete comparison with other approaches, we re-evaluated BEAR’s Jena and HDT-based <abbr title='Resource Description Framework'>RDF</abbr> archive implementations.
More specifically, we ran all BEAR-A queries against Jena with the <abbr title='Independent Copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr>, <abbr title='Timestamp-based'>TB</abbr> and hybrid CB/TB implementation,
and <abbr title='Header Dictionary Triples'>HDT</abbr> with the <abbr title='Independent Copies'>IC</abbr> and <abbr title='Change-based'>CB</abbr> implementations
using the BEAR-A dataset for ten versions.
We did the same for BEAR-B with the daily and hourly dataset.
After that, we evaluated <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> for the same queries and datasets.
We were not able to extend this benchmark with other similar systems such as X-RDF-3X, RDF-TX and Dydra,
because the source code of systems was either not publicly available,
or the system would require additional implementation work to support the required query interfaces.</p>

        <p>Additionally, we evaluated the ingestion rates and storage sizes for all approaches.
Furthermore, we compared the ingestion rate for the two different ingestion algorithms of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
The batch-based algorithm expectedly ran out of memory for larger amounts of versions,
so we used the streaming-based algorithm for all further evaluations.</p>

        <p>Finally, we evaluated the offset capabilities of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>
by comparing it with custom offset implementations for the other approaches.
We evaluated the blank triple pattern query with offsets ranging from 2 to 4,096 with a limit of 10 results.</p>

        <h4 id="results-3">Results</h4>

        <p>In this section, we present the results of our evaluation.
We report the ingestion results, compressibility, query evaluation times for all cases and offset result.
All raw results and the scripts that were used to process them are available on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.</p>

        <h5 id="ingestion">Ingestion</h5>

        <p><a href="#storing_results-ingestion-size">Table 11</a> and <a href="#storing_results-ingestion-time">Table 12</a>
respectively show the storage requirements and ingestion times for the different approaches for the three different benchmarks.
For BEAR-A, the HDT-based approaches outperform <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in terms of ingestion time, they are about two orders of magniture faster.
Only HDT-CB requires slightly less storage space.
The Jena-based approaches ingest one order of magnitude faster than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, but require more storage space.
For BEAR-B-daily, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires less storage space than all other approaches except for HDT-CB at the cost of slower ingestion.
For BEAR-B-hourly, only HDT-CB and Jena-CB/TB require about 8 to 4 times less space than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
For BEAR-B-daily and BEAR-B-hourly, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> even requires less storage space than gzip on raw N-Triples.</p>

        <p>As mentioned in <a href="#storing_addition-counts">Subsection 3.5.4</a>, we use a threshold to define which addition count values should be stored,
and which ones should be evaluated at query time.
For our experiments, we fixed this count threshold at 200,
which has been empirically determined through various experiments as a good value.
For values higher than 200, the addition counts started having a noticable impact on the performance of count estimation.
This threshold value means that when a triple pattern has 200 matching additions,
then this count will be stored.
<a href="#storing_results-addition-counts">Table 13</a> shows that the storage space of the addition count datastructure
in the case of BEAR-A and BEAR-B-hourly is insignificant compared to the total space requirements.
However, for BEAR-B-daily, addition counts take up 37.05% of the total size with still an acceptable absolute size,
as the addition and deletion trees require relatively less space,
because of the lower amount of versions.
Within the scope of this work, we use this fixed threshold of 200.
We consider investigating the impact of different threshold levels and methods for dynamically determining optimal levels future work.</p>

        <p><a href="#storing_results-ostrich-ingestion-rate-beara">Fig. 19</a> shows linearly increasing ingestion rate for each consecutive version for BEAR-A,
while <a href="#storing_results-ostrich-ingestion-size-beara">Fig. 20</a> shows corresponding linearly increasing storage sizes.
Analogously, <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 21</a> shows the ingestion rate for BEAR-B-hourly,
which increases linearly until around version 1100, after which it increases significantly.
<a href="#storing_results-ostrich-ingestion-size-bearb-hourly">Fig. 22</a> shows near-linearly increasing storage sizes.</p>

        <p><a href="#storing_results-ostrich-ingestion-rate-beara-compare">Fig. 23</a> compares the BEAR-A ingestion rate of the streaming and batch algorithms.
The streaming algorithm starts of slower than the batch algorithm but grows linearly,
while the batch algorithm consumes a large amount of memory, resulting in slower ingestion after version 8 and an out-of-memory error after version 10.</p>

        <figure id="storing_results-ingestion-size" class="table">

          <table>
            <thead>
              <tr>
                <th>Approach</th>
                <th style="text-align: right">BEAR-A</th>
                <th style="text-align: right">BEAR-B-daily</th>
                <th style="text-align: right">BEAR-B-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Raw (N-Triples)</td>
                <td style="text-align: right">46,069.76</td>
                <td style="text-align: right">556.44</td>
                <td style="text-align: right">8,314.86</td>
              </tr>
              <tr>
                <td>Raw (gzip)</td>
                <td style="text-align: right">3,194.88</td>
                <td style="text-align: right">30.98</td>
                <td style="text-align: right">466.35</td>
              </tr>
              <tr>
                <td>OSTRICH</td>
                <td style="text-align: right">3,102.72</td>
                <td style="text-align: right">12.32</td>
                <td style="text-align: right">187.46</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+1,484.80</td>
                <td style="text-align: right">+4.55</td>
                <td style="text-align: right">+263.13</td>
              </tr>
              <tr>
                <td>Jena-IC</td>
                <td style="text-align: right">32,808.96</td>
                <td style="text-align: right">415.32</td>
                <td style="text-align: right">6,233.92</td>
              </tr>
              <tr>
                <td>Jena-CB</td>
                <td style="text-align: right">18,216.96</td>
                <td style="text-align: right">42.82</td>
                <td style="text-align: right">473.41</td>
              </tr>
              <tr>
                <td>Jena-TB</td>
                <td style="text-align: right">82,278.4</td>
                <td style="text-align: right">23.61</td>
                <td style="text-align: right">3,678.89</td>
              </tr>
              <tr>
                <td>Jena-CB/TB</td>
                <td style="text-align: right">31,160.32</td>
                <td style="text-align: right">22.83</td>
                <td style="text-align: right">53.84</td>
              </tr>
              <tr>
                <td>HDT-IC</td>
                <td style="text-align: right">5,335.04</td>
                <td style="text-align: right">142.08</td>
                <td style="text-align: right">2,127.57</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+1,494.69</td>
                <td style="text-align: right">+6.53</td>
                <td style="text-align: right">+98.88</td>
              </tr>
              <tr>
                <td>HDT-CB</td>
                <td style="text-align: right"><em>2,682.88</em></td>
                <td style="text-align: right"><em>5.96</em></td>
                <td style="text-align: right"><em>24.39</em></td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+802.55</td>
                <td style="text-align: right">+0.25</td>
                <td style="text-align: right">+0.75</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 11:</span> Storage sizes for each of the <abbr title='Resource Description Framework'>RDF</abbr> archive approaches in MB with BEAR-A, BEAR-B-daily and BEAR-B-hourly.
The additional storage size for the auxiliary <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='Header Dictionary Triples'>HDT</abbr> indexes are provided as separate rows.
The lowest sizes per dataset are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="storing_results-ingestion-time" class="table">

          <table>
            <thead>
              <tr>
                <th>Approach</th>
                <th style="text-align: right">BEAR-A</th>
                <th style="text-align: right">BEAR-B-daily</th>
                <th style="text-align: right">BEAR-B-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>OSTRICH</td>
                <td style="text-align: right">2,256</td>
                <td style="text-align: right">12.36</td>
                <td style="text-align: right">4,497.32</td>
              </tr>
              <tr>
                <td>Jena-IC</td>
                <td style="text-align: right">443</td>
                <td style="text-align: right">8.91</td>
                <td style="text-align: right">142.26</td>
              </tr>
              <tr>
                <td>Jena-CB</td>
                <td style="text-align: right">226</td>
                <td style="text-align: right">9.53</td>
                <td style="text-align: right">173.48</td>
              </tr>
              <tr>
                <td>Jena-TB</td>
                <td style="text-align: right">1,746</td>
                <td style="text-align: right">0.35</td>
                <td style="text-align: right">70.56</td>
              </tr>
              <tr>
                <td>Jena-CB/TB</td>
                <td style="text-align: right">679</td>
                <td style="text-align: right">0.35</td>
                <td style="text-align: right">0.65</td>
              </tr>
              <tr>
                <td>HDT-IC</td>
                <td style="text-align: right">34</td>
                <td style="text-align: right">0.39</td>
                <td style="text-align: right">5.89</td>
              </tr>
              <tr>
                <td>HDT-CB</td>
                <td style="text-align: right"><em>18</em></td>
                <td style="text-align: right"><em>0.02</em></td>
                <td style="text-align: right"><em>0.07</em></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 12:</span> Ingestion times for each of the <abbr title='Resource Description Framework'>RDF</abbr> archive approaches with BEAR-A, BEAR-B-daily and BEAR-B-hourly.
The lowest times per dataset are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="storing_results-addition-counts" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right">BEAR-A</th>
                <th style="text-align: right">BEAR-B-daily</th>
                <th style="text-align: right">BEAR-B-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right">13.69 (0.29%)</td>
                <td style="text-align: right">6.25 (37.05%)</td>
                <td style="text-align: right">15.62 (3.46%)</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 13:</span> Storage sizes of the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> addition count component in MB with BEAR-A, BEAR-B-daily and BEAR-B-hourly.
The percentage of storage space that this component requires compared to the complete store is indicated between brackets.</p>
          </figcaption>
        </figure>

        <figure id="storing_results-ostrich-ingestion-rate-beara">
<img src="storing/img/results-ostrich-ingestion-rate-beara.svg" alt="[bear-a ostrich ingestion rate]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 19:</span> OSTRICH ingestion durations for each consecutive BEAR-A version in minutes for an increasing number of versions,
showing a lineair growth.</p>
          </figcaption>
</figure>

        <figure id="storing_results-ostrich-ingestion-size-beara">
<img src="storing/img/results-ostrich-ingestion-size-beara.svg" alt="[bear-a ostrich ingestion sizes]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 20:</span> Cumulative <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store sizes for each consecutive BEAR-A version in GB for an increasing number of versions,
showing a lineair growth.</p>
          </figcaption>
</figure>

        <figure id="storing_results-ostrich-ingestion-rate-bearb-hourly">
<img src="storing/img/results-ostrich-ingestion-rate-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion rate]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 21:</span> OSTRICH ingestion durations for each consecutive BEAR-B-hourly version in minutes for an increasing number of versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-ostrich-ingestion-size-bearb-hourly">
<img src="storing/img/results-ostrich-ingestion-size-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion sizes]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 22:</span> Cumulative <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store sizes for each consecutive BEAR-B-hourly version in GB for an increasing number of versions.</p>
          </figcaption>
</figure>

        <figure id="results-ostrich-compressability" class="table">

          <table>
            <thead>
              <tr>
                <th>Format</th>
                <th>Dataset</th>
                <th style="text-align: right">Size</th>
                <th style="text-align: right">gzip</th>
                <th style="text-align: right">Savings</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>N-Triples</strong></td>
                <td>A</td>
                <td style="text-align: right">46,069.76</td>
                <td style="text-align: right">3,194.88</td>
                <td style="text-align: right">93.07%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">8,314.86</td>
                <td style="text-align: right">466.35</td>
                <td style="text-align: right">94.39%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">556.44</td>
                <td style="text-align: right">30.98</td>
                <td style="text-align: right">94.43%</td>
              </tr>
              <tr>
                <td><strong>OSTRICH</strong></td>
                <td>A</td>
                <td style="text-align: right">3,117.64</td>
                <td style="text-align: right">2,155.13</td>
                <td style="text-align: right">95.32%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">187.46</td>
                <td style="text-align: right">34.92</td>
                <td style="text-align: right">99.58%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">12.32</td>
                <td style="text-align: right">3.35</td>
                <td style="text-align: right">99.39%</td>
              </tr>
              <tr>
                <td><strong>HDT-IC</strong></td>
                <td>A</td>
                <td style="text-align: right">5,335.04</td>
                <td style="text-align: right">1,854.48</td>
                <td style="text-align: right">95.97%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">2,127.57</td>
                <td style="text-align: right">388.02</td>
                <td style="text-align: right">95.33%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">142.08</td>
                <td style="text-align: right">25.69</td>
                <td style="text-align: right">95.33%</td>
              </tr>
              <tr>
                <td><strong>HDT-CB</strong></td>
                <td>A</td>
                <td style="text-align: right"><em>2,682.88</em></td>
                <td style="text-align: right"><em>856.39</em></td>
                <td style="text-align: right"><em>98.14%</em></td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right"><em>24.39</em></td>
                <td style="text-align: right"><em>2.86</em></td>
                <td style="text-align: right"><em>99.96%</em></td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right"><em>5.96</em></td>
                <td style="text-align: right"><em>1.14</em></td>
                <td style="text-align: right"><em>99.79%</em></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 14:</span> Compressability using gzip for all <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> datasets using <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, HDT-IC, HDT-CB and natively as N-Triples.
The columns represent the original size (MB), the resulting size after applying gzip (MB), and the total space savings.
The lowest sizes are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="storing_results-ostrich-ingestion-rate-beara-compare">
<img src="storing/img/results-ostrich-ingestion-rate-beara-compare.svg" alt="[Comparison of ostrich ingestion algorithms]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 23:</span> Comparison of the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> stream and batch-based ingestion durations.</p>
          </figcaption>
</figure>

        <h5 id="compressibility">Compressibility</h5>

        <p><a href="#storing_results-ostrich-compressability"></a> presents the compressibility of datasets without auxiliary indexes,
showing that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and the HDT-based approaches significantly improve compressibility compared to the original N-Triples serialization.
We omitted the results from the Jena-based approaches in this table,
as all compressed sizes were in all cases two to three times larger than the N-Triples compression.</p>

        <h5 id="query-evaluation">Query Evaluation</h5>

        <p>Figures <a href="#storing_results-beara-vm-sumary">10</a>, <a href="#storing_results-beara-dm-summary">11</a> and <a href="#storing_results-beara-vq-summary">12</a> respectively
summarize the <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> query durations of all BEAR-A queries on the ten first versions of the BEAR-A dataset for the different approaches.
HDT-IC clearly outperforms all other approaches in all cases,
while the Jena-based approaches are orders of magnitude slower than the HDT-based approaches and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in all cases.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is about two times faster than HDT-CB for <abbr title='Version Materialization'>VM</abbr> queries, and slightly slower for both <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> queries.
For <abbr title='Delta Materialization'>DM</abbr> queries, HDT-CB does however continuously become slower for larger versions, while the lookup times for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> remain constant.
From version 7, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than HDT-CB.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-a" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​a">Appendix A</a> contains more detailed plots for each BEAR-A queryset,
in which we can see that all approaches collectively become slower for queries with a higher result cardinality,
and that predicate-queries are also significantly slower for all approaches.</p>

        <figure id="storing_results-beara-vm-sumary">
<img src="storing/img/query/results_beara-vm-summary.svg" alt="[bear-a vm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 24:</span> Median BEAR-A <abbr title='Version Materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-beara-dm-summary">
<img src="storing/img/query/results_beara-dm-summary.svg" alt="[bear-a dm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 25:</span> Median BEAR-A <abbr title='Delta Materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-beara-vq-summary">
<img src="storing/img/query/results_beara-vq-summary.svg" alt="[bear-a vq]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 26:</span> Median BEAR-A <abbr title='Version Query'>VQ</abbr> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <p>Figures <a href="#storing_results-bearb-daily-vm-sumary">13</a>, <a href="#storing_results-bearb-daily-dm-summary">14</a> and <a href="#storing_results-bearb-daily-vq-summary">15</a>
contain the query duration results for the BEAR-B queries on the complete BEAR-B-daily dataset for the different approaches.
Jena-based approaches are again slower than both the HDT-based ones and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
For <abbr title='Version Materialization'>VM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is slower than HDT-IC, but faster than HDT-CB, which becomes slower for larger versions.
For <abbr title='Delta Materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than HDT-CB for the second half of the versions, and slightly faster HDT-IC.
The difference between HDT-IC and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is however insignificant in this case, as can be seen in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-daily" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​daily">Appendix B</a>.
For <abbr title='Version Query'>VQ</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is significantly faster than all other approaches.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-daily" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​daily">Appendix B</a> contains more detailed plots for this case,
in which we can see that predicate-queries are again consistently slower for all approaches.</p>

        <figure id="storing_results-bearb-daily-vm-sumary">
<img src="storing/img/query/results_bearb-daily-vm-summary.svg" alt="[bear-b-daily vm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 27:</span> Median BEAR-B-daily <abbr title='Version Materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-bearb-daily-dm-summary">
<img src="storing/img/query/results_bearb-daily-dm-summary.svg" alt="[bear-b-daily dm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 28:</span> Median BEAR-B-daily <abbr title='Delta Materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-bearb-daily-vq-summary">
<img src="storing/img/query/results_bearb-daily-vq-summary.svg" alt="[bear-b-daily vq]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 29:</span> Median BEAR-B-daily <abbr title='Version Query'>VQ</abbr> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <p>Figures <a href="#storing_results-bearb-hourly-vm-sumary">16</a>, <a href="#storing_results-bearb-hourly-dm-summary">17</a> and <a href="#storing_results-hourly-daily-vq-summary">18</a>
show the query duration results for the BEAR-B queries on the complete BEAR-B-hourly dataset for all approaches.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> again outperforms Jena-based approaches in all cases.
HDT-IC is faster for <abbr title='Version Materialization'>VM</abbr> queries than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, but HDT-CB is significantly slower, except for the first 100 versions.
For <abbr title='Delta Materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is comparable to HDT-IC, and faster than HDT-CB, except for the first 100 versions.
Finally, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms all HDT-based approaches for <abbr title='Version Query'>VQ</abbr> queries by almost an order of magnitude.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-hourly" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​hourly">Appendix C</a> contains the more detailed plots
with the same conclusion as before that predicate-queries are slower.</p>

        <figure id="storing_results-bearb-hourly-vm-sumary">
<img src="storing/img/query/results_bearb-hourly-vm-summary.svg" alt="[bear-b-hourly vm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 30:</span> Median BEAR-B-hourly <abbr title='Version Materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-bearb-hourly-dm-summary">
<img src="storing/img/query/results_bearb-hourly-dm-summary.svg" alt="[bear-b-hourly dm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 31:</span> Median BEAR-B-hourly <abbr title='Delta Materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="storing_results-bearb-hourly-vq-summary">
<img src="storing/img/query/results_bearb-hourly-vq-summary.svg" alt="[bear-b-hourly vq]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 32:</span> Median BEAR-B-hourly <abbr title='Version Query'>VQ</abbr> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <h5 id="offset">Offset</h5>

        <p>From our evaluation of offsets, <a href="#storing_results-offset-vm">Fig. 33</a> shows that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> offset evaluation remain below 1ms,
while other approaches grow beyond that for larger offsets, except for HDT-IC+.
HDT-CB, Jena-CB and Jena-CB/TB are not included in this and the following figures
because they require full materialization before offsets can be applied, which is expensive and therefore take a very long time to evaluate.
For <abbr title='Delta Materialization'>DM</abbr> queries, all approaches have growing evaluating times for larger offsets including <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, as can be seen in <a href="#storing_results-offset-dm">Fig. 34</a>.
Finally, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> has <abbr title='Version Query'>VQ</abbr> evaluation times that are approximately independent of the offset value,
while other approaches again have growing evaluation times, as shown in <a href="#storing_results-offset-vq">Fig. 35</a>.</p>

        <figure id="storing_results-offset-vm">
<img src="storing/img/query/results_offsets-vm.svg" alt="[Offsets vm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 33:</span> Median <abbr title='Version Materialization'>VM</abbr> query results for different offsets over all versions in the BEAR-A dataset.</p>
          </figcaption>
</figure>

        <figure id="storing_results-offset-dm">
<img src="storing/img/query/results_offsets-dm.svg" alt="[Offsets dm]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 34:</span> Median <abbr title='Delta Materialization'>DM</abbr> query results for different offsets between version 0 and all other versions in the BEAR-A dataset.</p>
          </figcaption>
</figure>

        <figure id="storing_results-offset-vq">
<img src="storing/img/query/results_offsets-vq.svg" alt="[Offsets vq]" height="200em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 35:</span> Median <abbr title='Version Query'>VQ</abbr> query results for different offsets in the BEAR-A dataset.</p>
          </figcaption>
</figure>

        <h4 id="discussion">Discussion</h4>

        <p>In this section, we interpret and discuss the results from previous section.
We discuss the ingestion, compressbility, query evaluation, offset efficiency and test our hypotheses.</p>

        <h5 id="ingestion-1">Ingestion</h5>

        <p>For all evaluated cases, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires less storage space than most non-CB approaches.
The <abbr title='Change-based'>CB</abbr> and CB/TB approaches in most cases outperform <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in terms of storage space efficiency due
to the additional metadata that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> stores per triple.
Because of this, most other approaches require less time to ingest new data.
These timing results should however be interpreted correctly,
because all other approaches receive their input data in the appropriate format (IC, <abbr title='Change-based'>CB</abbr>, <abbr title='Timestamp-based'>TB</abbr>, CB/TB),
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> does not.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> must convert <abbr title='Change-based'>CB</abbr> input at runtime to the alternative <abbr title='Change-based'>CB</abbr> structure where deltas are relative to the snapshot,
which explains the larger ingestion times.
As an example, <a href="#storing_triples-bearb-hourly-altcb">Fig. 36</a> shows the number of triples in each BEAR-B-hourly version
where the deltas have been transformed to the alternative delta structure that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> uses.
Just like the first part of <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 21</a>, this graph also increases linearly,
which indicates that the large number of triples that need to be handled for long delta chains is one of the main bottlenecks for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
This is also the reason why <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> has memory issues during ingestion at the end of such chains.
One future optimization could be to maintain the last version of each chain in a separate index for faster patching.
Or a new ingestion algorithm could be implemented that accepts input in the correct alternative <abbr title='Change-based'>CB</abbr> format.
Alternatively, a new snapshot could dynamically be created when ingestion time becomes too large,
which could for example for BEAR-B-hourly take place around version 1000.</p>

        <figure id="storing_triples-bearb-hourly-altcb">
<img src="storing/img/triples-bearb-hourly-altcb.svg" alt="[bear-b-hourly alternative cb]" height="150em" class="figure-medium-width" />
<figcaption>
            <p><span class="label">Fig. 36:</span> Total number of triples for each BEAR-B-hourly version when converted to the alternative <abbr title='Change-based'>CB</abbr> structure used by <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>,
i.e., each triple is an addition or deletion relative to the <em>first</em> version instead of the <em>previous</em> version.</p>
          </figcaption>
</figure>

        <p>The BEAR-A and BEAR-B-hourly datasets indicate the limitations of the ingestion algorithm in our system.
The results for BEAR-A show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingests slowly for many very large versions,
but it is still possible because of the memory-efficient streaming algorithm.
The results for BEAR-B-hourly show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> should not be used when the number of versions is very large.
Furthermore, for each additional version in a dataset, the ingestion time increases.
This is a direct consequence of our alternative delta chain method where all deltas are relative to a snapshot.
That is the reason why when new deltas are inserted,
the previous one must be fully materialized by iterating over all existing triples,
because no version index exists.</p>

        <p>In <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 21</a>, we can observe large fluctuations in ingestion time around version 1,200 of BEAR-B-hourly.
This is caused by the large amount of versions that are stored for each tree value.
Since each version requires a mapping to seven triple pattern indexes and one local change flag in the deletion tree,
value sizes become non-negligible for large amounts of versions.
Each version value requires 28 uncompressed bytes,
which results in more than 32KB for a triple in 1,200 versions.
At that point, the values start to form a bottleneck as only 1,024 elements
can be loaded in-memory using the default page cache size of 32MB,
which causes a large amount of swapping.
This could be solved by either tweaking the B+Tree parameters for this large amount of versions,
reducing storage requirements for each value,
or by dynamically creating a new snapshot.</p>

        <p>We compared the streaming and batch-based ingestion algorithm in <a href="#storing_results-ostrich-ingestion-rate-beara-compare">Fig. 23</a>.
The batch algorithm is initially faster because most operations can happen in memory,
while the streaming algorithm only uses a small fraction of that memory,
which makes the latter usable for very large datasets that don’t fit in memory.
In future work, a hybrid between the current streaming and batch algorithm could be investigated,
i.e., a streaming algorithm with a larger buffer size, which is faster, but doesn’t require unbounded amounts of memory.</p>

        <h5 id="compressibility-1">Compressibility</h5>

        <p>As shown in <a href="#storing_results-ostrich-compressability"></a>,
when applying gzip directly on the raw N-Triples input already achieves significant space savings.
However, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, HDT-IC and HDT-CB are able to reduce the required storage space <em>even further</em> when they are used as a preprocessing step before applying gzip.
This shows that these approaches are better—storage-wise—for the archival of versioned datasets.
This table also shows that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets with more versions are more prone to space savings
using compression techniques like gzip compared to <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets with fewer versions.</p>

        <h5 id="query-evaluation-1">Query Evaluation</h5>

        <p>The results from previous section show that the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> query evaluation efficiency is faster than all Jena-based approaches,
mostly faster than HDT-CB, and mostly slower than HDT-IC.
<abbr title='Version Materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are always slower than HDT-IC,
because <abbr title='Header Dictionary Triples'>HDT</abbr> can very efficiently query a single materialized snapshot in this case,
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more operations for materializing.
<abbr title='Version Materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are however always faster than HDT-CB, because the latter has to reconstruct complete delta chains,
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> only has to reconstruct a single delta relative to the snapshot.
For <abbr title='Delta Materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is slower or comparable to HDT-IC, slower than HDT-CB for early versions, but faster for later versions.
This slowing down of HDT-CB for <abbr title='Delta Materialization'>DM</abbr> queries is again caused by reconstruction of delta chains.
For <abbr title='Version Query'>VQ</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms all other approaches for datasets with larger amounts of versions.
For BEAR-A, which contains only 10 versions in our case,
the HDT-based approaches are slightly faster because only a small amount of versions need to be iterated.</p>

        <h5 id="offsets">Offsets</h5>

        <p>One of our initial requirements was to design a system that allows efficient offsetting of <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> result streams.
As shown in last section, for both <abbr title='Version Materialization'>VM</abbr> and <abbr title='Version Query'>VQ</abbr> queries, the lookup times for various offsets remain approximately constant.
For <abbr title='Version Materialization'>VM</abbr> queries, this can fluctuate slightly for certain offsets due to the loop section inside the <abbr title='Version Materialization'>VM</abbr> algorithm
for determining the starting position inside the snapshot and deletion tree.
For <abbr title='Delta Materialization'>DM</abbr> queries, we do however observe an increase in lookup times for larger offsets.
That is because the current <abbr title='Delta Materialization'>DM</abbr> algorithm naively offsets these streams by iterating
over the stream until a number of elements equal to the desired offset have been consumed.
Furthermore, other <abbr title='Independent Copies'>IC</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches outperform OSTRICH’s <abbr title='Delta Materialization'>DM</abbr> result stream offsetting.
This introduces a new point of improvement for future work,
seeing whether or not <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> would allow more efficient <abbr title='Delta Materialization'>DM</abbr> offsets by adjusting either the algorithm or the storage format.</p>

        <h5 id="hypotheses">Hypotheses</h5>

        <p>In <a href="#storing_problem-statement">Section 3.3</a>, we introduced six hypotheses, which we will validate in this section based on our experimental results.
We will only consider the comparison between <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and HDT-based approaches,
as <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms the Jena-based approaches for all cases in terms of lookup times.
These validations were done using R, for which the source code can be found on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.
Tables containing p-values of the results can be found in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-tests" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​tests">Appendix E</a>.</p>

        <p>For our <a href="#storing_hypothesis-qualitative-querying">first hypothesis</a>, we expect <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> lookup times to remain independent of version for <abbr title='Version Materialization'>VM</abbr> and <abbr title='Delta Materialization'>DM</abbr> queries.
We validate this hypothesis by building a linear regression model with as response the lookup time,
and as factors version and number of results.
The <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-1" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​1">appendix</a> contains the influence of each factor, which shows that for all cases,
we can accept the null hypothesis that the version factor has no influence on the models with a confidence of 99%.
Based on these results, we <em>accept</em> our <a href="#storing_hypothesis-qualitative-querying">first hypothesis</a>.</p>

        <meta property="lsc:confirms" resource="#storing_hypothesis-qualitative-querying" />

        <p><a href="#storing_hypothesis-qualitative-ic-storage">Hypothesis 2</a> states that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>less</em> storage space than IC-based approaches,
and <a href="#storing_hypothesis-qualitative-ic-querying">Hypothesis 3</a> correspondingly states that
query evaluation is <em>slower</em> for <abbr title='Version Materialization'>VM</abbr> and <em>faster</em> or <em>equal</em> for <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr>.
Results from previous section showed that for BEAR-A, BEAR-B-daily and BEAR-B-hourly,
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>less</em> space than HDT-IC, which means that we <em>accept</em> Hypothesis 2.
In order to validate that query evaluation is slower for <abbr title='Version Materialization'>VM</abbr> but faster or equal for <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr>,
we compared the means using the two-sample t-test, for which the results can be found in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-2" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​2">appendix</a>.
In all cases, the means are not equal with a confidence of 95%.
For BEAR-B-daily and BEAR-B-hourly, HDT-IC is faster for <abbr title='Version Materialization'>VM</abbr> queries, but slower for <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> queries.
For BEAR-A, HDT-IC is faster for all query types.
We therefore <em>reject</em> Hypothesis 3, as it does not apply for BEAR-A, but it is valid for BEAR-B-daily and BEAR-B-hourly.
This means that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> typically requires less storage space than IC-based approaches,
and outperforms other approaches in terms of querying efficiency
unless the number of versions is small or for <abbr title='Version Materialization'>VM</abbr> queries.</p>

        <meta property="lsc:confirms" resource="#storing_hypothesis-qualitative-ic-storage" />

        <meta property="lsc:falsifies" resource="#storing_hypothesis-qualitative-ic-querying" />

        <p>In <a href="#storing_hypothesis-qualitative-cb-storage">Hypothesis 4</a>, we stated that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>more</em>
storage space than CB-based approaches,
and in <a href="#storing_hypothesis-qualitative-cb-querying">Hypothesis 5</a> that query evaluation is <em>faster</em> or <em>equal</em>.
In all cases <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more storage space than HDT-CB, which is why we <em>accept</em> Hypothesis 4.
For the query evaluation, we again compare the means in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-3" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​3">appendix</a> using the same test.
In BEAR-A, <abbr title='Version Query'>VQ</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are not faster for BEAR-A, and <abbr title='Version Materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are not faster for BEAR-B-daily,
which is why we <em>reject</em> Hypothesis 5.
However, only one in three query atoms are not fulfilled, and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than HDT-CB for BEAR-B-hourly.
In general, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more storage space than CB-based approaches,
and query evaluation is faster unless the number of versions is low.</p>

        <meta property="lsc:confirms" resource="#storing_hypothesis-qualitative-cb-storage" />

        <meta property="lsc:falsifies" resource="#storing_hypothesis-qualitative-cb-querying" />

        <p>Finally, in our <a href="#storing_hypothesis-qualitative-ingestion">last hypothesis</a>,
we state that average query evaluation times are lower than other non-IC approaches at the cost of increased ingestion times.
In all cases, the ingestion time for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is higher than the other approaches,
and as shown in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-3" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​3">appendix</a>, query evaluation times for non-IC approaches are lower for BEAR-B-hourly.
This means that we <em>reject</em> Hypothesis 6 because it only holds for BEAR-B-hourly and not for BEAR-A and BEAR-B-daily.
In general, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingestion is slower than other approaches,
but improves query evaluation time compared to other non-IC approaches,
unless the number of versions is low.</p>

        <meta property="lsc:falsifies" resource="#storing_hypothesis-qualitative-ingestion" />

        <p>In this section, we accepted three of the six hypotheses.
As these are statistical hypotheses, these do not necessarily indicate negative results of our approach.
Instead, they allow us to provide general guidelines on where our approach can be used effectively, and where not.</p>

      </div>
</section>

    <section id="storing_conclusions" inlist="" rel="schema:hasPart" resource="#storing_conclusions">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Conclusions</h3>

        <p>In this article, we introduced an <abbr title='Resource Description Framework'>RDF</abbr> archive storage method with accompanied algorithms for evaluating <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr>, and <abbr title='Version Query'>VQ</abbr> queries,
with efficient result offsets.
Our novel storage technique is a hybrid of the IC/CB/TB approaches, because we store sequences of snapshots followed by delta chains.
The evaluation of our <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> implementation shows that this technique offers a new trade-off in terms of ingestion time, storage size and lookup times.
By preprocessing and storing additional data during ingestion, we can reduce lookup times for <abbr title='Version Materialization'>VM</abbr>, <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> queries compared to <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches.
Our approach requires less storage space than <abbr title='Independent Copies'>IC</abbr> approaches, at the cost of slightly slower <abbr title='Version Materialization'>VM</abbr> queries, but comparable <abbr title='Delta Materialization'>DM</abbr> queries.
Furthermore, our technique is faster than <abbr title='Change-based'>CB</abbr> approaches, at the cost of more storage space.
Additionally, <abbr title='Version Query'>VQ</abbr> queries become increasingly more efficient for datasets with larger amounts of versions compared to <abbr title='Independent Copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches.
Our current implementation supports a single snapshot and delta chain
as a proof of concept,
but production environments would normally incorporate more frequent snapshots,
balancing between storage and querying requirements.</p>

        <p>With lookup times of 1ms or less in most cases, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is an ideal candidate for Web querying,
as the network latency will typically be higher than that.
At the cost of increased ingestion times, lookups are fast.
Furthermore, by reusing the highly efficient <abbr title='Header Dictionary Triples'>HDT</abbr> format for snapshots,
existing <abbr title='Header Dictionary Triples'>HDT</abbr> files can directly be loaded by <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>
and patched with additional versions afterwards.</p>

        <p>OSTRICH fulfills the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">requirements</a> <span class="references">[<a href="#ref-80">80</a>]</span> for a backend <abbr title='Resource Description Framework'>RDF</abbr> archive storage solution
for supporting versioning queries in the <abbr title='Triple Pattern Fragments'>TPF</abbr> framework.
Together with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">VTPF</a> <span class="references">[<a href="#ref-40">40</a>]</span> interface feature, <abbr title='Resource Description Framework'>RDF</abbr> archives can be queried on the Web at a low cost,
as demonstrated on <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">our public <abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr> entrypoint</a>.
<abbr title='Triple Pattern Fragments'>TPF</abbr> only requires triple pattern indexes with count metadata,
which means that <abbr title='Triple Pattern Fragments'>TPF</abbr> clients are able to evaluate full <abbr title='Version Materialization'>VM</abbr> <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries using <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr>.
In future work, the <abbr title='Triple Pattern Fragments'>TPF</abbr> client will be extended to also support <abbr title='Delta Materialization'>DM</abbr> and <abbr title='Version Query'>VQ</abbr> <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries.</p>

        <p>With <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, we provide a technique for publishing and querying <abbr title='Resource Description Framework'>RDF</abbr> archives at Web-scale.
Several opportunities exist for advancing this technique in future work,
such as improving the ingestion efficiency, increasing the <abbr title='Delta Materialization'>DM</abbr> offset efficiency,
and supporting dynamic snapshot creation.
Solutions could be based on existing
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://users.ics.forth.gr/~fgeo/files/ER14.pdf">cost models</a> <span class="references">[<a href="#ref-81">81</a>]</span> for determining whether a new snapshot or delta
should be created based on quantified time and space parameters.
Furthermore, branching and merging of different version chains can be investigated.</p>

        <p>Our approach succeeds in reducing the cost for publishing <abbr title='Resource Description Framework'>RDF</abbr> archives on the Web.
This lowers the barrier towards intelligent clients in the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www-sop.inria.fr/acacia/cours/essi2006/Scientific%20American_%20Feature%20Article_%20The%20Semantic%20Web_%20May%202001.pdf">Semantic Web</a> <span class="references">[<a href="#ref-1">1</a>]</span> that require <em>evolving</em> data,
with the goal of time-sensitive querying over the ever-evolving Web of data.</p>

      </div>
</section>

    <div class="subfooter">
  <section id="storing_acknowledgements" inlist="" rel="schema:hasPart" resource="#storing_acknowledgements">
<div datatype="rdf:HTML" property="schema:description">
          <h3 property="schema:name" class="no-label-increment">Acknowledgements</h3>

          <p>We would like to thank Christophe Billiet for providing his insights into temporal databases.
We thank Giorgos Flouris for his comments on the structure and contents of this article,
and Javier D. Fernández for his help in setting up and running the <abbr title='Benchmark of <abbr title='Resource Description Framework'>RDF</abbr> Archives&#8217;>BEAR</abbr> benchmark.
The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

        </div>
</section>

</div>
  </section>
  
  <section class="sub-paper">
    <h2 id="querying">Querying a heterogeneous Web</h2>

    <section class="sub-preface">
<div datatype="rdf:HTML" property="schema:description">
        <p>In this chapter, we focus on the third challenge of this PhD: “The Web is highly <em>heterogeneous</em>”.
In order to query over such a highly heterogeneous Web,
a query engine is needed that is able to handle various kinds of interfaces on the Web.
Furthermore, in order to handle these different kinds of interfaces <em>efficiently</em>,
various kinds of interface-specific algorithms must be supported.
For example, if an interface exposes a triple pattern index,
then the query should be able to detect and exploit this index to improve the efficiency when evaluating triple pattern queries.</p>

        <p>The different kinds of Web interfaces,
and the large number of different querying algorithms that can be used with them
requires an intelligent query engine that detect these interfaces and apply these algorithms.
Our work in this chapter handles this problem by introducing a highly <em>flexible</em> and <em>modular</em> query engine platform <em>Comunica</em>.
Comunica has been designed in such a way that support for new interfaces and query algorithms can be developed <em>independently</em> as separate modules,
and these modules can then be <em>plugged</em> into Comunica when they are needed.
This engine simplifies the research and development of new query interfaces and algorithms,
as new techniques can be tested immediately in conjunction with other already existing interfaces and algorithms.
As we introduce a system architecture in this chapter, no research question is applicable here.</p>

      </div>
</section>

    <p class="published-as">Ruben Taelman, Joachim Van Herwegen, Miel Vander Sande, and Ruben Verborgh. 2018. <strong><a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica: a Modular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Engine for the Web</a></strong>. In Denny Vrandečić et al., eds. Proceedings of the 17th International Semantic Web Conference. Lecture Notes in Computer Science. Springer, 239–255.</p>

    <section id="querying_abstract" inlist="" rel="schema:hasPart" resource="#querying_abstract">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name" class="no-label-increment">Abstract</h3>

        <!-- Context      -->
        <p>Query evaluation over Linked Data sources has become a complex story,
given the multitude of algorithms and techniques
for single- and multi-source querying,
as well as the heterogeneity of Web interfaces
through which data is published online.
<!-- Need         -->
Today’s query processors are insufficiently adaptable
to test multiple query engine aspects in combination,
such as evaluating the performance of a certain join algorithm
over a federation of heterogeneous interfaces.
The Semantic Web research community is in need of a flexible query engine
that allows plugging in new components
such as different algorithms,
new or experimental <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> features,
and support for new Web interfaces.
<!-- Task         -->
We designed and developed a Web-friendly and modular meta query engine
called <em>Comunica</em>
that meets these specifications.
<!-- Object       -->
In this article,
we introduce this query engine
and explain the architectural choices behind its design.
<!-- Findings     -->
We show how its modular nature makes it an ideal research platform
for investigating new kinds of Linked Data interfaces and querying algorithms.
<!-- Conclusion   -->
Comunica facilitates the development, testing, and evaluation
of new query processing capabilities,
both in isolation and in combination with others.
<!-- Perspectives --></p>

      </div>
</section>

    <section id="querying_introduction" inlist="" rel="schema:hasPart" resource="#querying_introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Introduction</h3>

        <p>Linked Data on the Web exists in many shapes and forms—and
so do the processors we use to query data from one or multiple sources.
For instance,
engines that query <abbr title='Resource Description Framework'>RDF</abbr> data using the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL language</a> <span class="references">[<a href="#ref-3">3</a>]</span>
employ <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675"><em>different algorithms</em></a></span> <span class="references">[<a href="#ref-82">82</a>, <a href="#ref-83">83</a>]</span>
and support <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2"><a href="https://doi.org/10.1007/978-3-642-02184-8_2"><em>different language extensions</em></a></span> <span class="references">[<a href="#ref-84">84</a>, <a href="#ref-85">85</a>]</span>.
Furthermore,
Linked Data is increasingly published through <em>different Web interfaces</em>,
such as
data dumps, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/DesignIssues/LinkedData.html">Linked Data documents</a> <span class="references">[<a href="#ref-4">4</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL endpoints</a> <span class="references">[<a href="#ref-86">86</a>]</span>
and <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (TPF) interfaces</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>.
This has led to entirely different query evaluation strategies,
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">server-side</a> <span class="references">[<a href="#ref-86">86</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based</a> <span class="references">[<a href="#ref-87">87</a>]</span>,
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">shared client–server query processing</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>,
and
client-side (by downloading data dumps and loading them locally).</p>

        <p>The resulting variety of implementations
suffers from two main problems:
a lack of <em>sustainability</em>
and a lack of <em>comparability</em>.
Alternative query algorithms and features
are typically either implemented as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf"><em>forks</em> of existing software packages</a> <span class="references">[<a href="#ref-88">88</a>, <a href="#ref-89">89</a>, <a href="#ref-90">90</a>]</span>
or as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf"><em>independent</em> engines</a> <span class="references">[<a href="#ref-91">91</a>]</span>.
This practice has limited sustainability:
forks are often not merged into the main software distribution
and hence become abandoned;
independent implementations require a considerable upfront cost
and also risk abandonment more than established engines.
Comparability is also limited:
forks based on older versions of an engine
cannot meaningfully be evaluated against newer forks,
and evaluating <em>combinations</em> of cross-implementation features—such as
different algorithms on different interfaces—is
not possible without code adaptation.
As a result, many interesting comparisons are never performed
because they are too costly to implement and maintain.
For example,
it is currently unknown
how the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf">Linked Data Eddies algorithm</a> <span class="references">[<a href="#ref-91">91</a>]</span>
performs over a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>
of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF interfaces</a></span> <span class="references">[<a href="#ref-92">92</a>]</span>.
Another example is that the effects of various <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">optimizations and extensions for <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces</a> <span class="references">[<a href="#ref-88">88</a>, <a href="#ref-89">89</a>, <a href="#ref-90">90</a>, <a href="#ref-91">91</a>, <a href="#ref-92">92</a>, <a href="#ref-40">40</a>, <a href="#ref-93">93</a>, <a href="#ref-94">94</a>]</span>
have only been evaluated in isolation,
whereas certain combinations will likely prove complementary.</p>

        <p>In order to handle the increasing heterogeneity of Linked Data on the Web,
as well as various solutions for querying it,
there is a need for a flexible and modular query engine
to experiment with all of these techniques—both separately and in combination.
In this article, we introduce <em>Comunica</em> to realize this vision.
It is a highly modular meta engine for federated <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation
over heterogeneous interfaces,
including <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints, and data dumps.
Comunica aims to serve as a flexible research platform for
designing, implementing, and evaluating
new and existing Linked Data querying and publication techniques.</p>

        <p>Comunica differs from existing query processors on different levels:</p>

        <ol>
          <li>The <strong>modularity</strong> of the Comunica meta query engine allows for
<em>extensions</em> and <em>customization</em> of algorithms and functionality.
Users can build and fine-tune a concrete engine
by wiring the required modules through an <abbr title='Resource Description Framework'>RDF</abbr> configuration document.
By publishing this document,
experiments can repeated and adapted by others.</li>
          <li>Within Comunica, multiple <strong>heterogeneous interfaces</strong> are first-class citizens. This enables federated querying over heterogeneous sources and makes it for example possible to evaluate queries over any combination of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints, <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, datadumps, or other types of interfaces.</li>
          <li>Comunica is implemented using <strong>Web-based technologies</strong> in JavaScript, which enables usage through browsers, the command line, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL protocol</a> <span class="references">[<a href="#ref-86">86</a>]</span>, or any Web or JavaScript application.</li>
        </ol>

        <p>Comunica and its default modules are publicly available
on GitHub and the npm package manager under the open-source MIT license
(canonical citation: <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo">https:/​/​zenodo.org/record/1202509#.Wq9GZhNuaHo</a>).</p>

        <p>This article is structured as follows.
In the next section, we discuss the related work, followed by the main features of Comunica in <a href="#querying_features">Section 4.3</a>.
After that, we introduce the architecture of Comunica in <a href="#querying_architecture">Section 4.4</a>, and its implementation in <a href="#querying_implementation">Section 4.5</a>.
Next, we compare the performance of different Comunica configurations with the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client in <a href="#querying_comparison-tpf-client">Section 4.6</a>.
Finally, <a href="#querying_conclusions">Section 4.7</a> concludes and discusses future work.</p>

      </div>
</section>

    <section id="querying_related-work" inlist="" rel="schema:hasPart" resource="#querying_related-work">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Related Work</h3>

        <p>In this section, we illustrate the many possible degrees of freedom for <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation,
and show that they are hard to combine, which is the problem we aim to solve with Comunica.
We first discuss the <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query language, its engines, and algorithms.
After that, we discuss alternative Linked Data publishing interfaces, and their connection to querying.
Finally, we discuss the software design patterns that are essential in the architecture of Comunica.</p>

        <h4 id="the-different-facets-of-sparql">The Different Facets of SPARQL</h4>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL</a> <span class="references">[<a href="#ref-3">3</a>]</span> is the W3C-recommended <abbr title='Resource Description Framework'>RDF</abbr> query language.
The traditional way to implement a SPARQL query processor
is to use it as an interface to an underlying database,
resulting in a so-called <a property="schema:citation http://purl.org/spar/cito/citeAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><em>SPARQL endpoint</em></a> <span class="references">[<a href="#ref-86">86</a>]</span>.
This is similar to how an SQL interface
provides access to a relation database.
The internal storage can either be a native <abbr title='Resource Description Framework'>RDF</abbr> store, e.g., AllegroGraph <span class="references">[<a href="#ref-95">95</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.taylorfrancis.com/books/e/9780429102455/chapters/10.1201/b16859-17">Blazegraph</a> <span class="references">[<a href="#ref-75">75</a>]</span>,
or a non-RDF store, e.g., <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://doi.org/10.1007/978-3-642-04329-1_21">Virtuoso</a> <span class="references">[<a href="#ref-43">43</a>]</span> uses a object-relational database management system.</p>

        <p>Various algorithms have been proposed for optimized <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation.
Some algorithms for example use the concept of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675">query rewriting</a></span> <span class="references">[<a href="#ref-82">82</a>]</span> based on algebraic equivalent query operations,
others have proposed the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1367497.1367578"><a href="http://doi.acm.org/10.1145/1367497.1367578">optimization of Basic Graph Pattern evaluation</a></span> <span class="references">[<a href="#ref-83">83</a>]</span> using selectivity estimation of triple patterns.</p>

        <p>In order to evaluate <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries over datasets of different storage types,
<abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query frameworks were developed, such as
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://ieeexplore.ieee.org/iel5/4236/22924/01067737.pdf">Jena (ARQ)</a> <span class="references">[<a href="#ref-78">78</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdflib.readthedocs.io/en/stable/">RDFLib</a> <span class="references">[<a href="#ref-96">96</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/linkeddata/rdflib.js">rdflib.js</a> <span class="references">[<a href="#ref-97">97</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/antoniogarrote/rdfstore-js">rdfstore-js</a> <span class="references">[<a href="#ref-98">98</a>]</span>.
Jena is a Java framework, RDFLib is a python package, and rdflib.js and rdfstore-js are JavaScript modules.
Jena—or more specifically the ARQ API—and RDFLib are fully <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL 1.1</a> <span class="references">[<a href="#ref-3">3</a>]</span> compliant.
rdflib.js and rdfstore-js both support a subset of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1.
These <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engines support in-memory models or other sources,
such as Jena TDB in the case of ARQ.
Most of the query algorithms are tightly coupled to these frameworks,
which makes swapping out query algorithms for specific query operators hard or sometimes even impossible.
Furthermore, complex things such as federated querying over heterogeneous interfaces are difficult to implement using these frameworks,
as they are not supported out-of-the-box.
This issue of modularity and heterogeneity are two of the main problems we aim to solve within Comunica.
The differences between Comunica and existing frameworks will be explained in more detail in <a href="#features"></a>.</p>

        <p>The <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments client</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> (also known as Client.js or <code>ldf-client</code>) is a client-side <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engine
that retrieves data over <abbr title='Hypertext Transfer Protocol'>HTTP</abbr>
through <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (TPF) interfaces</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">Different algorithms</a> <span class="references">[<a href="#ref-88">88</a>, <a href="#ref-93">93</a>, <a href="#ref-94">94</a>]</span> for this client and
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/iswc2015-amf.pdf">TPF interface extensions</a> <span class="references">[<a href="#ref-89">89</a>, <a href="#ref-90">90</a>, <a href="#ref-92">92</a>, <a href="#ref-40">40</a>]</span> have been proposed to reduce effort of server or client in some way.
All of these efforts are however implemented and evaluated in isolation.
Furthermore, the implementations are tied to <abbr title='Triple Pattern Fragments'>TPF</abbr> interface, which makes it impossible to use them for other types of datasources and interfaces.
With Comunica, we aim to solve this by modularizing query operation implementations into separate modules,
so that they can be plugged in and combined in different ways, on top of different datasources and interfaces.</p>

        <p>With Semantic Web technologies providing the capability
to integrate data from different sources,
<em>federated query processing</em> has been an active area of research.
However, most of the existing frameworks require <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints on every source.
The <abbr title='Triple Pattern Fragments'>TPF</abbr> Client instead federates over <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces,
and achieves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">similar performance compared to the state of the art</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>
despite its usage of a more lightweight interface.
However, no frameworks exist that enable federation over heterogeneous interfaces,
such as the federation over any combination of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints and <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.
With Comunica, we aim to fill this gap.
In addition dataset-centric approaches,
alternative methods such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based query evaluation</a> <span class="references">[<a href="#ref-87">87</a>]</span> exist
to query a web of Linked Data documents.</p>

        <h4 id="linked-data-fragments">Linked Data Fragments</h4>

        <p>In order to formally capture the heterogeneity of different Web interfaces to publish <abbr title='Resource Description Framework'>RDF</abbr> data,
the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Linked Data Fragment</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> (LDF) conceptual framework
uniformly characterizes responses of Web interfaces to RDF-based knowledge graphs.
The simplest type of LDF is a <em>data dump</em>—it is the response of a single <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> requests for a complete <abbr title='Resource Description Framework'>RDF</abbr> dataset.
Other types of LDFs includes responses of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints,
<abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, and Linked Data documents.</p>

        <p>Existing LDF research highlights that,
when it comes to publishing datasets on the Web, there is no silver bullet:
no single interface works well in all situations,
as each one involves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">trade-offs</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>.
As such, data publishers must choose the type of interface that matches their intended use case, target audience and infrastructure.
This however complicates client-side engines that need to retrieve data from the resulting heterogeneity of interfaces.
As shown by the <abbr title='Triple Pattern Fragments'>TPF</abbr> approach, interfaces can be self-descriptive and expose one or more <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.07108">features</a> <span class="references">[<a href="#ref-99">99</a>]</span>,
to describe their functionality using a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-03.pdf">common vocabulary</a> <span class="references">[<a href="#ref-100">100</a>, <a href="#ref-101">101</a>]</span>.
This allows clients without prior knowledge of the exact inputs and outputs of an interface
to discover its usage at runtime.</p>

        <p>A design goal of Comunica is to
facilitate interaction with any current and future interface
within the LDF framework,
both in single-source and federated scenarios.</p>

        <h4 id="software-design-patterns">Software Design Patterns</h4>

        <p>In the following, we discuss three software design patterns that are relevant to the modular design of the Comunica engine.</p>

        <h5 id="publishsubscribe-pattern">Publish–subscribe pattern</h5>

        <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf"><em>publish-subscribe</em></a> <span class="references">[<a href="#ref-102">102</a>]</span> design pattern involves passing <em>messages</em> between <em>publishers</em> and <em>subscribers</em>.
Instead of programming publishers to send messages directly to subscribers, they are programmed to <em>publish</em> messages to certain <em>categories</em>.
Subscribers can <em>subscribe</em> to these categories which will cause them to receive these published messages, without requiring prior knowledge of the publishers.
This pattern is useful for decoupling software components from each other,
and only requiring prior knowledge of message categories.
We use this pattern in Comunica for allowing different implementations of certain tasks to subscribe to task-specific buses.</p>

        <h5 id="actor-model">Actor Model</h5>

        <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="http://worrydream.com/refs/Hewitt-ActorModel.pdf"><em>actor</em> model</a> <span class="references">[<a href="#ref-103">103</a>]</span> was designed as a way to achieve highly parallel systems consisting of many independent <em>agents</em>
communicating using messages, similar to the publish–subscribe pattern.
An actor is a computational unit that performs a specific task, acts on messages, and can send messages to other actors.
The main advantages of the actor model are that actors can be independently made to implement certain specific tasks based on messages,
and that these can be handled asynchronously.
These characteristics are highly beneficial to the modularity that we want to achieve with Comunica.
That is why we use this pattern in combination with the publish–subscribe pattern to let each implementation of a certain task correspond to a separate actor.</p>

        <h5 id="mediator-pattern">Mediator pattern</h5>

        <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.oreilly.com/library/view/design-patterns-elements/0201633612/"><em>mediator</em></a> <span class="references">[<a href="#ref-104">104</a>]</span> pattern is able to reduce coupling between software components that interact with each other,
and to easily change the interaction if needed.
This can be achieved by encapsulating the interaction between software components in a mediator component.
Instead of the components having to interact with each other directly,
they now interact through the mediator.
These components therefore do not require prior knowledge of each other,
and different implementations of these mediators can lead to different interaction results.
In Comunica, we use this pattern to handle actions when multiple actors are able to solve the same task,
by for example choosing the <em>best</em> actor for a task, or by combining the solutions of all actors.</p>

      </div>
</section>

    <section id="querying_features" inlist="" rel="schema:hasPart" resource="#querying_features">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Requirement analysis</h3>

        <p>In this section, we discuss the main requirements and features of the Comunica framework
as a research platform for <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation.
Furthermore, we discuss each feature based on the availability in related work.
The main feature requirements of Comunica are the following:</p>

        <dl>
          <dt>SPARQL query evaluation</dt>
          <dd>The engine should be able to interpret, process and output results for <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries.</dd>
          <dt>Modularity</dt>
          <dd>Different independent modules should contain the implementation of specific tasks, and they should be combinable in a flexible framework. The configurations should be describable in <abbr title='Resource Description Framework'>RDF</abbr>.</dd>
          <dt>Heterogeneous interfaces</dt>
          <dd>Different types of datasource interfaces should be supported, and it should be possible to add new types independently.</dd>
          <dt>Federation</dt>
          <dd>The engine should support federated querying over different interfaces.</dd>
          <dt>Web-based</dt>
          <dd>The engine should run in Web browsers using native Web technologies.</dd>
        </dl>

        <p>In <a href="#querying_features-comparison">Table 15</a>, we summarize the availability of these features in similar works.</p>

        <figure id="querying_features-comparison" class="table">

          <table>
            <thead>
              <tr>
                <th>Feature</th>
                <th>TPF Client</th>
                <th>ARQ</th>
                <th>RDFLib</th>
                <th>rdflib.js</th>
                <th>rdfstore-js</th>
                <th>Comunica</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SPARQL</td>
                <td>✓(1)</td>
                <td>✓</td>
                <td>✓</td>
                <td>✓(1)</td>
                <td>✓(1)</td>
                <td>✓(1)</td>
              </tr>
              <tr>
                <td>Modularity</td>
                <td> </td>
                <td> </td>
                <td> </td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Heterogeneous interfaces</td>
                <td> </td>
                <td>✓(2,3)</td>
                <td>✓(2,3)</td>
                <td>✓(3)</td>
                <td>✓(3)</td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Federation</td>
                <td>✓</td>
                <td>✓(4)</td>
                <td>✓(4)</td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Web-based</td>
                <td>✓</td>
                <td> </td>
                <td> </td>
                <td>✓</td>
                <td>✓</td>
                <td>✓</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 15:</span> Comparison of the availability of the main features of Comunica in similar works.
(1) A subset of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 is implemented.
(2) Querying over <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints, other types require implementing an internal storage interface.
(3) Downloading of dumps.
(4) Federation only over <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints using the SERVICE keyword.</p>
          </figcaption>
        </figure>

        <h4 id="sparql-query-evaluation">SPARQL query evaluation</h4>

        <p>The recommended way of querying within <abbr title='Resource Description Framework'>RDF</abbr> data, is using the <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query language.
All of the discussed frameworks support at least the parsing and execution of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries, and reporting of results.</p>

        <h4 id="modularity">Modularity</h4>

        <p>Adding new functionality or changing certain operations in Comunica should require minimal to no changes to existing code.
Furthermore, the Comunica environment should be developer-friendly, including well documented APIs and auto-generation of stub code.
In order to take full advantage of the Linked Data stack, modules in Comunica must be describable, configurable and wireable in <abbr title='Resource Description Framework'>RDF</abbr>.
By registering or excluding modules from a configuration file, the user is free to choose how heavy or lightweight the query engine will be.
Comunica’s modular architecture will be explained in <a href="#querying_architecture">Section 4.4</a>.
ARQ, RDFLib, rdflib.js and rdfstore-js only support customization by implementing a custom query engine programmatically to handle operators.
They do not allow plugging in or out certain modules.</p>

        <h4 id="heterogeneous-interfaces">Heterogeneous interfaces</h4>

        <p>Due to the existence of different types of Linked Data Fragments for exposing Linked Datasets,
Comunica should support <em>heterogeneous</em> interfaces types, including self-descriptive Linked Data interfaces such as <abbr title='Triple Pattern Fragments'>TPF</abbr>.
This <abbr title='Triple Pattern Fragments'>TPF</abbr> interface is the only interface that is supported by the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Additionally, Comunica should also enable querying over other sources,
such as <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints and data dumps in <abbr title='Resource Description Framework'>RDF</abbr> serializations.
The existing <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> frameworks mostly support querying against <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints,
local graphs, and specific storage types using an internal storage adapter.</p>

        <h4 id="federation">Federation</h4>

        <p>Next to the different type of Linked Data Fragments for exposing Linked Datasets,
data on the Web is typically spread over <em>different</em> datasets, at different locations.
As mentioned in <a href="#querying_related-work">Section 4.2</a>, federated query processing is a way to query over the combination of such datasets,
without having to download the complete datasets and querying over them locally.
The <abbr title='Triple Pattern Fragments'>TPF</abbr> client supports federated query evaluation over its single supported interface type, i.e., <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.
ARQ and RDFLib only support federation over <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints using the SERVICE keyword.
Comunica should enable <em>combined</em> federated querying over its supported heterogeneous interfaces.</p>

        <h4 id="web-based">Web-based</h4>

        <p>Comunica must be built using native Web technologies, such as JavaScript and <abbr title='Resource Description Framework'>RDF</abbr> configuration documents.
This allows Comunica to run in different kinds of environments, including Web browsers, local (JavaScript) runtime engines and command-line interfaces,
just like the TPF-client, rdflib.js and rdfstore-js.
ARQ and RDFLib are able to run in their language’s runtime and via a command-line interface, but not from within Web browsers.
ARQ would be able to run in browsers using a custom Java applet, which is not a native Web technology.</p>

      </div>
</section>

    <section id="querying_architecture" inlist="" rel="schema:hasPart" resource="#querying_architecture">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Architecture</h3>

        <p>In this section, we discuss the design and architecture of the Comunica meta engine,
and show how it conforms to the <em>modularity</em> feature requirement.
In summary, Comunica is collection of small modules that, when wired together,
are able to perform a certain task, such as evaluating <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries.
We first discuss the customizability of Comunica at design-time,
followed by the flexibility of Comunica at run-time.
Finally, we give an overview of all modules.</p>

        <h4 id="customizable-wiring-at-design-time-through-dependency-injection">Customizable Wiring at Design-time through Dependency Injection</h4>

        <p>There is no such thing as <em>the</em> Comunica engine,
instead, Comunica is a meta engine that can be <em>instantiated</em> into different engines based on different configurations.
Comunica achieves this customizability at design-time using the concept of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://martinfowler.com/articles/injection.html"><em>dependency injection</em></a> <span class="references">[<a href="#ref-105">105</a>]</span>.
Using a configuration file, which is created before an engine is started,
components for an engine can be <em>selected</em>, <em>configured</em> and <em>combined</em>.
For this, we use the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://componentsjs.readthedocs.io/en/latest/">Components.js</a> <span class="references">[<a href="#ref-106">106</a>]</span> JavaScript dependency injection framework,
This framework is based on semantic module descriptions and configuration files
using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">Object-Oriented Components ontology</a> <span class="references">[<a href="#ref-107">107</a>]</span>.</p>

        <h5 id="description-of-individual-software-components">Description of Individual Software Components</h5>

        <p>In order to refer to Comunica components from within configuration files,
we semantically describe all Comunica components using the Components.js framework in <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/json-ld/">JSON-LD</a> <span class="references">[<a href="#ref-108">108</a>]</span>.
<a href="#querying_config-actor">Listing 3</a> shows an example of the semantic description of an <abbr title='Resource Description Framework'>RDF</abbr> parser.</p>

        <h5 id="description-of-complex-software-configurations">Description of Complex Software Configurations</h5>

        <p>A specific instance of a Comunica engine
can be <em>initialized</em> using Components.js configuration files
that describe the wiring between components.
For example, <a href="#querying_config-parser">Listing 4</a> shows a configuration file of an engine that is able to parse N3 and JSON-LD-based documents.
This example shows that, due to its high degree of modularity,
Comunica can be used for other purposes than a query engine,
such as building a custom <abbr title='Resource Description Framework'>RDF</abbr> parser.</p>

        <p>Since many different configurations can be created,
it is important to know which one was used for a specific use case or evaluation.
For that purpose,
the <abbr title='Resource Description Framework'>RDF</abbr> documents that are used to instantiate a Comunica engine
can be <a property="schema:citation http://purl.org/spar/cito/citeAsEvidence" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">published as Linked Data</a> <span class="references">[<a href="#ref-107">107</a>]</span>.
They can then serve as provenance
and as the basis for derived set-ups or evaluations.</p>

        <figure id="querying_config-actor" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;npmd:@comunica/actor-rdf-parse-n3&quot;,
</code><code>  &quot;components&quot;: [
</code><code>    {
</code><code>      &quot;@id&quot;:            &quot;crpn3:Actor/RdfParse/N3&quot;,
</code><code>      &quot;@type&quot;:          &quot;Class&quot;,
</code><code>      &quot;extends&quot;:        &quot;cbrp:Actor/RdfParse&quot;,
</code><code>      &quot;requireElement&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;comment&quot;:        &quot;An actor that parses Turtle-like RDF&quot;,
</code><code>      &quot;parameters&quot;: [
</code><code>        {
</code><code>          &quot;@id&quot;: &quot;caam:Actor/AbstractMediaTypedFixed/mediaType&quot;,
</code><code>          &quot;default&quot;: [ &quot;text/turtle&quot;, &quot;application/n-triples&quot; ]
</code><code>        }
</code><code>      ]
</code><code>    }
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 3:</span> Semantic description of a component that is able to parse N3-based <abbr title='Resource Description Framework'>RDF</abbr> serializations.
This component has a single parameter that allows media types to be registered that this parser is able to handle.
In this case, the component has four default media types.</p>
          </figcaption>
</figure>

        <figure id="querying_config-parser" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;http://example.org/myrdfparser&quot;,
</code><code>  &quot;@type&quot;: &quot;Runner&quot;,
</code><code>  &quot;actors&quot;: [
</code><code>    { &quot;@type&quot;: &quot;ActorInitRdfParse&quot;,
</code><code>      &quot;mediatorRdfParse&quot;: {
</code><code>        &quot;@type&quot;: &quot;MediatorRace&quot;,
</code><code>        &quot;cc:Mediator/bus&quot;: { &quot;@id&quot;: &quot;cbrp:Bus/RdfParse&quot; }
</code><code>      } },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseJsonLd&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 4:</span> Comunica configuration of <code>ActorInitRdfParse</code> for parsing an <abbr title='Resource Description Framework'>RDF</abbr> document in an unknown serialization.
This actor is linked to a mediator with a bus containing two <abbr title='Resource Description Framework'>RDF</abbr> parsers for specific serializations.</p>
          </figcaption>
</figure>

        <h4 id="flexibility-at-run-time-using-the-actormediatorbus-pattern">Flexibility at Run-time using the Actor–Mediator–Bus Pattern</h4>

        <p>Once a Comunica engine has been configured and initialized,
components can interact with each other in a flexible way using the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://worrydream.com/refs/Hewitt-ActorModel.pdf"><em>actor</em></a> <span class="references">[<a href="#ref-103">103</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.oreilly.com/library/view/design-patterns-elements/0201633612/"><em>mediator</em></a> <span class="references">[<a href="#ref-104">104</a>]</span>, and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf"><em>publish–subscribe</em></a> <span class="references">[<a href="#ref-102">102</a>]</span> patterns.
Any number of <em>actor</em>, <em>mediator</em> and <em>bus</em> modules can be created,
where each actor interacts with mediators, that in turn invoke other actors that are registered to a certain bus.</p>

        <p><a href="#querying_actor-mediator-bus">Fig. 37</a> shows an example logic flow between actors through a mediator and a bus.
The relation between these components, their phases and the chaining of them will be explained hereafter.</p>

        <figure id="querying_actor-mediator-bus">
<img src="querying/img/actor-mediator-bus.svg" alt="[actor-mediator-bus pattern]" class="figure-small" />
<figcaption>
            <p><span class="label">Fig. 37:</span> Example logic flow where Actor 0 requires an <em>action</em> to be performed.
This is done by sending the action to the Mediator, which sends a <em>test action</em> to Actors 1, 2 and 3 via the Bus.
The Bus then sends all <em>test replies</em> to the Mediator,
which chooses the best actor for the action, in this case Actor 3.
Finally, the Mediator sends the original action to Actor 3, and returns its response to Actor 0.</p>
          </figcaption>
</figure>

        <h5 id="relation-between-actors-and-buses">Relation between Actors and Buses</h5>

        <p>Actors are the main computational units in Comunica, and buses and mediators form the <em>glue</em> that ties them together and makes them interactable.
Actors are responsible for being able to accept certain messages
via the bus to which they are subscribed,
and for responding with an answer.
In order to avoid a single high-traffic bus for all message types which could cause performance issues,
separate buses exist for different message types.
<a href="#querying_relation-actor-bus">Fig. 38</a> shows an example of how actors can be registered to buses.</p>

        <figure id="querying_relation-actor-bus">
<img src="querying/img/relation-actor-bus.svg" alt="[relation between actors and buses]" class="figure-small" />
<figcaption>
            <p><span class="label">Fig. 38:</span> An example of two different buses each having two subscribed actors.
The left bus has different actors for parsing triples in a certain <abbr title='Resource Description Framework'>RDF</abbr> serialization to triple objects.
The right bus has actors that join query bindings streams together in a certain way.</p>
          </figcaption>
</figure>

        <h5 id="mediators-handle-actor-run-and-test-phases">Mediators handle Actor Run and Test Phases</h5>

        <p>Each mediator is connected to a single bus, and its goal is to determine and invoke the <em>best</em> actor for a certain task.
The definition of ‘<em>best</em>’ depends on the mediator, and different implementations can lead to different choices in different scenarios.
A mediator works in two phases: the <em>test</em> phase and the <em>run</em> phase.
The test phase is used to check under which conditions the action can be performed in each actor on the bus.
This phase must always come before the <em>run</em> phase, and is used to select which actor is best suited to perform a certain task under certain conditions.
If such an actor is determined, the <em>run</em> phase of a single actor is initiated.
This <em>run</em> phase takes this same type of message, and requires to <em>effectively act</em> on this message,
and return the result of this action.
<a href="#querying_run-test-phases">Fig. 39</a> shows an example of a mediator invoking a run and test phase.</p>

        <figure id="querying_run-test-phases">
<img src="querying/img/run-test-phases.svg" alt="[mediators handle actor run and test phases]" />
<figcaption>
            <p><span class="label">Fig. 39:</span> Example sequence diagram of a mediator that chooses the fastest actor
on a parse bus with two subscribed actors.
The first parser is very fast but requires a lot of memory,
while the second parser is slower, but requires less memory.
Which one is best, depends on the use case and is determined by the Mediator.
The mediator first calls the <em>tests</em> the actors for the action, and then <em>runs</em> the action using the <em>best</em> actor.</p>
          </figcaption>
</figure>

        <h4 id="modules">Modules</h4>

        <p>At the time of writing, Comunica consists of 79 different modules.
This consists of 13 buses, 3 mediator types, 57 actors and 6 other modules.
In this section, we will only discuss the most important actors and their interactions.</p>

        <p>The main bus in Comunica is the <em>query operation</em> bus, which consists of 19 different actors
that provide at least one possible implementation of the typical <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> operations such as quad patterns, basic graph patterns (BGPs), unions, projects, …
These actors interact with each other using streams of <em>quad</em> or <em>solution mappings</em>,
and act on a query plan expressed in in <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL algebra</a> <span class="references">[<a href="#ref-3">3</a>]</span>.</p>

        <p>In order to enable heterogeneous sources to be queried in a federated way,
we allow a list of sources, annotated by type, to be passed when a query is initiated.
These sources are passed down through the chain of query operation actors,
until the quad pattern level is reached.
At this level, different actors exist for handling a single source of a certain type,
such as <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints, local or remote datadumps.
In the case of multiple sources, one actor exists that implements a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation algorithm defined for TPF</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>,
but instead of federating over different <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, it federates over different single-source quad pattern actors.</p>

        <p>At the end of the pipeline, different actors are available for serializing the results of a query in different ways.
For instance, there are actors for serializing the results according to
the <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">JSON</a> <span class="references">[<a href="#ref-109">109</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/rdf-sparql-XMLres/">XML</a> <span class="references">[<a href="#ref-110">110</a>]</span> result specifications,
but actors with more visual and developer-friendly formats are available as well.</p>

      </div>
</section>

    <section id="querying_implementation" inlist="" rel="schema:hasPart" resource="#querying_implementation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Implementation</h3>

        <p>Comunica is implemented in TypeScript/JavaScript as a collection of Node modules, which are able to run in Web browsers using native Web technologies.
Comunica is available under an open license on <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​1202509#.Wq9GZhNuaHo">GitHub</a>
and on the <a href="https://www.npmjs.com/org/comunica" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​org/​comunica">NPM package manager</a>.
The 79 Comunica modules are tested thoroughly, with more than 1,200 unit tests reaching a test coverage of 100%.
In order to be compatible with existing JavaScript <abbr title='Resource Description Framework'>RDF</abbr> libraries,
Comunica follows the JavaScript <abbr title='Application Programming Interface'>API</abbr> specification by the <a href="https://www.w3.org/community/rdfjs/" class="mandatory" data-link-text="https:/​/​www.w3.org/​community/​rdfjs/​">RDFJS community group</a>,
and will <a href="https://www.w3.org/community/rdfjs/2018/04/23/rdf-js-the-new-rdf-and-linked-data-javascript-library/">actively be further aligned</a> within this community.
In order to encourage collaboration within the community, we extensively use the <a href="https://github.com/comunica/comunica/issues" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​issues">GitHub issue tracker</a>
for planned features, bugs and other issues.
Finally, we publish detailed <a href="https://comunica.readthedocs.io" class="mandatory" data-link-text="https:/​/​comunica.readthedocs.io">documentation</a> for the usage and development of Comunica.</p>

        <p>We provide a default Linked Data-based configuration file with all available actors for evaluating federated <em>SPARQL queries</em> over heterogeneous sources.
This allows <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries to be evaluated using a command-line tool,
from a Web service implementing the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL protocol</a> <span class="references">[<a href="#ref-86">86</a>]</span>,
within a JavaScript application,
or within the browser.
We fully implemented <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/">SPARQL 1.0</a> <span class="references">[<a href="#ref-111">111</a>]</span> and a subset of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL 1.1</a> <span class="references">[<a href="#ref-3">3</a>]</span> at the time of writing.
In future work, we intend to implement additional actors for supporting <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 completely.</p>

        <p>Comunica currently supports querying over the following types of <em>heterogeneous datasources and interfaces</em>:</p>

        <ul>
          <li><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments interfaces</a></span> <span class="references">[<a href="#ref-34">34</a>]</span></li>
          <li>Quad Pattern Fragments interfaces (<a href="https://github.com/LinkedDataFragments/Server.js/tree/feature-qpf-latest" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Server.js/​tree/​feature-​qpf-​latest">an experimental extension of <abbr title='Triple Pattern Fragments'>TPF</abbr> with a fourth graph element</a>)</li>
          <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL endpoints</a> <span class="references">[<a href="#ref-86">86</a>]</span></li>
          <li>Local and remote dataset dumps in <abbr title='Resource Description Framework'>RDF</abbr> serializations.</li>
          <li><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT datasets</a> <span class="references">[<a href="#ref-53">53</a>]</span></li>
          <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-demo/">Versioned <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets</a> <span class="references">[<a href="#ref-112">112</a>]</span></li>
        </ul>

        <p>In order to demonstrate Comunica’s ability to evaluate <em>federated</em> query evaluation over <em>heterogeneous</em> sources,
the following guide shows how you can <a href="https://gist.github.com/rubensworks/34bb69fa6c83176bce60a5e8a25051e8" class="mandatory" data-link-text="https:/​/​gist.github.com/​rubensworks/​34bb69fa6c83176bce60a5e8a25051e8">try this out in Comunica yourself</a>.</p>

        <p>Support for new algorithms, query operators and interfaces can be implemented in an external module,
without having to create a custom fork of the engine.
The module can then be <em>plugged</em> into existing or new engines that are identified by
<a href="https://github.com/comunica/comunica/blob/master/packages/actor-init-sparql/config/config-default.json" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​blob/​master/​packages/​actor-​init-​sparql/​config/​config-​default.json">RDF configuration files</a>.</p>

        <p>In the future, we will also look into adding support for other interfaces such as
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF</a></span> <span class="references">[<a href="#ref-92">92</a>]</span> for more efficient join operations
and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">VTPF</a> <span class="references">[<a href="#ref-40">40</a>]</span> for queries over versioned datasets.</p>

      </div>
</section>

    <section id="querying_comparison-tpf-client" inlist="" rel="schema:hasPart" resource="#querying_comparison-tpf-client">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Performance Analysis</h3>

        <p>One of the goals of Comunica is to replace the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client as a more <em>flexible</em> and <em>modular</em> alternative,
with at least the same <em>functionality</em> and similar <em>performance</em>.
The fact that Comunica supports multiple heterogeneous interfaces and sources as shown in the previous section
validates this flexibility and modularity, as the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client only supports querying over <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.</p>

        <p>Next to a functional completeness, it is also desired that Comunica achieves similar <em>performance</em> compared to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
The higher modularity of Comunica is however expected to cause performance overhead,
due to the additional bus and mediator communication, which does not exist in the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Hereafter, we compare the performance of the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client and Comunica
and discover that Comunica has similar performance to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
As the main goal of Comunica is modularity, and not <em>absolute</em> performance, we do not compare with similar frameworks such as ARQ and RDFLib.
Instead, <em>relative</em> performance of evaluations using <em>the same engine</em> under <em>different configurations</em> is key for comparisons,
which will be demonstrated using Comunica hereafter.</p>

        <p>For the setup of this evaluation we used a single machine (Intel Core i5-3230M <abbr title='Central Processing Unit'>CPU</abbr> at 2.60 GHz with 8 GB of RAM),
running the Linked Data Fragments server with a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT-backend</a> <span class="references">[<a href="#ref-53">53</a>]</span> and the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client or Comunica,
for which the exact versions and configurations will be linked in the following workflow.
The main goal of this evaluation is to determine the performance impact of Comunica,
while keeping all other variables constant.</p>

        <p>In order to illustrate the benefit of modularity within Comunica,
we evaluate using two different configurations of Comunica.
The first configuration (<em>Comunica-sort</em>) implements a BGP algorithm that is similar to that of the original <abbr title='Triple Pattern Fragments'>TPF</abbr> Client:
it sorts triple patterns based on their estimated counts and evaluates and joins them in that order.
The second configuration (<em>Comunica-smallest</em>) implements a simplified version of this BGP algorithm that does not sort <em>all</em> triple patterns in a BGP,
but merely picks the triple pattern with the smallest estimated count to evaluate on each recursive call, leading to slightly different query plans.</p>

        <p>We used the following <a about="#evaluation-workflow" content="Comunica evaluation workflow" href="#evaluation-workflow" property="rdfs:label" rel="cc:license" resource="https://creativecommons.org/licenses/by/4.0/">evaluation workflow</a>:</p>

        <ol id="evaluation-workflow" property="schema:hasPart" resource="#evaluation-workflow" typeof="opmw:WorkflowTemplate">
<li id="workflow-data" about="#workflow-data" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Generate a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13"><a href="http://dx.doi.org/10.1007/978-3-319-11964-9_13">WatDiv</a></span> <span class="references">[<a href="#ref-113">113</a>]</span> dataset with scale factor=100.</p>
          </li>
<li id="workflow-queries" about="#workflow-queries" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Generate the corresponding default WatDiv <a href="https://github.com/comunica/test-comunica/tree/ISWC2018/sparql/watdiv-10M" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​test-​comunica/​tree/​ISWC2018/​sparql/​watdiv-​10M">queries</a> with query-count=5.</p>
          </li>
<li id="workflow-tpf-server" about="#workflow-tpf-server" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Install <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-config.jsonld" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​config.jsonld">the server software configuration</a>, implementing the <a href="https://www.hydra-cg.com/spec/latest/triple-pattern-fragments/" class="mandatory" data-link-text="https:/​/​www.hydra-​cg.com/​spec/​latest/​triple-​pattern-​fragments/​">TPF specification</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-setup.ttl">dependencies</a>.</p>
          </li>
<li id="workflow-tpf-client" about="#workflow-tpf-client" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Install <a href="https://github.com/LinkedDataFragments/Client.js" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Client.js">the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client software</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol">SPARQL 1.1 protocol</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-client.ttl" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​client.ttl">dependencies</a>.</p>
          </li>
<li id="workflow-tpf-run" about="#workflow-tpf-run" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Execute the generated WatDiv queries 3 times on the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client, after doing a warmup run, and record the execution times <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-ldf.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​ldf.csv">results</a>.</p>
          </li>
<li id="workflow-comunica-sort" about="#workflow-comunica-srt" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Install <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-sort.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​sort.json">the Comunica software configuration</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol">SPARQL 1.1 protocol</a>, with its <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/comunica-npm.ttl" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​comunica-​npm.ttl">dependencies</a>, using the <em>Comunica-sort</em> algorithm.</p>
          </li>
<li id="workflow-comunica-run-sort" about="#workflow-comunica-run-sort" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica-sort.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica-​sort.csv">execution times</a>.</p>
          </li>
<li id="workflow-comunica-smallest" about="#workflow-comunica-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Update the Comunica installation to use a new <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-smallest.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​smallest.json">configuration</a> supporting the <em>Comunica-smallest</em> algorithm.</p>
          </li>
<li id="workflow-comunica-run-smallest" about="#workflow-comunica-run-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
            <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica.csv">execution times</a>.</p>
          </li>
</ol>

        <figure id="querying_performance-average">
<center>
<img src="querying/img/avg.svg" alt="[performance-average]" class="plot" />
<img src="querying/img/avg_c23.svg" alt="[performance-average]" class="plot" />
</center>
<figcaption>
            <p><span class="label">Fig. 40:</span> Average query evaluation times for the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client, Comunica-sort, and Comunica-smallest for all queries (shorter is better).
C2 and C3 are shown separately because of their higher evaluation times.</p>
          </figcaption>
</figure>

        <p>The results from <a href="#querying_performance-average">Fig. 40</a> show that Comunica is able to achieve similar performance compared to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Concretely, both Comunica variants are faster for 11 queries, and slower for 9 queries.
However, the difference in evaluation times is in most cases very small,
and are caused by implementation details, as the implemented algorithms are equivalent.
Contrary to our expectations, the performance overhead of Comunica’s modularity is negligible.
Comunica therefore improves upon the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client in terms of <em>modularity</em> and <em>functionality</em>, and achieves similar <em>performance</em>.</p>

        <p>These results also illustrate the simplicity of comparing different algorithms inside Comunica.
In this case, we compared an algorithm that is similar to that of the original <abbr title='Triple Pattern Fragments'>TPF</abbr> Client with a simplified variant.
The results show that the performance is very similar, but the original algorithm (Comunica-sort) is faster in most of the cases.
It is however not always faster, as illustrated by query C1, where Comunica-sort is almost a second slower than Comunica-smallest.
In this case, the heuristic algorithm of the latter was able to come up with a slightly better query plan.
Our goal with this result is to show that Comunica can easily be used to compare such different algorithms,
where future work can focus on smart mediator algorithms to choose the best BGP actor in each case.</p>

      </div>
</section>

    <section id="querying_conclusions" inlist="" rel="schema:hasPart" resource="#querying_conclusions">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Conclusions</h3>

        <p>In this work, we introduced Comunica as a highly modular meta engine for federated <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation over heterogeneous interfaces.
Comunica is thereby the first system that accomplishes the Linked Data Fragments vision of a client that is able to query over heterogeneous interfaces.
Not only can Comunica be used as a client-side <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engine, it can also be customized to become a more lightweight engine and perform more specific tasks,
such as for example only evaluating BGPs over Turtle files,
evaluating the efficiency of different join operators,
or even serve as a complete server-side <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query endpoint that aggregates different datasources.
In future work, we will look into supporting supporting alternative (non-semantic) query languages as well, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://facebook.github.io/graphql/October2016/">GraphQL</a> <span class="references">[<a href="#ref-114">114</a>]</span>.</p>

        <p>If you are a Web researcher, then Comunica is the ideal research platform
for investigating new Linked Data publication interfaces,
and for experimenting with different query algorithms.
New modules can be implemented independently without having to fork existing codebases.
The modules can be combined with each other using an RDF-based configuration file
that can be instantiated into an actual engine through dependency injection.
However, the target audience is broader than just the research community.
As Comunica is built on Linked Data and Web technologies,
and is extensively documented and has a ready-to-use <abbr title='Application Programming Interface'>API</abbr>,
developers of RDF-consuming (Web) applications can also make use of the platform.
In the future, we will continue <a href="https://github.com/comunica/comunica/wiki/Sustainability-Plan" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​wiki/​Sustainability-​Plan">maintaining</a>
and developing Comunica and intend to support and collaborate with future researchers on this platform.</p>

        <p>The introduction of Comunica will trigger a <em>new generation of Web querying research</em>.
Due to its flexibility and modularity,
existing areas can be <em>combined</em> and <em>evaluated</em> in more detail,
and <em>new promising areas</em> that remained covered so far will be exposed.</p>

      </div>
</section>

    <div class="subfooter">
  <section id="querying_acknowledgements" inlist="" rel="schema:hasPart" resource="#querying_acknowledgements">
<div datatype="rdf:HTML" property="schema:description">
          <h3 property="schema:name" class="no-label-increment">Acknowledgements</h3>

          <p>The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

        </div>
</section>

</div>
  </section>

  <section class="sub-paper">
    <h2 id="querying-evolving">Querying Evolving Data</h2>

    <section class="sub-preface">
<div datatype="rdf:HTML" property="schema:description">
        <p>The challenge that is handled in this chapter is:
“Publishing <em>evolving</em> data via a <em>queryable interface</em> is costly.”
While the previous chapter focused on querying heterogeneous sources on the Web containing <em>static</em> knowledge graphs,
this chapter focuses on <em>continuous</em> querying on the Web with <em>evolving</em> knowledge graphs.
Compared to <a href="#storing">Chapter 3</a>—in which we introduced a storage technique for evolving knowledge graphs—this chapter focuses on the publishing interface on top of that.
This publishing interface is required for exposing evolving knowledge graphs on the Web.
As such, the interface introduced in this work could be implemented based on the storage backend from <a href="#storing">Chapter 3</a>.</p>

        <p>A query interface that accepts continuous queries over <em>evolving</em> knowledge graphs
inherently requires more server effort compared to one-time queries over <em>static</em> knowledge graphs.
That is because queries need to be evaluated <em>continuously</em> instead of only <em>once</em>.
As such, when evolving knowledge graphs need to be published on the Web,
an interface is needed that scales well in a public Web environment with a potentially large number of concurrent clients.</p>

        <p>The work in this chapter is based on the research question:
“Can clients use volatility knowledge to perform more efficient continuous <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation by polling for data?”.
We answer this research question by introducing a query interface
that exposes evolving knowledge graphs annotated with a description of their volatility.
Based on these descriptions, clients can detect for <em>how long</em> parts of the knowledge graph will remain valid,
and when new queries need to be initiated to calculate next up-to-date results.
We implemented our approach as a system called <em>TPF Query Streamer</em>.
Our evaluations show that the server load with this approach scales better
with an increasing number of concurrent clients
compared other solutions.
This shows that our technique is a good candidate for publishing evolving knowledge graphs on the Web.</p>

      </div>
</section>

    <p class="published-as">Ruben Taelman, Ruben Verborgh, Pieter Colpaert, and Erik Mannens. 2016. <strong><a href="https://www.rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf">Continuous Client-side Query Evaluation over Dynamic Linked Data</a></strong>. In Harald Sack, Giuseppe Rizzo, Nadine Steinmetz, Dunja Mladenić, Sören Auer, &amp; Christoph Lange, eds. Proceedings of the 13th Extended Semantic Web Conference: Satellite events. Lecture Notes in Computer Science. Springer, 273–289.</p>

    <section id="querying-evolving_abstract" inlist="" rel="schema:hasPart" resource="#querying-evolving_abstract">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name" class="no-label-increment">Abstract</h3>

        <p>Existing solutions to query dynamic Linked Data sources extend the <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> language,
and require continuous server processing for each query.
Traditional <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints already accept highly expressive queries,
so extending these endpoints for time-sensitive queries increases the server cost even further.
To make continuous querying over dynamic Linked Data more affordable,
we extend the low-cost Triple Pattern Fragments (TPF)
interface with support for time-sensitive queries.
In this paper, we introduce the <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer that allows clients to evaluate <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries
with continuously updating results.
Our experiments indicate that this extension significantly lowers the server complexity,
at the expense of an increase in the execution time per query.
We prove that by moving the complexity of continuously evaluating queries over
dynamic Linked Data to the clients and thus increasing bandwidth usage,
the cost at the server side is significantly reduced.
Our results show that this solution makes real-time querying more scalable for a large amount
of concurrent clients when compared to the alternatives.</p>

      </div>
</section>

    <section id="querying-evolving_introduction" inlist="" rel="schema:hasPart" resource="#querying-evolving_introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Introduction</h3>

        <p>As the Web of Data is a <em>dynamic</em> dataspace, different results may be returned depending on when a question was asked.
The end-user might be interested in seeing the query results update over time,
for instance, by re-executing the entire query over and over again (“polling”).
This is, however, not very practical,
especially if it is unknown beforehand when data will change.
An additional problem is that
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://link.springer.com/chapter/10.1007/978-3-642-41338-4_18">many public (even static) <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query endpoints suffer from a low availability</a> <span class="references">[<a href="#ref-115">115</a>]</span>.
The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://users.dcc.uchile.cl/~cgutierr/papers/sparql.pdf">unrestricted complexity of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries</a> <span class="references">[<a href="#ref-116">116</a>]</span> combined
with the public character of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints entails a high server cost, which makes it expensive to host such an interface with high availability.
<em>Dynamic</em> <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> streaming solutions offer combined access to dynamic data streams and
static background data through continuously executing queries. Because of this continuous querying, the cost
for these servers is even higher than with static querying.</p>

        <p>In this work, we therefore devise a solution that enables clients to continuously evaluate non-high frequency
queries by polling specific fragments of the data.
The resulting framework performs this without the server needing to remember any client state.
Its mechanism requires the server to <em>annotate</em> its data so that the client can efficiently determine when to retrieve fresh data.
The generic approach in this paper is applied to the use case of public transit route planning.
It can be used in various other domains with continuously updating data, such as smart city dashboards, business intelligence, or sensor networks.
This paper extends our <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_01.pdf">earlier work</a> <span class="references">[<a href="#ref-117">117</a>]</span> with additional experiments.</p>

        <p>In the next section, we discuss related research on which our solution will be based.
After that, <a href="#querying-evolving_problem-statement">Section 5.3</a> gives a general problem statement.
In <a href="#querying-evolving_use-case">Section 5.4</a>, we present a motivating use case.
<a href="#querying-evolving_dynamic-data-representation">Section 5.5</a> discusses different techniques to represent dynamic data,
after which <a href="#querying-evolving_query-engine">Section 5.6</a> gives an explanation of our proposed query solution.
Next, <a href="#querying-evolving_evaluation">Section 5.7</a> shows an overview of our experimental setup and its results.
Finally, <a href="#querying-evolving_conclusions">Section 5.8</a> discusses the conclusions of this work with further research opportunities.</p>

      </div>
</section>

    <section id="querying-evolving_related-work" inlist="" rel="schema:hasPart" resource="#querying-evolving_related-work">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Related Work</h3>

        <p>In this section, we first explain techniques to perform <abbr title='Resource Description Framework'>RDF</abbr> annotation,
which will be used to determine freshness.
Then, we zoom in on possible representations of temporal data in RDF.
We finish by discussing existing <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> streaming extensions
and a low-cost (static) Linked Data publication technique.</p>

        <h4 id="querying-evolving_related-work_annotations">RDF Annotations</h4>

        <p>Annotations allow us to attach metadata to triples.
We might for example want to say that a triple is
only valid within a certain time interval, or that a triple is only valid in a certain geographical area.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/">RDF 1.0</a> <span class="references">[<a href="#ref-118">118</a>]</span> allows triple annotation through <em>reification</em>.
This mechanism uses <em>subject</em>, <em>predicate</em>, and <em>object</em> as predicates, which allow the addition of annotations
to such reified <abbr title='Resource Description Framework'>RDF</abbr> triples.
The downside of this approach is that one triple is now transformed to three triples, which significantly increases the
total amount of triples.</p>

        <p><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/2566486.2567973"><a href="http://dl.acm.org/citation.cfm?id=2567973">Singleton Properties</a></span> <span class="references">[<a href="#ref-119">119</a>]</span> create unique instances (singletons) of predicates, which then can be used for further specifying
that relationship, for example, by adding annotations. New instances of predicates are created by relating them to the
old predicate through the <code>sp:singletonPropertyOf</code> predicate.
While this approach requires fewer triples than reification to represent the same information, it still has
the issue of the original triple being lost, because the predicate is changed in this approach.</p>

        <p>With <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF 1.1</a> <span class="references">[<a href="#ref-2">2</a>]</span> came <em>graph</em> support, which allows triples to be encapsulated into named graphs, which can also be annotated.
Graph-based annotation requires fewer triples than both reification and singleton properties when representing
the same information. It requires the addition of a fourth element to the triple which transforms it to a quad.
This fourth element, the <em>graph</em>, can be used to add the annotations to.</p>

        <h4 id="temporal-data-in-the-rdf-model">Temporal data in the <abbr title='Resource Description Framework'>RDF</abbr> model</h4>

        <p>Regular <abbr title='Resource Description Framework'>RDF</abbr> triples cannot express the time and space in which the fact they describe is true.
In domains where data needs to be represented for certain times or time ranges, these traditional representations
should thus be extended.
There are <a property="schema:citation http://purl.org/spar/cito/cites" href="http://link.springer.com/chapter/10.1007/11431053_7">two main mechanisms for adding time</a> <span class="references">[<a href="#ref-120">120</a>]</span>.
<em>Versioning</em> will take snapshots of the complete graph every time a change occurs.
<em>Time labeling</em> will annotate triples with their change time.
The latter is believed to be a better approach in the context of <abbr title='Resource Description Framework'>RDF</abbr>,
because complete snapshots introduce overhead,
especially if only a small part of the graph changes.
Gutierrez et al. made a distinction between <em>point-based</em> and <em>interval-based</em> labeling,
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1109/TKDE.2007.34"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4039284">which are interchangeable</a></span> <span class="references">[<a href="#ref-121">121</a>]</span>.
The former states information about an element at a certain time instant, while the latter states
information at all possible times between two time instants.</p>

        <p>The same authors introduced a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1109/TKDE.2007.34"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4039284">temporal vocabulary</a></span> <span class="references">[<a href="#ref-121">121</a>]</span> for the discussed mechanisms, which will
be referred to as <code>tmp</code> in the remainder of this chapter. Its core predicates are:</p>

        <ul>
          <li><code>tmp:interval</code>: This predicate can be used on a subject to make it valid in a certain time interval.
  The range of this property is a time interval, which is represented by the two mandatory
  properties <code>tmp:initial</code> and <code>tmp:final</code>.</li>
          <li><code>tmp:instant</code>: Used on subjects to make it valid on a certain time instant as a point-based time representation.
  The range of this property is <code>xsd:dateTime</code>.</li>
          <li><code>tmp:initial</code> and <code>tmp:final</code>: The domain of these predicates is a time interval.
  Their range is a <code>xsd:dateTime</code>, and they respectively indicate the start and the
  end of the interval-based time representation.</li>
        </ul>

        <p>Next to these properties, we will also introduce our own predicate <code>tmp:expiration</code> with range <code>xsd:dateTime</code>
which indicates that the subject is only valid up until the given time.</p>

        <h4 id="sparql-streaming-extensions">SPARQL Streaming Extensions</h4>

        <p>Several <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> extensions exist that enable querying over data streams.
These data streams are traditionaly represented as a monotonically non-decreasing stream of triples that are annotated with their timestamp.
These require <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf"><em>continuous processing</em></a> <span class="references">[<a href="#ref-31">31</a>]</span> of queries because of the constantly changing data.</p>

        <p><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1860702.1860705"><a href="http://doi.acm.org/10.1145/1860702.1860705">C-SPARQL</a></span> <span class="references">[<a href="#ref-14">14</a>]</span> is an approach to querying over static and dynamic data.
This system requires the client to <em>register</em> a query to the server in an extended
<abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> syntax that allows the use of <em>windows</em> over dynamic data.
This <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf"><em>query registration</em></a> <span class="references">[<a href="#ref-31">31</a>, <a href="#ref-122">122</a>]</span>
must occur by clients to make sure that the streaming-enabled <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoint can continuously re-evaluate this query,
as opposed to traditional endpoints where the query is evaluated only once.
A <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ilpubs.stanford.edu:8090/641/1/2004-20.pdf"><em>window</em></a> <span class="references">[<a href="#ref-123">123</a>]</span> is a subsection of facts ordered by time so that not all available
information has to be taken into account while processing. These windows can have a certain size which
indicates the time range and is advanced in time by a <em>stepsize</em>.
C-SPARQL’s execution of queries is based on the combination of a regular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engine with a
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://ilpubs.stanford.edu:8090/641/1/2004-20.pdf"><em>Data Stream Management System</em> (DSMS)</a> <span class="references">[<a href="#ref-123">123</a>]</span>. The internal model of <abbr title='Continuous SPARQL'>C-SPARQL</abbr> 
creates queries that distribute work between the <abbr title='Data Stream Management System'>DSMS</abbr> and the <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> engine to respectively
process the dynamic and static data.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf"><em>CQELS</em></a> <span class="references">[<a href="#ref-15">15</a>]</span> is a <em>white box</em> approach, as opposed to <em>black box</em>
approaches like <abbr title='Continuous SPARQL'>C-SPARQL</abbr>.
This means that <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> natively implements all query operators without transforming it to another
language, removing the overhead of delegating it to another system.
The syntax is similar to that of <abbr title='Continuous SPARQL'>C-SPARQL</abbr>, also supporting query registration and time windows.
According to previous research <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> performs much better than <abbr title='Continuous SPARQL'>C-SPARQL</abbr> for large
datasets; for simple queries and small datasets the opposite is true.</p>

        <h4 id="triple-pattern-fragments">Triple Pattern Fragments</h4>

        <p>Experiments have shown that <a property="schema:citation http://purl.org/spar/cito/cites" href="http://link.springer.com/chapter/10.1007/978-3-642-41338-4_18">more than half of public <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints have an availability of less than 95%</a> <span class="references">[<a href="#ref-115">115</a>]</span>.
Any number of clients can send arbitrarily complex <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries, which could form a bottleneck in endpoints.
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><em>Triple Pattern Fragments</em> (TPF)</a></span> <span class="references">[<a href="#ref-34">34</a>]</span> aim to solve this issue of high interface cost by moving part of
the query evaluation to the client, which reduces the server load, at the cost of increased query times and bandwidth.
The purposely limited interface only accepts separate triple pattern queries.
Clients can use it to evaluate more complex <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> queries locally,
also over federations of interfaces.</p>

      </div>
</section>

    <section id="querying-evolving_problem-statement" inlist="" rel="schema:hasPart" resource="#querying-evolving_problem-statement">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Problem Statement</h3>

        <p>In order to lower server load during continuous query evaluation,
we move a significant part of the query evaluation from server to client.
We annotate dynamic data with their valid time to make it possible for clients
to derive an optimal query evaluation frequency.</p>

        <p>For this research, we identified the following research questions:</p>

        <div rel="schema:question">
          <blockquote id="querying-evolving_researchquestion1" about="#querying-evolving_researchquestion1" property="schema:name">
            <p>Can clients use volatility knowledge to perform more efficient continuous <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation by polling for data?</p>
          </blockquote>

          <blockquote id="querying-evolving_researchquestion2" about="#querying-evolving_researchquestion2" property="schema:name">
            <p>How does the client and server load of our solution compare to alternatives?</p>
          </blockquote>

          <blockquote id="querying-evolving_researchquestion3" about="#querying-evolving_researchquestion3" property="schema:name">
            <p>How do different time-annotation methods perform in terms of the resulting execution times?</p>
          </blockquote>
        </div>

        <p>These research questions lead to the following hypotheses:</p>

        <ol rel="lsc:tests">
    <li id="querying-evolving_hypothesis1" about="#querying-evolving_hypothesis1" property="schema:name">The proposed framework has a lower server cost than alternatives.</li>
    <li id="querying-evolving_hypothesis2" about="#querying-evolving_hypothesis2" property="schema:name">The proposed framework has a higher client cost than streaming-based <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> approaches for equivalent queries.</li>
    <li id="querying-evolving_hypothesis3" about="#querying-evolving_hypothesis3" property="schema:name">Client-side caching of static data reduces the execution times proportional to the fraction of static triple patterns that are present in the query.</li>
</ol>

      </div>
</section>

    <section id="querying-evolving_use-case" inlist="" rel="schema:hasPart" resource="#querying-evolving_use-case">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Use Case</h3>

        <p>A guiding use case, based on public transport, will be referred to in the remainder of this paper.
When public transport route planning applications return dynamic data,
they can account for factors such as train delays
as part of a continuously updating route plan.
In this use case, different clients need to obtain all train departure information for a certain station.
This requires the following concepts:</p>

        <ul>
          <li><strong>Departure</strong> (<em>static</em>): Unique <abbr title='Internationalized Resource Identifier'>IRI</abbr> for the departure of a certain train.</li>
          <li><strong>Headsign</strong> (<em>static</em>): The label of the train showing its destination.</li>
          <li><strong>Departure Time</strong> (<em>static</em>): The <em>scheduled</em> departure time of the train.</li>
          <li><strong>Route Label</strong> (<em>static</em>): The identifier for the train and its route.</li>
          <li><strong>Delay</strong> (<em>dynamic</em>): The delay of the train, which can increase through time.</li>
          <li><strong>Platform</strong> (<em>dynamic</em>): The platform number of the station at which the train will depart, which can be changed through time if delays occur.</li>
        </ul>

        <p><a href="#querying-evolving_listing:ta:originaltriples">Listing 9</a> shows example data in this model.
The <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query in <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a>
can retrieve all information using this basic data model.</p>

        <figure id="querying-evolving_listing:ta:originaltriples" class="listing">
<pre><code>@prefix t: &lt;http://example.org/train/&gt;.
</code><code>@prefix td: &lt;http://example.org/traindata/&gt;.
</code><code>td:departure-48 t:delay         &quot;0S&quot;^^xsd:xs:duration;
</code><code>                t:platform      td:platform-1a;
</code><code>                t:departureTime &quot;2014-12-05T10:37:00+01:00&quot;^^xsd:dateTimeStamp;
</code><code>                t:headSign      &quot;Ghent&quot;;
</code><code>                t:routeLabel    &quot;IC 1831&quot;.
</code></pre>
<figcaption>
            <p><span class="label">Listing 9:</span> Train information with static time information according to the basic data model.</p>
          </figcaption>
</figure>

        <figure id="querying-evolving_listing:usecase:basicquery" class="listing">
<pre><code>SELECT ?delay ?platform ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    _:id t:delay         ?delay.
</code><code>    _:id t:platform      ?platform.
</code><code>    _:id t:departureTime ?departureTime.
</code><code>    _:id t:headSign      ?headSign.
</code><code>    _:id t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 6:</span> The basic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query for retrieving all upcoming train departure information in a certain station.
The two first triple patterns are dynamic, the last three are static.</p>
          </figcaption>
</figure>

      </div>
</section>

    <section id="querying-evolving_dynamic-data-representation" inlist="" rel="schema:hasPart" resource="#querying-evolving_dynamic-data-representation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Dynamic Data Representation</h3>

        <p>Our solution consists of a partial redistribution of query evaluation workload from the server to the client,
which requires the client to be able to access the server data.
There needs to be a distinction between regular static data and continuously updating dynamic data in the server’s
dataset.
For this, we chose to define a certain temporal range in which these dynamic facts are valid, as a consequence
the client will know when the data becomes invalid and has to fetch new data to remain up-to-date.
To capture the temporal scope of data triples, we annotate this data with time.
In this section, we discuss two different types of time labeling, and different methods to annotate this data.</p>

        <h4 id="query-evolving_subsec:temporaldomains">Time Labeling Types</h4>

        <p>We use interval-based labeling to indicate the <em>start and endpoint</em> of the period during which triples are valid.
Point-based labeling is used to indicate the <em>expiration time</em>.</p>

        <p>With expiration times, we only save the latest version of a given fact in a dataset, assuming that 
the old version can be removed when a newer one arrives.
These expiration times provide enough information to determine when a certain fact becomes invalid in time.
We use time intervals for storing multiple versions of the same fact, i.e., for maintaining a history of facts.
These time intervals must indicate a start- and endtime for making it possible to distinguish between different versions
of a certain fact. These intervals cannot overlap in time for the same facts.
When data is volatile, consecutive interval-based facts will accumulate quickly.
Without techniques to aggregate or remove old data, datasets will quickly grow, which can cause increasingly slower query executions.
This problem does not exist with expiration times because in this approach we decided to only save the latest version of a fact, so
this volatility will not have any effect on the dataset size.</p>

        <h4 id="query-evolving_sec:tatypes">Methods for Time Annotation</h4>

        <p>The two time labeling types introduced in the last section can be annotated on triples in different ways.
In <a href="#querying-evolving_related-work_annotations">Subsection 5.2.1</a> we discussed several methods for <abbr title='Resource Description Framework'>RDF</abbr> annotation.
We will apply time labels to triples using the singleton properties, graphs and implicit graphs annotation techniques.</p>

        <p><strong>Singleton Properties</strong>
<em>Singleton properties</em> annotation is done by creating
a singleton property for the predicate of each dynamic triple.
Each of these singleton properties can then be annotated with its time annotation, being either
a time interval or expiration times.</p>

        <h5 id="graphs">Graphs</h5>
        <p>To time-annotate triples using <em>graphs</em>, we can encapsulate triples inside contexts,
and annotate each context graph with a time annotation.</p>

        <h5 id="implicit-graphs">Implicit Graphs</h5>
        <p>A TPF interface gives a unique <abbr title='Internationalized Resource Identifier'>IRI</abbr> to each fragment corresponding to a triple pattern, including patterns without variables, i.e., actual triples.
Since Triple Pattern Fragments are the basis of our solution, we can interpret each fragment as a graph.
We will refer to these as <em>implicit graphs</em>.
This <abbr title='Internationalized Resource Identifier'>IRI</abbr> can then be used as graph identifier for this triple for adding time information.
For example, the <abbr title='Internationalized Resource Identifier'>IRI</abbr> for the triple <code>&lt;s&gt; &lt;p&gt; &lt;o&gt;</code> on the <abbr title='Triple Pattern Fragments'>TPF</abbr> interface located at <code>http:/​/​example.org/dataset/</code> is<br /> <code>http:/​/​example.org/dataset?subject=s&amp;predicate=p&amp;object=o</code>.</p>

        <p>The choice of time annotation method for publishing temporal data will also depend on its capability to
<em>group</em> time labels.
If certain dynamic triples have identical time labels, these annotations can be shared to further reduce the required
amount of triples if we are using singleton properies or graphs.
When we would have three train delay triples which are valid for the same time interval using
graph annotation, these three triples can be placed in the same graph.
This will make sure they refer to the same time interval without having to replicate this annotation two times more.
In the case of implicit graph annotation, this grouping of triples is not possible, because each triple has a unique
graph identifier determined by the interface.
This would be possible if these different identifiers are linked to each other with
for example <code>owl:sameAs</code> relationships that our query engine takes into account, which would introduce further overhead.</p>

        <p>We will execute our use case for each of these annotation methods.
In practise, an annotation method must be chosen depending on the requirements and available technologies.
If we have a datastore that supports quads, graph-based annotation is the best choice because of it requires the least amount of triples.
If our datastore does not support quads, we can use singleton properties.
If we have a TPF-like interface at which our data is hosted, we can use implicit graphs as annotation technique,
if however many of those triples can be grouped under the same time label, singleton properties are a better alternative because
the latter has grouping support.</p>

      </div>
</section>

    <section id="querying-evolving_query-engine" inlist="" rel="schema:hasPart" resource="#querying-evolving_query-engine">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Query Engine</h3>

        <p>TPF query evaluation involves server and client software, because the client actively takes part in the
query evaluation, as opposed to traditional <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> endpoints where the server does all of the work.
Our solution allows users to send a normal <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query to the local query engine
which autonomously detects the dynamic parts of the query and continuously sends back results
from that query to the user.
In this section, we discuss the architecture of our proposed solution and the most important
algorithms that were used to implement this.</p>

        <h4 id="architecture">Architecture</h4>

        <p>Our solution must be able to handle regular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 queries,
detect the dynamic parts, and produce continuously updating results for non-high frequency queries.
To achieve this, we chose to build an extra software layer on top of the existing <abbr title='Triple Pattern Fragments'>TPF</abbr> client that
supports each discussed labeling type and annotation method and is capable of doing
dynamic query transformation and result streaming.
At the <abbr title='Triple Pattern Fragments'>TPF</abbr> server, dynamic data must be annotated with time depending on the
used combination of labeling type and method.
The server expects dynamic data to be pushed to the platform by an external process with varying data.
In the case of graph-based annotation, we have to extend the <abbr title='Triple Pattern Fragments'>TPF</abbr> server implementation,
so that it supports quads.
This dynamic data should be pushed to the platform by an external process with varying data.</p>

        <figure id="querying-evolving_fig:architecture">
<img src="querying-evolving/img/solution-architecture.svg" alt="[TPF Query Streamer architecture]" />
<figcaption>
            <p><span class="label">Fig. 41:</span> Overview of the proposed client-server architecture.</p>
          </figcaption>
</figure>

        <p><a href="#querying-evolving_fig:architecture">Fig. 41</a> shows an overview of the architecture for this extra layer on top of the
<abbr title='Triple Pattern Fragments'>TPF</abbr> client, which will be called the <em>TPF Query Streamer</em> from now on.
The left-hand side shows the <em>User</em> that can send a regular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer
entry-point and receives a stream of query results.
The system can execute queries through the local <em>Basic Graph Iterator</em>, which is part of
the <abbr title='Triple Pattern Fragments'>TPF</abbr> client and executes queries against a TPF server.</p>

        <p>The <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer consists of six major components.
First, there is the <em>Rewriter</em> module which is executed only once at the start of the query streaming loop.
This module is able to transform the original input query into a <em>static</em> and a <em>dynamic query</em>
which will respectively retrieve the static background data and the time-annotated changing data.
This transformation happens by querying metadata of the triple patterns against the entry-point through
the local <abbr title='Triple Pattern Fragments'>TPF</abbr> client.
The <em>Streamer</em> module takes this dynamic query, executes it and forwards its results
to the <em>Time Filter</em>.
The <em>Time Filter</em> checks the time annotation for each of the results and rejects those that are
not valid for the current time.
The minimal expiration time of all these results is then determined and used as a delayed call to the
<em>Streamer</em> module to continue with the <em>streaming loop</em>, which is determined by the repeated
invocation of the <em>Streamer</em> module.
This minimal expiration time will make sure that when at least one of the results expire, a new set
of results will be fetched as part of the next query iteration.
The filtered dynamic results will be passed on to the <em>Materializer</em> which is responsible for
creating <em>materialized static queries</em>.
This is a transformation of the <em>static query</em> with the dynamic results filled in.
These <em>materialized static queries</em> are passed to the <em>Result Manager</em> which is able to cache
these queries.
Finally, the <em>Result Manager</em> retrieves previous <em>materialized static query</em> results from
the local cache or executes this query for the first time and stores its results in the cache.
These results are then sent to the client who had initiated continuous query.</p>

        <h4 id="algorithms">Algorithms</h4>

        <p><strong>Query rewriting</strong>
As mentioned in the previous section, the <em>Rewriter</em> module performs a preprocessing step
that can transform a regular <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 query into a static and dynamic query.
A first step in this transformation is to detect which triple patterns inside the original query
refer to static triples and which refer to dynamic triples.
We detect this by making a separate query for each of the triple patterns and transforming each of them
to a dynamic query.
An example of such a transformation can be found in <a href="#query-evolving_listing:example:dynamic"></a>.
We then evaluate each of these transformed queries and assume a triple pattern is
<em>dynamic</em> if its corresponding query has at least one result.
Another step before the actual query splitting is the conversion of blank nodes to variables.
We will end up with one static query and one dynamic query,
in case these graphs were originally connected, they still need to be connected after the query splitting.
This connection is only possible with variables that are visible,
meaning that these variables need to be part of the <code>SELECT</code> clause.
However, a variable can also be anonymous and not visible: these are blank nodes.
To make sure that we take into account blank nodes that connect the static and dynamic graph,
these nodes have to be converted to variables, while maintaining their semantics.
After this step, we iterate over each
triple pattern of the original query and assign them to either the static or the dynamic query
depending on whether or not the pattern is respectively static or dynamic.
This assignment must maintain the hierarchical structure of the original query,
in some cases this causes triple patterns to be present in the dynamic query when using complex operators
like <code>UNION</code> to maintain correct query semantics.
An example of this query transformation for our basic query from <a href="#query-evolving_listing:usecase:basicquery"></a>
can be found in <a href="#query-evolving_listing:usecase:staticquery"></a> and <a href="#query-evolving_listing:usecase:dynamicquery"></a>.</p>

        <figure id="querying-evolving_listing:example:dynamic" class="listing">
<pre><code>SELECT ?s ?p ?o ?time WHERE {
</code><code>    GRAPH ?g0 { ?s ?p ?o }
</code><code>    ?g0 tmp:expiration ?time
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 7:</span> Dynamic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query for the triple pattern <code>?s ?p ?o</code> for graph-based annotation with expiration times.</p>
          </figcaption>
</figure>

        <figure id="querying-evolving_listing:usecase:dynamicquery" class="listing">
<pre><code>SELECT ?id ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    ?id t:departureTime ?departureTime.
</code><code>    ?id t:headSign      ?headSign.
</code><code>    ?id t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 8:</span> Static <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query which has been derived from the basic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query from <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a> by the <em>Rewriter</em> module.</p>
          </figcaption>
</figure>

        <figure id="querying-evolving_listing:ta:originaltriples" class="listing">
<pre><code>SELECT ?id ?delay ?platform ?final0 ?final1
</code><code>WHERE {
</code><code>    GRAPH ?g0 { ?id t:delay ?delay. }
</code><code>    ?g0 tmp:expiration ?final0.
</code><code>    GRAPH ?g1 { ?id t:platform ?platform. }
</code><code>    ?g1 tmp:expiration ?final1.
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 9:</span> Dynamic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query which has been derived from the basic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query from <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a> by the <em>Rewriter</em> module. Graph-based annotation is used with expiration times.</p>
          </figcaption>
</figure>

        <p><strong>Query materialization</strong>
The <em>Materializer</em> module is responsible for creating <em>materialized static queries</em>
from the static query and the current dynamic query results.
This is done by filling in each dynamic result into the static query variables.
It is possible that multiple results are returned from the dynamic query evaluation, which
is the same amount of materialized static queries that can be derived.
Assuming that we, for example, find the following single dynamic query result from the dynamic query in
<a href="#query-evolving_listing:usecase:dynamicquery"></a>:
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><mtext mathvariant="monospace">?id</mtext><mo>&#x21a6;</mo><mtext mathvariant="monospace">&lt;http://example.org/train#train4815&gt;</mtext><mo separator="true">,</mo><mtext mathvariant="monospace">?delay</mtext><mo>&#x21a6;</mo><mrow><mtext mathvariant="monospace">&quot;P10S&quot;</mtext><mover accent="true"><mrow></mrow><mo>&#x2c6;</mo></mover><mover accent="true"><mrow></mrow><mo>&#x2c6;</mo></mover><mtext mathvariant="monospace">xsd:duration</mtext></mrow><mo>}</mo></mrow><annotation encoding="application/x-tex">\{ \texttt{?id} \mapsto \texttt{&lt;http://example.org/train\#train4815&gt;},
    \texttt{?delay} \mapsto \texttt{&quot;P10S&quot;\^{}\^{}xsd:duration}
\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord text"><span class="mord texttt">?id</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&#x21a6;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9166599999999999em;vertical-align:-0.22222em;"></span><span class="mord text"><span class="mord texttt">&lt;http://example.org/train#train4815&gt;</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord texttt">?delay</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&#x21a6;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord texttt">&quot;P10S&quot;</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">&#x2c6;</span></span></span></span></span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">&#x2c6;</span></span></span></span></span></span><span class="mord texttt">xsd:duration</span></span><span class="mclose">}</span></span></span></span>
then we can derive the materialized static query by filling in these two variables into the static query from
<a href="#query-evolving_listing:usecase:staticquery"></a>, the resulting query can be found in 
<a href="#query-evolving_listing:usecase:materializedstaticquery"></a>.</p>

        <figure id="querying-evolving_listing:usecase:materializedstaticquery" class="listing">
<pre><code>SELECT ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    &lt;http://example.org/train#train4815&gt; t:departureTime ?departureTime.
</code><code>    &lt;http://example.org/train#train4815&gt; t:headSign      ?headSign.
</code><code>    &lt;http://example.org/train#train4815&gt; t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Listing 10:</span> Materialized static <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query derived by filling in the dynamic query results into the static query from <a href="#querying-evolving_listing:usecase:materializedstaticquery">Listing 10</a>.</p>
          </figcaption>
</figure>

        <p><strong>Caching</strong>
The <em>Result manager</em> is the last step in the streaming loop for returning the materialized
static query results of one time instance.
This module is responsible for either getting results for given queries from its cache,
or fetching the results from the <abbr title='Triple Pattern Fragments'>TPF</abbr> client.
First, an identifier will be determined for each materialized static query.
This identifier will serve as a key to cache static data and should correctly
and uniquely identify static results based on dynamic results.
This is equivalent to saying that this identifier should be the <em>connection</em>
between the static and dynamic graphs.
This connection is the intersection of the variables present in the <code>WHERE</code> clause of the
static and dynamic queries.
Since the dynamic query results are already available at this point, these variables
all have values, so this cache identifier can be represented by these variable results.
The graph connection between the static and dynamic queries from <a href="#query-evolving_listing:usecase:staticquery"></a> and <a href="#query-evolving_listing:usecase:dynamicquery"></a> is <code>?id</code>.
The cache identifier for a result where <code>?id</code> is <code>"train:4815"</code> is for example <code>"?id=train:4815"</code>.</p>

      </div>
</section>

    <section id="querying-evolving_evaluation" inlist="" rel="schema:hasPart" resource="#querying-evolving_evaluation">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Evaluation</h3>

        <p>In order to validate our hypotheses from <a href="#querying-evolving_problem-statement">Section 5.3</a>, we set up an experiment to measure the
impact of our proposed redistribution of workload between the client and server by simultaneously executing
a set of queries against a server using our proposed solution.
We repeat this experiment for two
state-of-the-art solutions: <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.</p>

        <p>To test the client and server performance, our experiment consisted of one server and ten physical clients.
Each of these clients can execute from one to twenty unique concurrent queries
based on the use case from <a href="#querying-evolving_use-case">Section 5.4</a>.
The data for this experiment was derived from real-world Belgian railway data
using the <a href="https://hello.irail.be/api/1-0/" class="mandatory" data-link-text="https:/​/​hello.irail.be/​api/​1-​0/​">iRail API</a>.
This results in a series of 10 to 200 concurrent query executions.
This setup was used to test the client and server performance of different <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> streaming approaches.</p>

        <p>For comparing the efficiency of different time annotation methods
and for measuring the effectiveness of our client-side cache,
we measured the execution times of the query for our use case from <a href="#querying-evolving_use-case">Section 5.4</a>.
This measurement was done for different annotation methods, once with the cache and once without the cache.
For discovering the evolution of the query evaluation efficiency through time,
the measurements were done over each query stream iteration of the query.</p>

        <p>The discussed architecture was <a href="https://github.com/LinkedDataFragments/QueryStreamer.js/tree/eswc2016" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​QueryStreamer.js/​tree/​eswc2016">implemented in JavaScript using Node.js</a> to
allow for easy communication with the existing <abbr title='Triple Pattern Fragments'>TPF</abbr> client.</p>

        <p>The <a href="https://github.com/rubensworks/TPFStreamingQueryExecutor-experiments/" class="mandatory" data-link-text="https:/​/​github.com/​rubensworks/​TPFStreamingQueryExecutor-​experiments/​">tests</a> were executed on the
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://ilabt.iminds.be/virtualwall">Virtual Wall (generation 2) environment from imec</a> <span class="references">[<a href="#ref-124">124</a>]</span>.
Each machine had two Hexacore Intel E5645 (2.4GHz) CPUs with 24 GB <abbr title='Random Access Memory'>RAM</abbr> and was running Ubuntu 12.04 LTS.
For <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>, we used <a property="schema:citation http://purl.org/spar/cito/cites" href="https://code.google.com/p/cqels/wiki/CQELS_engine">version 1.0.1 of the engine</a> <span class="references">[<a href="#ref-125">125</a>]</span>. For <abbr title='Continuous SPARQL'>C-SPARQL</abbr>, this was <a property="schema:citation http://purl.org/spar/cito/cites" href="http://streamreasoning.org/download">version 0.9</a> <span class="references">[<a href="#ref-126">126</a>]</span>.
The dataset for this use case consisted of about 300 static triples, and around
200 dynamic triples that were created and removed each ten seconds.
Even this relatively small dataset size already reveals important differences
in server and client cost, as we will discuss in the paragraphs below.</p>

        <h4 id="querying-evolving_subsec:Results-ServerCost">Server Cost</h4>

        <p>The server performance results from our main experiment can be seen in <a href="#querying-evolving_fig:res:scalability-server">Subfig. 42.1</a>.
This plot shows an increasing <abbr title='Central Processing Unit'>CPU</abbr> usage for <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> for higher numbers of concurrent query executions.
On the other hand, our solution never reaches more than one percent of server <abbr title='Central Processing Unit'>CPU</abbr> usage.
<a href="#querying-evolving_fig:res:scalability-server-200">Subfig. 43.1</a> shows a detailed view on the measurements in the case of 200 simultaneous
query executions: the <abbr title='Central Processing Unit'>CPU</abbr> peaks for the alternative approaches are much higher and more frequent than for our solution.</p>

        <h4 id="querying-evolving_subsec:Results-ClientCost">Client Cost</h4>

        <p>The results for the average <abbr title='Central Processing Unit'>CPU</abbr> usage across the duration of the query evaluation
of all clients that sent queries to the server in our main experiment can be seen
in <a href="#querying-evolving_fig:res:scalability-client">Subfig. 42.2</a> and <a href="#querying-evolving_fig:res:scalability-client-all">Subfig. 43.2</a>.
The clients that were sending <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> queries to the server had a client
<abbr title='Central Processing Unit'>CPU</abbr> usage of nearly zero percent for the whole duration of the query evaluation.
The clients using the client-side <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer solution that was presented in this work
had an initial <abbr title='Central Processing Unit'>CPU</abbr> peak reaching about 80%, which dropped to about
5% after 4 seconds.</p>

        <h4 id="querying-evolving_subsec:Results-AnnotationMethods">Annotation Methods</h4>

        <p>The execution times for the different annotation methods, once with and once without cache can be seen in <a href="#querying-evolving_fig:res:overview">Fig. 44</a>.
The three annotation methods have about the same relative performance in all figures, but the execution
times are generally lower in the case where the client-side cache was used, except for the first
query iteration.
The execution times for expiration time annotation when no cache is used are constant,
while the execution times with caching slightly decrease over time.</p>

        <figure id="querying-evolving_fig:res:scalability">

<figure id="querying-evolving_fig:res:scalability-server" class="subfigure">
<center><strong>Server load</strong></center>
<img src="querying-evolving/img/scalability.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 42.1:</span> The server CPU usage of our solution proves to be influenced less by the number of clients.</p>
            </figcaption>
</figure>

<figure id="querying-evolving_fig:res:scalability-client" class="subfigure">
<center><strong>Client load</strong></center>
<img src="querying-evolving/img/scalability-client.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 42.2:</span> In the case of 200 concurrent clients,
client <abbr title='Central Processing Unit'>CPU</abbr> usage initially is high after which it converges to about 5%.
The usage for <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> is almost non-existing.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 42:</span> Average server and client <abbr title='Central Processing Unit'>CPU</abbr> usage for one query stream for <abbr title='Continuous SPARQL'>C-SPARQL</abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> and the proposed solution.
Our solution effectively moves complexity from the server to the client.</p>
          </figcaption>
</figure>

        <figure id="querying-evolving_fig:res:boxplots">

<figure id="querying-evolving_fig:res:scalability-server-200" class="subfigure">
<center><strong>Server load</strong></center>
<img src="querying-evolving/img/scalabilityBoxplot-200.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 43.1:</span> Server <abbr title='Central Processing Unit'>CPU</abbr> peaks for <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> compared to our solution.</p>
            </figcaption>
</figure>

<figure id="querying-evolving_fig:res:scalability-client-all" class="subfigure">
<center><strong>Client load</strong></center>
<img src="querying-evolving/img/clientScalabilityBoxplot.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 43.2:</span> Client <abbr title='Central Processing Unit'>CPU</abbr> usage for our solution is significantly higher.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 43:</span> Detailed view on all server and client <abbr title='Central Processing Unit'>CPU</abbr> measurements for <abbr title='Continuous SPARQL'>C-SPARQL</abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> and the solution presented in this work for 200 simultaneous query evaluations against the server.</p>
          </figcaption>
</figure>

        <figure id="querying-evolving_fig:res:overview">

<figure id="querying-evolving_fig:res:ItCf" class="subfigure">
<img src="querying-evolving/img/interval-true_caching-false.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 44.1:</span> Time intervals without caching.</p>
            </figcaption>
</figure>

<figure id="querying-evolving_fig:res:ItCt" class="subfigure">
<img src="querying-evolving/img/interval-true_caching-true.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 44.2:</span> Time intervals with caching.</p>
            </figcaption>
</figure>

<figure id="querying-evolving_fig:res:IfCf" class="subfigure">
<img src="querying-evolving/img/interval-false_caching-false.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 44.3:</span> Expiration times without caching.</p>
            </figcaption>
</figure>

<figure id="querying-evolving_fig:res:IfCt" class="subfigure">
<img src="querying-evolving/img/interval-false_caching-true.png" />
<figcaption class="for-subfigure">
              <p><span class="label">Subfig. 44.4:</span> Expiration times with caching.</p>
            </figcaption>
</figure>

<figcaption>
            <p><span class="label">Fig. 44:</span> Executions times for the three different types of dynamic data representation for several subsequent streaming requests.
The figures show a mostly linear increase when using time intervals and constant execution times for annotation using expiration times.
In general, caching results in lower execution times.
They also reveal that the graph approach has the lowest execution times.</p>
          </figcaption>
</figure>

      </div>
</section>

    <section id="querying-evolving_conclusions" inlist="" rel="schema:hasPart" resource="#querying-evolving_conclusions">
<div datatype="rdf:HTML" property="schema:description">
        <h3 property="schema:name">Conclusions</h3>

        <p>In this paper, we researched a solution for querying over dynamic data with a low server cost,
by continuously polling the data based on volatility information.
In this section, we draw conclusions from our evaluation results to give an answer
to the research questions and hypotheses we defined in <a href="#querying-evolving_problem-statement">Section 5.3</a>.
First, the server and client costs for our solution will be compared with the alternatives.
After that, the effect of our client-side cache will be explained.
Next, we will discuss the effect of time annotation on the amount of requests to be sent, after which the
performance of our solution will be shown and the effects of the annotation methods.</p>

        <h4 id="server-cost">Server cost</h4>
        <p>The results from <a href="#querying-evolving_subsec:Results-ServerCost">Subsection 5.7.1</a> confirm <a href="#querying-evolving_hypothesis1">Hypothesis 1</a>, in which we wanted to
know if we could lower the server cost when compared to <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.
Not only is the server cost for our solution more than ten times lower on average when compared to the alternatives,
this cost also increases much slower for a growing number of simultaneous clients.
This makes our proposed solution more scalable for the server.
Another disadvantage of <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> is the fact that the server load for a large number of
concurrent clients varies significantly, as can be seen in <a href="#querying-evolving_fig:res:scalability-server-200">Subfig. 43.1</a>.
This makes it hard to scale the required processing powers for servers using these technologies.
Our solution has a low and more constant <abbr title='Central Processing Unit'>CPU</abbr> usage.</p>

        <meta property="lsc:confirms" resource="#querying-evolving_hypothesis1" />

        <h4 id="client-cost">Client cost</h4>
        <p>The results for the client load measurements from <a href="#querying-evolving_subsec:Results-ClientCost">Subsection 5.7.2</a> confirm
<a href="#querying-evolving_hypothesis2">Hypothesis 2</a>, which stated that our solution increases the client’s processing need.
The required client processing power using our solution is clearly much higher than for <abbr title='Continuous SPARQL'>C-SPARQL</abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.
This is because we redistributed the required processing power from the server to the client.
In our solution, it is the client that has to do most of the work for evaluating queries, which puts
less load on the server.
The load on the client still remains around 5% for the largest part of the query evaluation
as shown in <a href="#querying-evolving_fig:res:scalability-client">Subfig. 42.2</a>. Only during the first few seconds, the query engines
<abbr title='Central Processing Unit'>CPU</abbr> usage peaks, which is because of the processor-intensive rewriting step that needs to be done once
at the start of each dynamic query evaluation.</p>

        <meta property="lsc:confirms" resource="#querying-evolving_hypothesis2" />

        <h4 id="caching">Caching</h4>
        <p>We can also confirm <a href="#querying-evolving_hypothesis3">Hypothesis 3</a> about the positive effect of caching
from the results in <a href="#querying-evolving_subsec:Results-AnnotationMethods">Subsection 5.7.3</a>.
Our caching solution has a positive effect on the execution times.
In an optimal scenario for our use case, caching would lead to an execution time reduction of 60% because three of the five triple
patterns in the query for our use case from <a href="#querying-evolving_use-case">Section 5.4</a> are dynamic.
For our results, this caching leads to an average reduction of 56% which is close to the optimal case.
Since we are working with dynamic data, some required background-data is bound to overlap, in these
cases it is advantageous to have a client-side caching solution so that these redundant requests for
static data can be avoided.
The longer our query evaluation runs, the more static data the cache accumulates, so the bigger the
chance that there are cache hits when background data is needed in a certain query iteration.
Future research should indicate what the limits of such a client-side cache for static data are, and
whether or not it is advantageous to reuse this cache for different queries.</p>

        <meta property="lsc:confirms" resource="#querying-evolving_hypothesis3" />

        <h4 id="request-reduction">Request reduction</h4>
        <p>By annotating dynamic data with a time annotation, we successfully reduced the amount of required requests
for polling-based <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> querying to a minimum, which answers <a href="#querying-evolving_researchquestion1">Research Question 1</a>
about the question if clients can use volatility knowledge to perform continuous querying.
Because now, the client can derive the exact moment at which the data can change on the server, and this will be used
to schedule a new query execution on the server.
In future research, it is still possible to reduce the amount of requests our client engine needs to send
through a better caching strategy, which could for example also temporarily cache dynamic data which changes
at different frequencies.
We can also look into differential data transmission by only sending data to the client that has been changed since
the last time the client has requested a specific resource.</p>

        <h4 id="performance">Performance</h4>
        <p>For answering <a href="#querying-evolving_researchquestion2">Research Question 2</a>, the performance of our solution compared to alternatives,
we compared our solution with two state-of-the-art approaches for dynamic <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> querying.
Our solution significantly reduces the required server processing per client, this complexity is mostly moved to the client.
This comparison shows that our technique allows data providers to offer dynamic data which can be used to continuously
evaluate dynamic queries with a low server cost.
Our low-cost publication technique for dynamic data is useful when the number of potential simultaneous clients
is large. When this data is needed for only a small number of clients in a closed off environment
and query evaluation must happen fast, traditional approaches like <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> or <abbr title='Continuous SPARQL'>C-SPARQL</abbr> are advised.
These are only two possible points on the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><em>Linked Data Fragments</em> axis</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>, depending on the
publication requirements, combinations of these approaches can be used.</p>

        <h4 id="annotation-methods">Annotation methods</h4>
        <p>In <a href="#querying-evolving_researchquestion3">Research Question 3</a>, we wanted to know how the different annotation methods influenced the execution times.
From the results in <a href="#querying-evolving_subsec:Results-AnnotationMethods">Subsection 5.7.3</a>, we can conclude that graph-based annotation results in the lowest execution times.
It can also be seen that annotation with time intervals has the problem of continuously increasing execution times, because
of the continuously growing dataset. Time interval annotation can be desired if we for example want to maintain
the history of certain facts, as opposed to just having the last version of facts using expiration times.
In future work, we will investigate alternative techniques to support time interval annotation without the continuously increasing execution times.</p>

        <p>In this work, the frequency at which our queries are updated is purely data-driven using time intervals or expiration times.
In the future it might be interesting, to provide a control to the user to change this frequency, if for example this user
only desires query updates at a lower frequency than the data actually changes.</p>

        <p>In future work, it is important to test this approach with a larger variety of use cases.
The time annotation mechanisms we use are generic enough to
transform all static facts to dynamic data for any number of triples.
The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf">CityBench</a> <span class="references">[<a href="#ref-32">32</a>]</span> benchmark can for example be
used to evaluate these different cases based on city sensor data.
These tests must be scaled (both in terms of clients as in terms of dataset size),
so that the maximum number of concurrent requests can be determined, with respect to the dataset size.</p>

      </div>
</section>

  </section>

  <section id="conclusions" inlist="" rel="schema:hasPart" resource="#conclusions">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Conclusions</h2>

      <p>The <a href="#research-question">research question of this PhD</a> was defined as
<em>“How to store and query evolving knowledge graphs on the Web?”</em>
The answer to this question is neither simple nor complete.
In this final chapter,
I first summarize an answer to this question,
the limitations of my work,
and I discuss future research efforts that are needed to advance this work further.</p>

      <h3 id="contributions">Contributions</h3>

      <p>Based on my research question, I focussed on four main challenges:</p>

      <ol>
        <li><strong>Experimentation requires <em>representative</em> evolving data.</strong></li>
        <li><strong>Indexing evolving data involves a <em>trade-off</em> between <em>storage efficiency</em> and <em>lookup efficiency</em>.</strong></li>
        <li><strong>Web interfaces are highly <em>heterogeneous</em>.</strong></li>
        <li><strong>Publishing <em>evolving</em> data via a <em>queryable interface</em> involves <em>continuous</em> updates to clients.</strong></li>
      </ol>

      <p>I will discuss the findings within these challenges hereafter.</p>

      <h4 id="generating-evolving-data">Generating Evolving Data</h4>

      <p>In <a href="#generating">Chapter 2</a>, the first challenge was tackled as a prerequisite for the next challenges.
The domain of public transport was chosen for this challenge,
as it contains both geospatial and temporal dimensions,
which makes it useful for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems that can handle various dimensions like these.
Even though many real-world public transit network design and scheduling methodologies already exist,
the synthetic generation of such datasets is not trivial.
The goal of this work was to determine wether or not population distributions could be used as input to such a mimicking algorithm.
Hence, this lead to the following <a href="#generating_researchquestion">research question</a>:</p>

      <blockquote>
        <p>Can population distribution data be used to generate realistic synthetic public transport networks and scheduling?</p>
      </blockquote>

      <p>The main hypothesis of this work was: <em>public transport networks and schedules are correlated with the population distribution within the same area</em>.
This hypothesis was tested and validated for two countries with a high level of confidence.
As such, population distributions formed the basis of the mimicking algorithm of this work.</p>

      <p>Inspired by real-world public transit network design and scheduling methodologies,
a multi-step algorithm was determined where regions, stops, edges, routes and trips are generated
based on any population distribution and dependency rules.
To evaluate the realism of generated datasets, an implementation of the algorithm was provided.
For each step in the algorithm, distance functions were determined to measure the realism for each step.
This realism was confirmed for different regions and transport types.</p>

      <p>With the provided mimicking algorithm,
synthetic public transport networks and scheduling can be generated based on population distributions,
which answers our research question.
This tackles our initial challenge to support experimentation on systems that handle evolving knowledge graphs.</p>

      <h4 id="indexing-evolving-data">Indexing Evolving Data</h4>

      <p>Next, in <a href="#storing">Chapter 3</a>, the challenge was to determine an approach
that achieves a trade-off between storage size and lookup efficiency
that is beneficial for publishing evolving knowledge graphs on the Web.
This approach had to enable a Web-friendly storage approach,
so that evolving knowledge graphs can be published on the Web without requiring very costly machines.
Previous work has shown that by restricting queries on servers to <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">triple pattern queries</a></span> <span class="references">[<a href="#ref-34">34</a>]</span>,
and executing more complex queries client-side, server load can be reduced significantly.
As such, our work built upon this idea by focusing on <em>triple pattern queries</em>.
Furthermore, to reduce memory usage during query execution,
we focus on <em>streaming</em> results with optional <em>offsets</em>.
Finally, we focus on three main versioned query atoms to support various kinds of temporal queries over evolving knowledge graphs.
This lead to the following <a href="#storing_researchquestion">research question</a>:</p>

      <blockquote>
        <p>How can we store <abbr title='Resource Description Framework'>RDF</abbr> archives to enable efficient versioned triple pattern queries with offsets?</p>
      </blockquote>

      <p>This research question is answered by introducing
(1) a storage technique for maintaining multiple versions of a knowledge graph
and (2) querying algorithms that can be used to efficiently extract data from these versions.
As was shown via our hypotheses, this storage technique is a hybrid of different existing storage approaches,
which lead to a trade-off between all of them in terms of storage requirements and querying efficiency.
Important to note is that the introduced storage technique is therefore not the most optimal for all situations.
For specific use cases where only very specific query types are required,
dedicated systems will likely be more efficient.
However, when the domain of queries is broad,
a more general-purpose like our approach is more fitting,
as this will lead to sufficiently fast query execution in most cases,
with acceptable storage requirements.</p>

      <p>In conclusion, our storage approach can be used be used as a backend
for publishing evolving knowledge graphs through a low-cost triple pattern interface,
which has been illustrated via <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><em>Versioned</em> Triple Pattern Fragments</a> <span class="references">[<a href="#ref-40">40</a>]</span>
on <a href="http://versioned.linkeddatafragments.org/bear">http:/​/​versioned.linkeddatafragments.org/bear</a>.
Future challenges include the handling of very large numbers of versions and improving ingestion efficiency,
which both could be resolved by dynamically creating intermediary snapshots within the delta chain.</p>

      <h4 id="heterogeneous-web-interfaces">Heterogeneous Web Interfaces</h4>

      <p>In <a href="#querying">Chapter 4</a>, the challenge on handling the heterogeneous nature of Web interfaces during querying was tackled.
This was done through the design and development of a highly modular <em>meta</em> query engine (Comunica)
that simplifies the handling of various kinds of sources,
and lowers the barrier for researching new query interfaces and algorithms.</p>

      <p>In order for Comunica to be usable as a research platform,
its architecture needed to be flexible enough to handle the complete <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 specification,
and support heterogeneous interfaces.
For this, the <em>actor</em>, <em>mediator</em>, and <em>publish-subscribe</em> software patterns were applied
to achieve an architecture where task-specific actors form building blocks,
and buses and mediators are used to handle their inter-communication,
which can be wired together through <em>dependency injection</em>.</p>

      <p>With Comunica, evaluating the performance of different query algorithms and other query-related approaches become more fair.
Query algorithms are typically compared by implementing them in separate systems,
which leads to confounding factors that may impact the performance results,
such as the use of different programming languages or software libraries.
As Comunica consists of small task-specific building blocks,
different algorithms become different instances of such building blocks,
which reduces confounding during experiments.</p>

      <p>Comunica’s architecture is flexible enough to go outside the realm of standard <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr>.
It is for example usable to create an engine for <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-mocha-2018/">querying over evolving knowledge graphs</a> <span class="references">[<a href="#ref-127">127</a>]</span>.
Concretely, support for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasources from <a href="#storing">Chapter 3</a> was implemented,
together with support for versioned queries.
For this, the streaming results capability of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> proved compatible and beneficial
to the streaming query evaluation of Comunica.</p>

      <h4 id="publishing-and-querying-evolving-data">Publishing and Querying Evolving Data</h4>

      <p>Finally, <a href="#querying-evolving">Chapter 5</a> handled the challenge on a query interface for evolving knowledge graphs.
The main goal of this work was to determine whether (part of) the effort for executing continuous queries
over evolving knowledge graphs could be moved from server to client,
in order to allow the server to handle more concurrent clients.
The outcome of this work was a polling-based Web interface for evolving knowledge graphs,
and a client-side algorithm that is able to perform continuous queries using this interface.</p>

      <p>The first <a href="#querying-evolving_researchquestion1">research question</a> of this work was:</p>

      <blockquote>
        <p>Can clients use volatility knowledge to perform more efficient continuous <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> query evaluation by polling for data?</p>
      </blockquote>

      <p>This question was answered by annotating dynamic data server-side with time annotations,
and by introducing a client-side algorithm that can detect these annotations,
and determine a polling frequency for continuous query evaluation based on that.
By doing this, clients only have to re-download data from the server when it was changed.
Furthermore, static data only have to be downloaded once from the server when needed,
and can therefore optimally be cached by the client.
In practise, one could however argue that no data is never truly indefinitely <em>static</em>,
which is why practical implementations will require caches with a high maximum age for static data
when performing continuous querying over long periods of time.</p>

      <p>Our second <a href="#querying-evolving_researchquestion2">research question</a> was formulated as:</p>

      <blockquote>
        <p>How does the client and server load of our solution compare to alternatives?</p>
      </blockquote>

      <p>This question was answered by comparing the server and client load
of our approach with state of the art server-side engines.
Results show a clear movement of load from server to client,
at the cost of increased bandwidth usage and execution time.
The benefit of this becomes especially clear when the number of concurrent clients increase.
The server load of our approach scales significantly better compared to other approaches for an increasing number of clients.
This is caused by the fact that each clients now helps with query execution,
which frees up a significant portion of server load.
Since multiple concurrent clients also lead to server requests for overlapping URLs,
a server cache should theoretically be beneficial as well.
However, follow-up work has shown that <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2018/on_the_semantics_of_tpf-qs_towards_publishing_and_querying_rdf_streams_at_web-scale.pdf">such a cache leads to higher server load</a> <span class="references">[<a href="#ref-128">128</a>]</span>
due to the high cost of cache invalidation over dynamic data.
This shows that caching dynamic data is unlikely to achieve overall performance benefits.
More intelligent caching techniques may lead to better efficiency,
by for example only caching data that will be valid for at least a given time period.</p>

      <p>The final <a href="#querying-evolving_researchquestion3">research question</a> was defined as:</p>

      <blockquote>
        <p>How do different time-annotation methods perform in terms of the resulting execution times?</p>
      </blockquote>

      <p>Results have shown that by exploiting named graphs for annotating expiration times to dynamic data,
total execution times are the lowest compared to other annotation approaches.
This is caused by the fact that the named graphs approach leads to a lower amount of triples to be downloaded from the server.
And since bandwidth usage has a significant impact on query execution times,
the number of triples that need to be download have such an impact.</p>

      <h4 id="overview">Overview</h4>

      <p>By investigating these four challenges,
our main research question can be answered.
Concretely, evolving knowledge graphs with a low volatility (order of minutes or slower)
can be made queryable on the Web through a low-cost polling-based interface,
with a hybrid snapshot/delta/timestamp-based storage system in the back end.
On top of this and other interfaces,
intelligent client-side query engines can perform continuous queries.
This comes at the cost of an increase in bandwidth usage and execution time,
but with a higher guarantee on result completeness as server availability is improved.
All of this can be evaluated thoroughly using synthetic evolving datasets
that can for example be generated with a mimicking algorithm for public transport network.</p>

      <p>This proves that evolving knowledge graphs <em>can</em> be published and queried on the Web.
Furthermore, no high-cost Web infrastructure is needed to publish or query such graphs,
which lowers the barrier for smaller, <em>decentralized</em> evolving knowledge graphs to be published,
without having to be a giant company with a large budget.</p>

      <h3 id="limitations">Limitations</h3>

      <p>There are several limitations to my contributions that require attention,
which will be discussed hereafter.</p>

      <h4 id="generating-evolving-data-1">Generating Evolving Data</h4>

      <p>In <a href="#generating">Chapter 2</a>, I introduced a mimicking algorithm for generating public transport datasets.
One could however question whether such domain-specific datasets are sufficient for testing evolving knowledge graphs systems in general.
As shown in <a href="#generating_methodology">Section 2.5</a>, the introduced data model contains a relatively small number of <abbr title='Resource Description Framework'>RDF</abbr> properties and classes.
While large domain specific knowledge graphs like these are valuable,
domain-overlapping knowledge graphs such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.cis.upenn.edu/~zives/research/dbpedia.pdf">DBpedia</a> <span class="references">[<a href="#ref-38">38</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/42240.pdf">Wikidata</a> <span class="references">[<a href="#ref-129">129</a>]</span>
many more distinct properties and classes, which place additional demands on systems.
For such cases, multi-domain (evolving) knowledge graph generators could be created in future work.</p>

      <p>Furthermore, the mimicking algorithm produces temporal data in a batch-based manner,
instead of a continuous <em>streaming</em> process.
This requires an evolving knowledge graph to be produced with a fixed temporal range,
and does it does not allow knowledge graphs to evolve continuously for an non-predetermined amount of time.
The latter would be valuable for stream processing systems that need to be evaluated for long periods of time,
which would require an adaptation to the algorithm to make it streaming.</p>

      <h4 id="indexing-evolving-data-1">Indexing Evolving Data</h4>

      <p>In <a href="#storing">Chapter 3</a>, a storage mechanism for evolving knowledge graphs was introduced.
The main limitation of this work is that ingestion times continuously increase
when more versions are added.
This is caused by the fact that versions are typically relative to the previous version,
whereas this storage approach handles versions relative to the initial version.
As such, such versions need to be converted at ingestion time,
which takes continuously longer for more versions.
This shows that this approach can currently not be used for knowledge graphs that evolve indefinitely long,
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://jens-lehmann.org/files/2012/program_el_dbpedia_live.pdf">DBpedia Live</a> <span class="references">[<a href="#ref-76">76</a>]</span>.
One possible solution to this problem would be to fully maintain the latest version for faster relative version recalculation.</p>

      <p>The second main limitation is the fact that delta (DM) queries do not efficiently support result offsets.
As such, my approach is not ideal for use cases where random-access in version differences is needed within very large evolving knowledge graphs,
such as for example finding the 10th or 1000th most read book between 2018 and 2019.
My algorithm naively applies an offset by iterating and voiding results until the offset amount is reach,
as opposed to the more intelligent offset algorithms for the other versioned query types where an index is used to apply the offset.
One possible solution would be to add an additional index for optimizing the offsets for delta queries,
which would also lead to increased storage space and ingestion times.</p>

      <h4 id="heterogeneous-web-interfaces-1">Heterogeneous Web Interfaces</h4>

      <p>The main limitation of the Comunica meta query engine from <a href="#querying">Chapter 4</a>
is its non-interruptible architecture.
This means that once the execution of a certain query operation is started,
it can not be stopped until it is completed without killing the engine completely.
This means that meta-algorithms that dynamically switch between algorithms depending on their execution times
can not be implemented within Comunica.
In order to make this possible, a significant change to the architecture of Comunica would
be required where every actor could be interrupted after being started,
where these interruptions would have to be propagated through to chained operations.</p>

      <p>Another limitation of Comunica is its development complexity,
which is a consequence of its modularity.
Practise has shown that there is a steep learning curve for adding new modules to Comunica,
which is due to the dependency injection system that is error-prone.
To alleviate this problem, <a href="https://github.com/comunica?utf8=%E2%9C%93&amp;q=tutorial&amp;type=&amp;language=">tutorials are being created and presented</a>,
and <a href="https://github.com/LinkedSoftwareDependencies/Components.js-Generator">tools are being developed</a> to simplify the usage of the dependency injection framework.
Furthermore, higher-level tools such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://comunica.github.io/Article-ISWC2018-Demo-GraphQlLD/">GraphQL-LD</a> <span class="references">[<a href="#ref-130">130</a>]</span> and <a href="https://github.com/RubenVerborgh/LDflex">LDflex</a> are being developed
to lower the barrier for querying with Comunica.</p>

      <h4 id="publishing-and-querying-evolving-data-1">Publishing and Querying Evolving Data</h4>

      <p>The main limitation of our publishing and querying approach for evolving data from <a href="#querying-evolving">Chapter 5</a>
is the fact that it only works for slowly evolving data.
From the moment that data changes at the order of one second or faster,
then the polling-based query approach becomes too slow,
and results become outdated even before they are produced.
This is mainly caused by the roundtrip times of <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> requests,
and the fact that multiple of them are needed because of the Triple Pattern Fragments querying approach.
For data that evolves much faster, a polling-based approach like this is not a good solution.
Socket-like solutions where client and server maintain an open connection would be able to reach much higher data velocities,
since servers can send updates to subscribed clients immediately,
without having to wait for a client request, which reduces result latency.</p>

      <p>The second limitation to consider is the significantly higher bandwidth usage
compared to other approaches, which has been shown in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2018/on_the_semantics_of_tpf-qs_towards_publishing_and_querying_rdf_streams_at_web-scale.pdf">follow-up work</a> <span class="references">[<a href="#ref-128">128</a>]</span>.
This means that this approach is not ideal for use cases where bandwidth is limited,
such as querying from low-end mobile devices,
or querying in rural areas with a slow internet connection.
This higher bandwidth usage is inherent to the Triple Pattern Fragments approach,
since more data needs to be downloaded from the server,
so that the client can process it locally.</p>

      <h3 id="open-challenges">Open Challenges</h3>

      <p>While I have formulated one possible answer the question
on how to store and query evolving knowledge graphs on the Web,
this is definitely not the <em>only</em> answer.
As such, further research is needed on all aspects.</p>

      <p>Regarding the storage aspect, alternative techniques for storing evolving knowledge graphs
with different trade-offs will be useful for different scenarios.
On the one hand, dedicated storage techniques should be developed for low-end devices,
such as small sensors in the Internet of Things.
On the other hand, storage techniques should be developed for very high-end devices,
such as required for the infrastructure within nuclear reactors.</p>

      <p>Furthermore, current storage solutions mainly focus on the <em>syntactical</em> querying over evolving knowledge graphs,
but they do not really consider the issue of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-versioned-reasoning/"><em>semantic querying</em></a> <span class="references">[<a href="#ref-131">131</a>]</span> yet,
which involves taking into account the <em>meaning</em> of things through ontology-based inferencing.
Semantic querying over evolving knowledge graphs is needed to enable semantic analysis over such knowledge,
such as analyzing concept drift <span class="references">[<a href="#ref-132">132</a>]</span> or tracking diseases in biomedical datasets over time <span class="references">[<a href="#ref-133">133</a>]</span>.
As such, the area of semantic querying over evolving knowledge graphs requires further research.</p>

      <p>During this PhD, I mainly focused on publishing and querying evolving knowledge graphs
with predictable periodicity in the order of one minute.
Knowledge graphs with faster, slower or unpredictable periodicities may require different techniques.
As such, more work is needed to investigate the impact of different kinds of
evolving knowledge graphs on publishing and querying.
For instance, evolving knowledge graphs with slower periodicities may benefit more from being published
through an interface that is well cacheable, compared to more volatile knowledge graphs.</p>

      <p>Next, standardization efforts will be needed to truly bring evolving knowledge graphs to the Web,
in the form of temporal query languages, temporal models and exchange formats.
For the sake of compatibility, these should be extensions or they should be representable
in the existing Linked Data stack, which will mainly impact <abbr title='Resource Description Framework'>RDF</abbr> and <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr>.
The <a href="https://www.w3.org/community/rsp/" class="mandatory" data-link-text="https:/​/​www.w3.org/​community/​rsp/​">W3C <abbr title='Resource Description Framework'>RDF</abbr> Stream Processing community group</a>
is a first effort that aims to explore these issues.</p>

      <p>Due to the many remaining challenges,
it will take more research and engineering effort before we will see
the true adoption of publishing and querying evolving knowledge graphs on the Web.
Nevertheless, it is important to open up these evolving knowledge graphs to the public Web,
so that humanity can benefit from this as whole,
instead of only being usable by organizations internally behind closed doors.</p>

      <p>In a broader sense, more work will be needed to solve open problems with <em>decentralized</em> knowledge graphs.
The <a property="schema:citation http://purl.org/spar/cito/cites" href="http://crosscloud.org/2016/www-mansour-pdf.pdf">Solid ecosystem</a> <span class="references">[<a href="#ref-134">134</a>]</span> is becoming an important driver within this decentralization effort,
as it offers several fundamental standards to build a decentralized Web.
As such, future Web research will benefit significantly by building upon these standards.
Concretely, new techniques and algorithms are needed to
(1) intelligently navigate the the Web by <a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">following relevant links for a given query</a> <span class="references">[<a href="#ref-87">87</a>]</span>,
(2) enable efficient querying over a <em>large number of sources</em>,
(3) allow <em>authentication-aware</em> querying over <em>private</em> data,
and (4) support <em>collaborative</em> querying between agents that handle similar queries.</p>

      <p>Next to these technical issues, organizational and societal changes will also be needed.
For instance, the European General Data Protection Regulation places strict demands on companies that handle personal data.
Decentralization efforts such as Solid are being investigated by organizations
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://drive.verborgh.org/publications/buyle_egose_2019.pdf">governments to reshape the relationship with their citizens</a> <span class="references">[<a href="#ref-135">135</a>]</span>,
by giving people true ownership over their data, and making governments data consumers.</p>

      <p>As I placed a strong emphasis on <em>reusability</em> during this PhD,
all of the tools and experiments that were implemented
are available under an open license.
Furthermore, well-established development methods from the software industry were followed
to achieve implementations with decent code quality and valuable usage and development documentation.
This should therefore lower the barrier for other researchers in the future
to build upon this research and its tools.</p>

      <p>For the next couple of years,
I aim to focus more on the topic of querying decentralized knowledge graphs.
For this, I will collaborate further with my colleagues from IDLab,
researchers from other labs,
and companies with similar goals.
With this, I hope to empower <em>individuals</em> on the Web,
by allowing them to find the information <em>they</em> want,
instead of what is being forced upon them,
which is <a href="https://www.un.org/en/universal-declaration-human-rights/">a fundamental human right</a>.</p>

    </div>
</section>

</main>

<footer><section>
<h2 id="references">References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://www-sop.inria.fr/acacia/cours/essi2006/Scientific%20American_%20Feature%20Article_%20The%20Semantic%20Web_%20May%202001.pdf" typeof="schema:Article">Berners-Lee, T., Hendler, J., Lassila, O., others: The Semantic Web. Scientific American. 284, 28–37 (2001).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: <abbr title='Resource Description Framework'>RDF</abbr> 1.1: Concepts and Abstract Syntax. W3C, <a href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">https:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/" typeof="schema:CreativeWork">Harris, S., Seaborne, A., Prud’hommeaux, E.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 Query Language. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-query-20130321/</a> (2013).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="https://www.w3.org/DesignIssues/LinkedData.html" typeof="schema:CreativeWork">Berners-Lee, T.: Linked Data. <a href="https://www.w3.org/DesignIssues/LinkedData.html">https:/​/​www.w3.org/DesignIssues/LinkedData.html</a> (2006).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-SPARQL-Benchmark-IJSWIS.pdf" typeof="schema:Article">Bizer, C., Schultz, A.: The Berlin <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> benchmark. International Journal on Semantic Web and Information Systems. 5, 1–24 (2009).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="http://swat.cse.lehigh.edu/pubs/guo05a.pdf" typeof="schema:Article">Guo, Y., Pan, Z., Heflin, J.: LUBM: A Benchmark for OWL Knowledge Base Systems. Web Semantics: Science, Services and Agents on the World Wide Web. 3, 158–182 (2005).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="https://arxiv.org/pdf/0806.4627v2.pdf" typeof="schema:Article">Schmidt, M., Hornung, T., Lausen, G., Pinkel, C.: SP2Bench: a <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Performance Benchmark. In: 2009 IEEE 25th International Conference on Data Engineering. pp. 222–233. IEEE (2009).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="http://ceur-ws.org/Vol-1700/paper-02.pdf" typeof="schema:Article">Spasić, M., Jovanovik, M., Prat-Pérez, A.: An <abbr title='Resource Description Framework'>RDF</abbr> Dataset Generator for the Social Network Benchmark with Real-World Coherence. In: Fundulaki, I., Krithara, A., Ngonga Ngomo, A.-C., and Rentoumi, V. (eds.) Proceedings of the Workshop on Benchmarking Linked Data (2016).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="https://researcher.watson.ibm.com/researcher/files/us-sduan/sigmod2011_RDF_benchmark_duan.pdf" typeof="schema:Article">Duan, S., Kementsietsidis, A., Srinivas, K., Udrea, O.: Apples and Oranges: a Comparison of <abbr title='Resource Description Framework'>RDF</abbr> benchmarks and Real <abbr title='Resource Description Framework'>RDF</abbr> Datasets. In: Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. pp. 145–156. ACM, New York, NY, USA (2011).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="http://ceur-ws.org/Vol-1486/paper_28.pdf" typeof="schema:Article">Colpaert, P., Llaves, A., Verborgh, R., Corcho, O., Mannens, E., Van de Walle, R.: Intermodal Public Transit Routing using Linked Connections. In: Proceedings of the 14th International Semantic Web Conference: Posters and Demos. pp. 1–5 (2015).</dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="https://i11www.iti.kit.edu/extra/publications/dpsw-isftr-13.pdf" typeof="schema:Chapter">Dibbelt, J., Pajor, T., Strasser, B., Wagner, D.: Intriguingly Simple and Fast Transit Routing. In: Bonifaci, V., Demetrescu, C., and Marchetti-Spaccamela, A. (eds.) Experimental Algorithms. pp. 43–54. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf" typeof="schema:Article">Kyzirakos, K., Karpathiotakis, M., Koubarakis, M.: Strabon: a Semantic Geospatial DBMS. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 295–311. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="http://www.semantic-web-journal.net/sites/default/files/swj176_3.pdf" typeof="schema:Article">Battle, R., Kolas, D.: Enabling the Geospatial Semantic Web with Parliament and GEOSPARQL. Semantic Web. 3, 355–370 (2012).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="https://dx.doi.org/10.1145/1860702.1860705" typeof="schema:Article">Barbieri, D.F., Braga, D., Ceri, S., Valle, E.D., Grossniklaus, M.: Querying <abbr title='Resource Description Framework'>RDF</abbr> Streams with <abbr title='Continuous SPARQL'>C-SPARQL</abbr>. SIGMOD Rec. 39, 20–26 (2010).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Parreira, J.X., Hauswirth, M.: A Native and Adaptive Approach for Unified Processing of Linked Streams and Linked Data. In: Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2011. pp. 370–388. Springer Berlin Heidelberg, Berlin, Heidelberg (2011).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="http://dx.doi.org/10.1007/978-3-642-41338-4_22" typeof="schema:Article">Garbis, G., Kyzirakos, K., Koubarakis, M.: Geographica: A Benchmark for Geospatial <abbr title='Resource Description Framework'>RDF</abbr> Stores. In: Alani, H., Kagal, L., Fokoue, A., Groth, P., Biemann, C., Parreira, J.X., Aroyo, L., Noy, N., Welty, C., and Janowicz, K. (eds.) Proceedings of the 12th International Semantic Web Conference. pp. 343–359. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="http://iswc2012.semanticweb.org/sites/default/files/76500294.pdf" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Pham, M.-D., Boncz, P., Eiter, T., Fink, M.: Linked Stream Data Processing Engines: Facts and Figures. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 300–312. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="https://doi.org/10.1016/j.tra.2008.03.011" typeof="schema:Article">Guihaire, V., Hao, J.-K.: Transit Network Design and Scheduling: A Global Review. Transportation Research Part A: Policy and Practice. 42, 1251–1273 (2008).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="https://pdfs.semanticscholar.org/cc2e/d20e54dfbc86dddcc77e2a6e37ec16e4be9c.pdf" typeof="schema:Article">Nascimento, M.A., Pfoser, D., Theodoridis, Y.: Synthetic and Real Spatiotemporal Datasets. IEEE Data Eng. Bull. 26, 26–32 (2003).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="https://doi.org/10.1023/A:1015231126594" typeof="schema:Article">Brinkhoff, T.: A Framework for Generating Network-based Moving Objects. GeoInformatica. 6, 153–180 (2002).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="https://doi.org/10.1109/ITNG.2006.51" typeof="schema:Article">Lin, P.J., Samadi, B., Cipolone, A., Jeske, D.R., Cox, S., Rendon, C., Holt, D., Xiao, R.: Development of a Synthetic Data Set Generator for Building and Testing Information Discovery Systems. In: Third International Conference on Information Technology: New Generations (ITNG’06). pp. 707–712. IEEE (2006).</dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="https://www.sti-innsbruck.at/sites/default/files/fileadmin/documents/articles/p27-angles.pdf" typeof="schema:Article">Angles, R., Boncz, P., Larriba-Pey, J., Fundulaki, I., Neumann, T., Erling, O., Neubauer, P., Martinez-Bazan, N., Kotsev, V., Toma, I.: The Linked Data Benchmark Council: a Graph and <abbr title='Resource Description Framework'>RDF</abbr> Industry Benchmarking Effort. ACM SIGMOD Record. 43, 27–31 (2014).</dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="https://doi.org/10.1007/978-3-642-02094-0_7" typeof="schema:Chapter">Delling, D., Sanders, P., Schultes, D., Wagner, D.: Engineering Route Planning Algorithms. In: Lerner, J., Wagner, D., and Zweig, K.A. (eds.) Algorithmics of Large and Complex Networks: Design, Analysis, and Simulation. pp. 117–139. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="http://doi.acm.org/10.1145/1227161.1227166" typeof="schema:Article">Pyrga, E., Schulz, F., Wagner, D., Zaroliagis, C.: Efficient Models for Timetable Information in Public Transportation Systems. Journal of Experimental Algorithmics (JEA). 12, 2.4:1–2.4:39 (2008).</dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="https://dx.doi.org/10.1137/1.9781611974317.2" typeof="schema:Article">Bast, H., Hertel, M., Storandt, S.: Scalable Transfer Patterns. 2016 Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments (ALENEX). 15–29</dd>
  <dt id="ref-26">[26]</dt>
  <dd resource="http://www2016.net/proceedings/companion/p873.pdf" typeof="schema:Article">Colpaert, P., Chua, A., Verborgh, R., Mannens, E., Van de Walle, R., Vande Moere, A.: What public transit <abbr title='Application Programming Interface'>API</abbr> logs tell us about travel flows. In: Proceedings of the 6th USEWOD Workshop on Usage Analysis and the Web of Data. pp. 873–878. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland (2016).</dd>
  <dt id="ref-27">[27]</dt>
  <dd resource="http://citeseerx.ist.psu.edu/showciting?cid=9035" typeof="schema:Book">Gray, J.: Benchmark Handbook: for Database and Transaction Processing Systems. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, <a href="http://citeseerx.ist.psu.edu/showciting?cid=9035">http:/​/​citeseerx.ist.psu.edu/showciting?cid=9035</a> (1992).</dd>
  <dt id="ref-28">[28]</dt>
  <dd resource="http://doi.acm.org/10.1145/3132218.3132242" typeof="schema:Article">Petzka, H., Stadler, C., Katsimpras, G., Haarmann, B., Lehmann, J.: Benchmarking Faceted Browsing Capabilities of Triplestores. Proceedings of the 13th International Conference on Semantic Systems. 128–135 (2017).</dd>
  <dt id="ref-29">[29]</dt>
  <dd resource="https://doi.org/10.1109/MIC.2008.55" typeof="schema:Article">Eno, J., Thompson, C.W.: Generating Synthetic Data to Match Data Mining Patterns. IEEE Internet Computing. 12, (2008).</dd>
  <dt id="ref-30">[30]</dt>
  <dd resource="http://arxiv.org/abs/1609.08764" typeof="schema:Article">Wong, S.C., Gatt, A., Stamatescu, V., McDonnell, M.D.: Understanding Data Augmentation for Classification: When to Warp? In: Digital Image Computing: Techniques and Applications (DICTA), 2016 International Conference on. pp. 1–6. IEEE (2016).</dd>
  <dt id="ref-31">[31]</dt>
  <dd resource="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf" typeof="schema:Article">Della Valle, E., Ceri, S., van Harmelen, F., Fensel, D.: It’s a Streaming World! Reasoning upon Rapidly Changing Information. Intelligent Systems, IEEE. 24, 83–89 (2009).</dd>
  <dt id="ref-32">[32]</dt>
  <dd resource="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf" typeof="schema:Article">Ali, M.I., Gao, F., Mileo, A.: CityBench: a Configurable Benchmark to Evaluate RSP engines using Smart City Datasets. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) International Semantic Web Conference. pp. 374–389. Springer International Publishing, Cham (2015).</dd>
  <dt id="ref-33">[33]</dt>
  <dd resource="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf" typeof="schema:Article">Georgala, K., Spasić, M., Jovanovik, M., Petzka, H., Röder, M., Ngomo, A.-C.N.: MOCHA2017: The Mighty Storage Challenge at ESWC 2017. In: Dragoni, M., Solanki, M., and Blomqvist, E. (eds.) Semantic Web Challenges. pp. 3–15. Springer International Publishing, Cham (2017).</dd>
  <dt id="ref-34">[34]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, 184–206 (2016).</dd>
  <dt id="ref-35">[35]</dt>
  <dd resource="http://ceur-ws.org/Vol-1377/paper6.pdf" typeof="schema:Article">Fernández, J.D., Polleres, A., Umbrich, J.: Towards Efficient Archiving of Dynamic Linked Open Data. In: Debattista, J., d’Aquin, M., and Lange, C. (eds.) Proceedings of te First DIACHRON Workshop on Managing the Evolution and Preservation of the Data Web. pp. 34–49 (2015).</dd>
  <dt id="ref-36">[36]</dt>
  <dd resource="http://events.linkeddata.org/ldow2010/papers/ldow2010_paper12.pdf" typeof="schema:Article">Umbrich, J., Decker, S., Hausenblas, M., Polleres, A., Hogan, A.: Towards Dataset Dynamics: Change Frequency of Linked Open Data Sources. 3rd International Workshop on Linked Data on the Web (LDOW). (2010).</dd>
  <dt id="ref-37">[37]</dt>
  <dd resource="https://export.arxiv.org/pdf/1504.01891" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Viglas, S., Stavrakas, Y., Pateritsas, C., Anagnostopoulos, I.: A Query Language for Multi-version Data Web Archives. Expert Systems. 33, 383–404 (2016).</dd>
  <dt id="ref-38">[38]</dt>
  <dd resource="http://www.cis.upenn.edu/~zives/research/dbpedia.pdf" typeof="schema:Chapter">Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: DBpedia: A Nucleus for a Web of Open Data. In: The semantic web. pp. 722–735. Springer (2007).</dd>
  <dt id="ref-39">[39]</dt>
  <dd resource="https://sites.fas.harvard.edu/~cs265/papers/neumann-2008.pdf" typeof="schema:Article">Neumann, T., Weikum, G.: RDF-3X: a RISC-style engine for <abbr title='Resource Description Framework'>RDF</abbr>. Proceedings of the VLDB Endowment. 1, 647–659 (2008).</dd>
  <dt id="ref-40">[40]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2017/vtpf.pdf" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R., Mannens, E.: Versioned Triple Pattern Fragments: A Low-cost Linked Data Interface Feature for Web Archives. In: Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web (2017).</dd>
  <dt id="ref-41">[41]</dt>
  <dd resource="http://linkeddatafragments.org/publications/jod2017.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Hochstenbach, P., Van de Sompel, H.: Towards Sustainable Publishing and Querying of Distributed Linked Data Archives. Journal of Documentation. 73, (2017).</dd>
  <dt id="ref-42">[42]</dt>
  <dd resource="https://arxiv.org/pdf/0911.1112.pdf" typeof="schema:Article">Van de Sompel, H., Nelson, M.L., Sanderson, R., Balakireva, L.L., Ainsworth, S., Shankar, H.: Memento: Time travel for the Web. arXiv preprint arXiv:0911.1112. (2009).</dd>
  <dt id="ref-43">[43]</dt>
  <dd resource="https://doi.org/10.1007/978-3-642-04329-1_21" typeof="schema:Chapter">Erling, O., Mikhailov, I.: Virtuoso: <abbr title='Resource Description Framework'>RDF</abbr> Support in a Native RDBMS. In: Virgilio, R. de, Giunchiglia, F., and Tanca, L. (eds.) Semantic Web Information Management: A Model-Based Perspective. pp. 501–519. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).</dd>
  <dt id="ref-44">[44]</dt>
  <dd resource="https://cindy.informatik.uni-bremen.de/cosy/staff/dylla/publications/Paper/cosy:sparq-ki-ws06.pdf" typeof="schema:Article">Wallgrün, J.O., Frommberger, L., Wolter, D., Dylla, F., Freksa, C.: Qualitative Spatial Representation and Reasoning in the SparQ-toolbox. In: International Conference on Spatial Cognition. pp. 39–58. Springer (2006).</dd>
  <dt id="ref-45">[45]</dt>
  <dd resource="http://www.www2015.it/documents/proceedings/proceedings/p864.pdf" typeof="schema:Article">Pham, M.-D., Passing, L., Erling, O., Boncz, P.: Deriving an Amergent Relational Schema from <abbr title='Resource Description Framework'>RDF</abbr> Data. In: Proceedings of the 24th International Conference on World Wide Web. pp. 864–874. International World Wide Web Conferences Steering Committee (2015).</dd>
  <dt id="ref-46">[46]</dt>
  <dd resource="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_28" typeof="schema:Article">Pham, M.-D., Boncz, P.: Exploiting Emergent Schemas to make <abbr title='Resource Description Framework'>RDF</abbr> Systems More Efficient. In: International Semantic Web Conference. pp. 463–479. Springer (2016).</dd>
  <dt id="ref-47">[47]</dt>
  <dd resource="http://www.cs.uoi.gr/~nikos/ICDE17.pdf" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Mamoulis, N., Anagnostopoulos, I.: Extended Characteristic Sets: Graph Indexing for <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Optimization. In: Data Engineering (ICDE), 2017 IEEE 33rd International Conference on. pp. 497–508. IEEE (2017).</dd>
  <dt id="ref-48">[48]</dt>
  <dd resource="https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/204.pdf" typeof="schema:Article">Montoya, G., Skaf-Molli, H., Hose, K.: The Odyssey Approach for Optimizing Federated <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Queries. In: International Semantic Web Conference. pp. 471–489. Springer (2017).</dd>
  <dt id="ref-49">[49]</dt>
  <dd resource="https://people.csail.mit.edu/tdanford/6830papers/weiss-hexastore.pdf" typeof="schema:Article">Weiss, C., Karras, P., Bernstein, A.: Hexastore: Sextuple Indexing for Semantic Web Data Management. Proceedings of the VLDB Endowment. 1, 1008–1019 (2008).</dd>
  <dt id="ref-50">[50]</dt>
  <dd resource="http://www.vldb.org/pvldb/vol6/p517-yuan.pdf" typeof="schema:Article">Yuan, P., Liu, P., Wu, B., Jin, H., Zhang, W., Liu, L.: TripleBit: a Fast and Compact System for Large Scale <abbr title='Resource Description Framework'>RDF</abbr> Data. Proceedings of the VLDB Endowment. 6, 517–528 (2013).</dd>
  <dt id="ref-51">[51]</dt>
  <dd resource="https://arxiv.org/pdf/1105.4004.pdf" typeof="schema:Article">Álvarez-Garcı́a Sandra, Brisaboa, N.R., Fernández, J.D., Martı́nez-Prieto Miguel A: Compressed K2-triples for Full-in-memory <abbr title='Resource Description Framework'>RDF</abbr> Engines. arXiv preprint arXiv:1105.4004. (2011).</dd>
  <dt id="ref-52">[52]</dt>
  <dd resource="https://link.springer.com/chapter/10.1007/978-3-319-23826-5_11" typeof="schema:Article">Brisaboa, N.R., Cerdeira-Pena, A., Fariña, A., Navarro, G.: A Compact <abbr title='Resource Description Framework'>RDF</abbr> store using Suffix Arrays. In: International Symposium on String Processing and Information Retrieval. pp. 103–115. Springer (2015).</dd>
  <dt id="ref-53">[53]</dt>
  <dd resource="http://www.websemanticsjournal.org/index.php/ps/article/view/328" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary <abbr title='Resource Description Framework'>RDF</abbr> Representation for Publication and Exchange (HDT). Web Semantics: Science, Services and Agents on the World Wide Web. 19, 22–41 (2013).</dd>
  <dt id="ref-54">[54]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007%2F978-3-642-30284-8_36.pdf" typeof="schema:Article">Martı́nez-Prieto Miguel A, Gallego, M.A., Fernández, J.D.: Exchange and Consumption of Huge <abbr title='Resource Description Framework'>RDF</abbr> Data. In: Extended Semantic Web Conference. pp. 437–452. Springer (2012).</dd>
  <dt id="ref-55">[55]</dt>
  <dd resource="http://lodlaundromat.org/pdf/lodlaundry.pdf" typeof="schema:Article">Beek, W., Rietveld, L., Bazoobandi, H.R., Wielemaker, J., Schlobach, S.: LOD Laundromat: a Uniform Way of Publishing Other People’s Dirty Data. In: International Semantic Web Conference. pp. 213–228. Springer (2014).</dd>
  <dt id="ref-56">[56]</dt>
  <dd resource="https://github.com/datablend/fluxgraph" typeof="schema:CreativeWork">Suvee, D.: Fluxgraph. <a href="https://github.com/datablend/fluxgraph">https:/​/​github.com/datablend/fluxgraph</a> (2012).</dd>
  <dt id="ref-57">[57]</dt>
  <dd resource="https://github.com/dmontag/neo4j-versioning" typeof="schema:CreativeWork">Montag, D.: Neo4j Versioning. <a href="https://github.com/dmontag/neo4j-versioning">https:/​/​github.com/dmontag/neo4j-versioning</a> (2011).</dd>
  <dt id="ref-58">[58]</dt>
  <dd resource="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j" typeof="schema:CreativeWork">Cattuto, C.: Representing Time Dependent Graphs in Neo4j. <a href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">https:/​/​github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j</a> (2013).</dd>
  <dt id="ref-59">[59]</dt>
  <dd resource="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/" typeof="schema:CreativeWork">Staff, N.: Modeling a Multilevel Index in Neoj4. <a href="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/">https:/​/​neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/</a> (2012).</dd>
  <dt id="ref-60">[60]</dt>
  <dd resource="http://semantic-web-journal.org/system/files/swj1814.pdf" typeof="schema:Article">Fernández, J.D., Umbrich, J., Polleres, A., Knuth, M.: Evaluating Query and Storage Strategies for <abbr title='Resource Description Framework'>RDF</abbr> Archives. Semantic Web Journal. (2018).</dd>
  <dt id="ref-61">[61]</dt>
  <dd resource="https://core.ac.uk/display/15108588" typeof="schema:Article">Volkel, M., Winkler, W., Sure, Y., Kruk, S.R., Synak, M.: Semversion: A Versioning System for <abbr title='Resource Description Framework'>RDF</abbr> and Ontologies. In: Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29–June 1, 2005. Proceedings (2005).</dd>
  <dt id="ref-62">[62]</dt>
  <dd resource="https://pdfs.semanticscholar.org/b5e3/220451f41a55d4eb3790414bf46dad2175b4.pdf" typeof="schema:Article">Cassidy, S., Ballantine, J.: Version Control for <abbr title='Resource Description Framework'>RDF</abbr> Triple Stores. ICSOFT (ISDM/EHST/DC). 7, 5–12 (2007).</dd>
  <dt id="ref-63">[63]</dt>
  <dd resource="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-01.pdf" typeof="schema:Article">Vander Sande, M., Colpaert, P., Verborgh, R., Coppens, S., Mannens, E., Van de Walle, R.: R&amp;Wbase: Git for Triples. In: Proceedings of the 6th Workshop on Linked Data on the Web (2013).</dd>
  <dt id="ref-64">[64]</dt>
  <dd resource="https://pdfs.semanticscholar.org/187e/60acfcc687b21c2a8887626b1e28d19f03aa.pdf" typeof="schema:Article">Graube, M., Hensel, S., Urbas, L.: R43ples: Revisions for Triples. In: Proceedings of the 1st Workshop on Linked Data Quality co-located with 10th International Conference on Semantic Systems (SEMANTiCS 2014) (2014).</dd>
  <dt id="ref-65">[65]</dt>
  <dd resource="https://pdfs.semanticscholar.org/ed73/3deb78dfffd53279ec3eb768477646204236.pdf" typeof="schema:Article">Hauptmann, C., Brocco, M., Wörndl, W.: Scalable Semantic Version Control for Linked Data Management. In: Proceedings of the 2nd Workshop on Linked Data Quality co-located with 12th Extended Semantic Web Conference (ESWC 2015), Portorož, Slovenia (2015).</dd>
  <dt id="ref-66">[66]</dt>
  <dd resource="https://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/R22.pdf" typeof="schema:Article">Neumann, T., Weikum, G.: x-RDF-3X: Fast Querying, High Update Rates, and Consistency for <abbr title='Resource Description Framework'>RDF</abbr> Databases. Proceedings of the VLDB Endowment. 3, 256–263 (2010).</dd>
  <dt id="ref-67">[67]</dt>
  <dd resource="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf" typeof="schema:Article">Gao, S., Gu, J., Zaniolo, C.: RDF-TX: A Fast, User-Friendly System for Querying the History of <abbr title='Resource Description Framework'>RDF</abbr> Knowledge Bases. In: Proceedings of the 19th International Conference on Extending DatabaseTechnology. pp. 269–280 (2016).</dd>
  <dt id="ref-68">[68]</dt>
  <dd resource="http://lbd.udc.es/Repository/Publications/Drafts/1521360121882_dcc16_150.pdf" typeof="schema:Article">Cerdeira-Pena, A., Farina, A., Fernández, J.D., Martı́nez-Prieto Miguel A: Self-Indexing <abbr title='Resource Description Framework'>RDF</abbr> Archives. In: Data Compression Conference (DCC), 2016. pp. 526–535. IEEE (2016).</dd>
  <dt id="ref-69">[69]</dt>
  <dd resource="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_02.pdf" typeof="schema:Article">Anderson, J., Bendiken, A.: Transaction-time Queries in Dydra. In: Joint Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (MEPDaW 2016) and the 3rd Workshop on Linked Data Quality (LDQ 2016) co-located with 13th European Semantic Web Conference (ESWC 2016): MEPDaW-LDQ. pp. 11–19 (2016).</dd>
  <dt id="ref-70">[70]</dt>
  <dd resource="https://www.fiz-karlsruhe.de/sites/default/files/FIZ/Dokumente/Forschung/ISE/Publications/Conferences-Workshops/2015Meinhard-SEMANTICS.pdf" typeof="schema:Article">Meinhardt, P., Knuth, M., Sack, H.: TailR: a Platform for Preserving History on the Web of Data. In: Proceedings of the 11th International Conference on Semantic Systems. pp. 57–64. ACM (2015).</dd>
  <dt id="ref-71">[71]</dt>
  <dd resource="http://darcs.net" typeof="schema:CreativeWork">Roundy, D.: Darcs. <a href="http://darcs.net">http:/​/​darcs.net</a> (2008).</dd>
  <dt id="ref-72">[72]</dt>
  <dd resource="https://www.w3.org/TR/trig/" typeof="schema:CreativeWork">Bizer, C., Cyganiak, R.: RDF 1.1 TriG. World Wide Web Consortium, <a href="https://www.w3.org/TR/trig/">https:/​/​www.w3.org/TR/trig/</a> (2014).</dd>
  <dt id="ref-73">[73]</dt>
  <dd resource="https://www.worldscientific.com/doi/abs/10.1142/S0218194012500040" typeof="schema:Article">Im, D.-H., Lee, S.-W., Kim, H.-J.: A Version Management Framework for <abbr title='Resource Description Framework'>RDF</abbr> Triple Stores. International Journal of Software Engineering and Knowledge Engineering. 22, 85–106 (2012).</dd>
  <dt id="ref-74">[74]</dt>
  <dd resource="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_28" typeof="schema:Article">Broekstra, J., Kampman, A., Van Harmelen, F.: Sesame: A Generic Architecture for Storing and Querying <abbr title='Resource Description Framework'>RDF</abbr> and <abbr title='Resource Description Framework'>RDF</abbr> Schema. In: International semantic web conference. pp. 54–68. Springer (2002).</dd>
  <dt id="ref-75">[75]</dt>
  <dd resource="https://www.taylorfrancis.com/books/e/9780429102455/chapters/10.1201/b16859-17" typeof="schema:Article">Thompson, B.B., Personick, M., Cutcher, M.: The Bigdata® <abbr title='Resource Description Framework'>RDF</abbr> Graph Database. Linked Data Management. 193–237 (2014).</dd>
  <dt id="ref-76">[76]</dt>
  <dd resource="http://jens-lehmann.org/files/2012/program_el_dbpedia_live.pdf" typeof="schema:Article">Morsey, M., Lehmann, J., Auer, S., Stadler, C., Hellmann, S.: DBpedia and the Live Extraction of Structured Data from Wikipedia. Program. 46, 157–181 (2012).</dd>
  <dt id="ref-77">[77]</dt>
  <dd resource="https://aic.ai.wu.ac.at/~polleres/publications/neum-etal-2016JDIQ.pdf" typeof="schema:Article">Neumaier, S., Umbrich, J., Polleres, A.: Automated Auality Assessment of Metadata Across Open Data Portals. Journal of Data and Information Quality (JDIQ). 8, 2 (2016).</dd>
  <dt id="ref-78">[78]</dt>
  <dd resource="https://ieeexplore.ieee.org/iel5/4236/22924/01067737.pdf" typeof="schema:Article">McBride, B.: Jena: A Semantic Web Toolkit. IEEE Internet computing. 6, 55–59 (2002).</dd>
  <dt id="ref-79">[79]</dt>
  <dd resource="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_03.pdf" typeof="schema:Article">Meimaris, M., Papastefanatos, G.: The EvoGen Benchmark Suite for Evolving <abbr title='Resource Description Framework'>RDF</abbr> Data. In: Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web. pp. 20–35 (2016).</dd>
  <dt id="ref-80">[80]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Mannens, E.: Exposing <abbr title='Resource Description Framework'>RDF</abbr> Archives using Triple Pattern Fragments. In: Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management: Posters and Demos (2016).</dd>
  <dt id="ref-81">[81]</dt>
  <dd resource="http://users.ics.forth.gr/~fgeo/files/ER14.pdf" typeof="schema:Article">Stefanidis, K., Chrysakis, I., Flouris, G.: On Designing Archiving Policies for Evolving <abbr title='Resource Description Framework'>RDF</abbr> Datasets on the Web. In: International Conference on Conceptual Modeling. pp. 43–56. Springer (2014).</dd>
  <dt id="ref-82">[82]</dt>
  <dd resource="https://dx.doi.org/10.1145/1804669.1804675" typeof="schema:Article">Schmidt, M., Meier, M., Lausen, G.: Foundations of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Optimization. In: Proceedings of the 13th International Conference on Database Theory. pp. 4–33 (2010).</dd>
  <dt id="ref-83">[83]</dt>
  <dd resource="https://dx.doi.org/10.1145/1367497.1367578" typeof="schema:Article">Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., Reynolds, D.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Basic Graph Pattern Optimization Using Selectivity Estimation. In: Proceedings of the 17th International Conference on World Wide Web. pp. 595–604 (2008).</dd>
  <dt id="ref-84">[84]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2" typeof="schema:CreativeWork">Erling, O., Mikhailov, I.: <abbr title='Resource Description Framework'>RDF</abbr> Support in the Virtuoso DBMS. In: Pellegrini, T., Auer, S., Tochtermann, K., and Schaffert, S. (eds.) Networked Knowledge - Networked Media: Integrating Knowledge Management, New Media Technologies and Semantic Systems. pp. 7–24. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-85">[85]</dt>
  <dd resource="https://pdfs.semanticscholar.org/b6a5/7bcff56b791e6a39fea687316a3d169a619b.pdf" typeof="schema:Article">Cheng, J., Ma, Z.M., Yan, L.: f-SPARQL: A Flexible Extension of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr>. In: Bringas, P.G., Hameurlain, A., and Quirchmayr, G. (eds.) Database and Expert Systems Applications. pp. 487–494. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).</dd>
  <dt id="ref-86">[86]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/" typeof="schema:CreativeWork">Feigenbaum, L., Todd Williams, G., Grant Clark, K., Torres, E.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 Protocol. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-protocol-20130321/</a> (2013).</dd>
  <dt id="ref-87">[87]</dt>
  <dd resource="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf" typeof="schema:Article">Hartig, O.: An Overview on Execution Strategies for Linked Data Queries. Datenbank-Spektrum. 13, 89–99 (2013).</dd>
  <dt id="ref-88">[88]</dt>
  <dd resource="http://linkeddatafragments.org/publications/eswc2015.pdf" typeof="schema:Article">Van Herwegen, J., Verborgh, R., Mannens, E., Van de Walle, R.: Query Execution Optimization for Clients of Triple Pattern Fragments. In: Gandon, F., Sabou, M., Sack, H., d’Amato, C., Cudré-Mauroux, P., and Zimmermann, A. (eds.) The Semantic Web. Latest Advances and New Domains. pp. 302–318 (2015).</dd>
  <dt id="ref-89">[89]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-amf.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Van Herwegen, J., Mannens, E., Van de Walle, R.: Opportunistic Linked Data Querying through Approximate Membership Metadata. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 92–110. Springer (2015).</dd>
  <dt id="ref-90">[90]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-substring.pdf" typeof="schema:Article">Van Herwegen, J., De Vocht, L., Verborgh, R., Mannens, E., Van de Walle, R.: Substring Filtering for Low-Cost Linked Data Interfaces. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 128–143. Springer (2015).</dd>
  <dt id="ref-91">[91]</dt>
  <dd resource="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf" typeof="schema:Article">Acosta, M., Vidal, M.-E.: Networks of Linked Data Eddies: An Adaptive Web Query Processing Engine for <abbr title='Resource Description Framework'>RDF</abbr> Data. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 111–127 (2015).</dd>
  <dt id="ref-92">[92]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48" typeof="schema:Article">Hartig, O., Buil-Aranda, C.: Bindings-Restricted Triple Pattern Fragments. In: Debruyne, C., Panetto, H., Meersman, R., Dillon, T., Kühn, eva, O’Sullivan, D., and Ardagna, C.A. (eds.) Proceedings of the 15th International Conference on Ontologies, DataBases, and Applications of Semantics. pp. 762–779 (2016).</dd>
  <dt id="ref-93">[93]</dt>
  <dd resource="https://hal.archives-ouvertes.fr/hal-01251654/document" typeof="schema:Article">Folz, P., Skaf-Molli, H., Molli, P.: CyCLaDEs: a Decentralized Cache for Triple Pattern Fragments. In: International Semantic Web Conference. pp. 455–469. Springer (2016).</dd>
  <dt id="ref-94">[94]</dt>
  <dd resource="https://biblio.ugent.be/publication/8539622/file/8539624.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E.: Continuous Client-side Query Evaluation over Dynamic Linked Data. In: International Semantic Web Conference. pp. 273–289. Springer (2016).</dd>
  <dt id="ref-95">[95]</dt>
  <dd resource="#allegrograph" typeof="schema:Article">Aasman, J.: AllegroGraph: <abbr title='Resource Description Framework'>RDF</abbr> triple database. Cidade: Oakland Franz Incorporated. 17, (2006).</dd>
  <dt id="ref-96">[96]</dt>
  <dd resource="https://rdflib.readthedocs.io/en/stable/" typeof="schema:CreativeWork">RDFLib. <a href="https://rdflib.readthedocs.io/en/stable/">https:/​/​rdflib.readthedocs.io/en/stable/</a></dd>
  <dt id="ref-97">[97]</dt>
  <dd resource="https://github.com/linkeddata/rdflib.js" typeof="schema:CreativeWork">rdflib.js. <a href="https://github.com/linkeddata/rdflib.js">https:/​/​github.com/linkeddata/rdflib.js</a></dd>
  <dt id="ref-98">[98]</dt>
  <dd resource="https://github.com/antoniogarrote/rdfstore-js" typeof="schema:CreativeWork">rdfstore-js. <a href="https://github.com/antoniogarrote/rdfstore-js">https:/​/​github.com/antoniogarrote/rdfstore-js</a></dd>
  <dt id="ref-99">[99]</dt>
  <dd resource="http://arxiv.org/abs/1609.07108" typeof="schema:Article">Verborgh, R., Dumontier, M.: A Web <abbr title='Application Programming Interface'>API</abbr> ecosystem through feature-based reuse. CoRR. abs/1609.07108, (2016).</dd>
  <dt id="ref-100">[100]</dt>
  <dd resource="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-03.pdf" typeof="schema:Article">Lanthaler, M., Gütl, C.: Hydra: A Vocabulary for Hypermedia-Driven Web APIs. In: Proceedings of the 6th Workshop on Linked Data on the Web (2013).</dd>
  <dt id="ref-101">[101]</dt>
  <dd resource="https://linkeddatafragments.github.io/Article-Declarative-Hypermedia-Responses/" typeof="schema:Article">Taelman, R., Verborgh, R.: Declaratively Describing Responses of Hypermedia-Driven Web APIs. In: Proceedings of the 9th International Conference on Knowledge Capture (2017).</dd>
  <dt id="ref-102">[102]</dt>
  <dd resource="https://www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf" typeof="schema:Book">Birman, K., Joseph, T.: Exploiting Virtual Synchrony in Distributed Systems. ACM, <a href="https://www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf">https:/​/​www.cs.cornell.edu/home/rvr/sys/p123-birman.pdf</a> (1987).</dd>
  <dt id="ref-103">[103]</dt>
  <dd resource="http://worrydream.com/refs/Hewitt-ActorModel.pdf" typeof="schema:Article">Hewitt, C., Bishop, P., Steiger, R.: Session 8 Formalisms for Artificial Intelligence a Universal Modular Actor Formalism for Artificial Intelligence. In: Advance Papers of the Conference. p. 235. Stanford Research Institute (1973).</dd>
  <dt id="ref-104">[104]</dt>
  <dd resource="https://www.oreilly.com/library/view/design-patterns-elements/0201633612/" typeof="schema:Book">Gamma, E.: Design patterns: Elements of Reusable Object-Oriented Software. Pearson Education India, <a href="https://www.oreilly.com/library/view/design-patterns-elements/0201633612/">https:/​/​www.oreilly.com/library/view/design-patterns-elements/0201633612/</a> (1995).</dd>
  <dt id="ref-105">[105]</dt>
  <dd resource="https://martinfowler.com/articles/injection.html" typeof="schema:CreativeWork">Fowler, M.: Inversion of Control Containers and the Dependency Injection pattern. <a href="https://martinfowler.com/articles/injection.html">https:/​/​martinfowler.com/articles/injection.html</a> (2004).</dd>
  <dt id="ref-106">[106]</dt>
  <dd resource="http://componentsjs.readthedocs.io/en/latest/" typeof="schema:CreativeWork">Taelman, R.: Components.js. <a href="http://componentsjs.readthedocs.io/en/latest/">http:/​/​componentsjs.readthedocs.io/en/latest/</a></dd>
  <dt id="ref-107">[107]</dt>
  <dd resource="https://linkedsoftwaredependencies.org/articles/describing-experiments/" typeof="schema:Article">Van Herwegen, J., Taelman, R., Capadisli, S., Verborgh, R.: Describing Configurations of Software Experiments as Linked Data. In: Proceedings of the 1st Workshop on Enabling Open Semantic Science (2017).</dd>
  <dt id="ref-108">[108]</dt>
  <dd resource="https://www.w3.org/TR/json-ld/" typeof="schema:Article">Consortium, W.W.W., others: JSON-LD 1.0: a JSON-based Serialization for Linked Data. (2014).</dd>
  <dt id="ref-109">[109]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/" typeof="schema:CreativeWork">Grant Clark, K., Feigenbaum, L., Torres, E.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> 1.1 Query Results JSON Format. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-results-json-20130321/</a> (2013).</dd>
  <dt id="ref-110">[110]</dt>
  <dd resource="https://www.w3.org/TR/rdf-sparql-XMLres/" typeof="schema:CreativeWork">Hawke, S.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Results XML Format (Second Edition). W3C, <a href="https://www.w3.org/TR/rdf-sparql-XMLres/">https:/​/​www.w3.org/TR/rdf-sparql-XMLres/</a> (2013).</dd>
  <dt id="ref-111">[111]</dt>
  <dd resource="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" typeof="schema:CreativeWork">Prud’hommeaux, E., Seaborne, A.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Query Language for <abbr title='Resource Description Framework'>RDF</abbr>. W3C, <a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/">https:/​/​www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/</a> (2008).</dd>
  <dt id="ref-112">[112]</dt>
  <dd resource="https://rdfostrich.github.io/article-demo/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: OSTRICH: Versioned Random-Access Triple Store. In: Proceedings of the 27th International Conference Companion on World Wide Web (2018).</dd>
  <dt id="ref-113">[113]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13" typeof="schema:Article">Aluç, G., Hartig, O., Özsu, M.T., Daudjee, K.: Diversified Stress Testing of <abbr title='Resource Description Framework'>RDF</abbr> Data Management Systems. In: Proceedings of the 13th International Semantic Web Conference - Part I. pp. 197–212. Springer-Verlag New York, Inc. (2014).</dd>
  <dt id="ref-114">[114]</dt>
  <dd resource="http://facebook.github.io/graphql/October2016/" typeof="schema:CreativeWork">Facebook, I.: GraphQL. Working Draft, Oct. 2016. <a href="http://facebook.github.io/graphql/October2016/">http:/​/​facebook.github.io/graphql/October2016/</a></dd>
  <dt id="ref-115">[115]</dt>
  <dd resource="http://link.springer.com/chapter/10.1007/978-3-642-41338-4_18" typeof="schema:Chapter">Buil-Aranda, C., Hogan, A., Umbrich, J., Vandenbussche, P.-Y.: <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> Web-Querying Infrastructure: Ready for Action? In: The Semantic Web–ISWC 2013. pp. 277–293. Springer (2013).</dd>
  <dt id="ref-116">[116]</dt>
  <dd resource="https://users.dcc.uchile.cl/~cgutierr/papers/sparql.pdf" typeof="schema:Article">Pérez, J., Arenas, M., Gutierrez, C.: Semantics and Complexity of <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr>. In: International semantic web conference. pp. 30–43. Springer (2006).</dd>
  <dt id="ref-117">[117]</dt>
  <dd resource="http://ceur-ws.org/Vol-1585/mepdaw2016_paper_01.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E., Van de Walle, R.: Continuously Updating Query Results over Real-Time Linked Data. In: Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (2016).</dd>
  <dt id="ref-118">[118]</dt>
  <dd resource="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/" typeof="schema:CreativeWork">Klyne, G., J. Carroll, J.: Resource Description Framework (RDF): Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/">http:/​/​www.w3.org/TR/2004/REC-rdf-concepts-20040210/</a> (2004).</dd>
  <dt id="ref-119">[119]</dt>
  <dd resource="https://dx.doi.org/10.1145/2566486.2567973" typeof="schema:Article">Nguyen, V., Bodenreider, O., Sheth, A.: Don’t Like <abbr title='Resource Description Framework'>RDF</abbr> Reification? Making Statements About Statements Using Singleton Property. In: Proceedings of the 23rd International Conference on World Wide Web. pp. 759–770. ACM, New York, NY, USA (2014).</dd>
  <dt id="ref-120">[120]</dt>
  <dd resource="http://link.springer.com/chapter/10.1007/11431053_7" typeof="schema:Chapter">Gutierrez, C., Hurtado, C., Vaisman, A.: Temporal <abbr title='Resource Description Framework'>RDF</abbr>. In: The Semantic Web: Research and Applications. pp. 93–107. Springer (2005).</dd>
  <dt id="ref-121">[121]</dt>
  <dd resource="https://dx.doi.org/10.1109/TKDE.2007.34" typeof="schema:Article">Gutierrez, C., Hurtado, C.A., Vaisman, A.: Introducing Time into <abbr title='Resource Description Framework'>RDF</abbr>. Knowledge and Data Engineering, IEEE Transactions on. 19, 207–218 (2007).</dd>
  <dt id="ref-122">[122]</dt>
  <dd resource="http://wasp.cs.vu.nl/larkc/nefors10/paper/nefors10_paper_0.pdf" typeof="schema:Article">Barbieri, D., Braga, D., Ceri, S., Della Valle, E., Grossniklaus, M.: Stream Reasoning: Where We Got So Far. In: Proceedings of the NeFoRS2010 Workshop (2010).</dd>
  <dt id="ref-123">[123]</dt>
  <dd resource="http://ilpubs.stanford.edu:8090/641/1/2004-20.pdf" typeof="schema:Article">Arasu, A., Babcock, B., Babu, S., Cieslewicz, J., Datar, M., Ito, K., Motwani, R., Srivastava, U., Widom, J.: STREAM: The Stanford Data Stream Management System. Book chapter. (2004).</dd>
  <dt id="ref-124">[124]</dt>
  <dd resource="http://ilabt.iminds.be/virtualwall" typeof="schema:CreativeWork">iLab.t, iMinds: Virtual Wall: wired networks and applications. <a href="http://ilabt.iminds.be/virtualwall">http:/​/​ilabt.iminds.be/virtualwall</a></dd>
  <dt id="ref-125">[125]</dt>
  <dd resource="https://code.google.com/p/cqels/wiki/CQELS_engine" typeof="schema:CreativeWork">Levan, C.: <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> engine: Instructions on Experimenting <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>. <a href="https://code.google.com/p/cqels/wiki/CQELS_engine">https:/​/​code.google.com/p/cqels/wiki/CQELS_engine</a></dd>
  <dt id="ref-126">[126]</dt>
  <dd resource="http://streamreasoning.org/download" typeof="schema:CreativeWork">StreamReasoning: Continuous <abbr title='SPARQL Protocol and RDF Query Language'>SPARQL</abbr> (C-SPARQL) Ready To Go Pack. <a href="http://streamreasoning.org/download">http:/​/​streamreasoning.org/download</a></dd>
  <dt id="ref-127">[127]</dt>
  <dd resource="https://rdfostrich.github.io/article-mocha-2018/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: Versioned Querying with <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and Comunica in MOCHA 2018. In: Proceedings of the 5th SemWebEval Challenge at ESWC 2018 (2018).</dd>
  <dt id="ref-128">[128]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2018/on_the_semantics_of_tpf-qs_towards_publishing_and_querying_rdf_streams_at_web-scale.pdf" typeof="schema:Article">Taelman, R., Tommasini, R., Van Herwegen, J., Vander Sande, M., Della Valle, E., Verborgh, R.: On the Semantics of TPF-QS towards Publishing and Querying <abbr title='Resource Description Framework'>RDF</abbr> Streams at Web-scale. In: Proceedings of the 14th International Conference on Semantic Systems (2018).</dd>
  <dt id="ref-129">[129]</dt>
  <dd resource="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/42240.pdf" typeof="schema:Article">Vrandečić, D., Krötzsch, M.: Wikidata: a free collaborative knowledge base. (2014).</dd>
  <dt id="ref-130">[130]</dt>
  <dd resource="https://comunica.github.io/Article-ISWC2018-Demo-GraphQlLD/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: GraphQL-LD: Linked Data Querying with GraphQL. In: Proceedings of the 17th International Semantic Web Conference: Posters and Demos (2018).</dd>
  <dt id="ref-131">[131]</dt>
  <dd resource="https://rdfostrich.github.io/article-versioned-reasoning/" typeof="schema:Article">Taelman, R., Takeda, H., Vander Sande, M., Verborgh, R.: The Fundamentals of Semantic Versioned Querying. In: Proceedings of the 12th International Workshop on Scalable Semantic Web Knowledge Base Systems co-located with 17th International Semantic Web Conference (2018).</dd>
  <dt id="ref-132">[132]</dt>
  <dd resource="#conceptdrift" typeof="schema:Article">Wang, S., Schlobach, S., Klein, M.: Concept drift and how to identify it. Web Semantics: Science, Services and Agents on the World Wide Web. 9, 247–265 (2011).</dd>
  <dt id="ref-133">[133]</dt>
  <dd resource="#biomedical" typeof="schema:Article">Afgan, E., Baker, D., Van den Beek, M., Blankenberg, D., Bouvier, D., Čech, M., Chilton, J., Clements, D., Coraor, N., Eberhard, C., others: The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update. Nucleic acids research. 44, W3–W10 (2016).</dd>
  <dt id="ref-134">[134]</dt>
  <dd resource="http://crosscloud.org/2016/www-mansour-pdf.pdf" typeof="schema:Article">Mansour, E., Sambra, A.V., Hawke, S., Zereba, M., Capadisli, S., Ghanem, A., Aboulnaga, A., Berners-Lee, T.: A demonstration of the solid platform for social web applications. In: Proceedings of the 25th International Conference Companion on World Wide Web. pp. 223–226. International World Wide Web Conferences Steering Committee (2016).</dd>
  <dt id="ref-135">[135]</dt>
  <dd resource="https://drive.verborgh.org/publications/buyle_egose_2019.pdf" typeof="schema:Article">Buyle, R., Taelman, R., Mostaert, K., Joris, G., Mannens, E., Verborgh, R., Berners-Lee, T.: Streamlining governmental processes by putting citizens in control of their personal data. Proceedings of the 6th International Conference on Electronic Governance and Open Society: Challenges in Eurasia. (2019).</dd>
</dl>
</section>
</footer>

<div class="empty-page">&nbsp;</div>
<div class="empty-page">&nbsp;</div>

<script type="text/javascript">
    window.addEventListener('load', updateIndex);
    window.addEventListener('scroll', updateIndex);
    
    function updateIndex(){
        // Unselect all other entries
        const entries = document.querySelectorAll('a.index-entry-name');
        for (var i = 0; i < entries.length; i++) {
            entries[i].classList.remove('index-entry-active');
        }
    
        // Select the hovered entry
        var pos = getTocPos();
        if (pos) {            
            var toc = document.querySelector('.index-entries-root');
            var match = toc.querySelector('a[href="#' + pos + '"]');
            if (match) {
                match.classList.add('index-entry-active');
            }
        }
    }
    
    // Get the first section that is visible
    function getTocPos() {
        var sections = document.getElementsByTagName('section');
        for (var i = 0; i < sections.length; i++) {
            var section = sections[i];
            if (section.id && section.getElementsByTagName('section').length === 0 && section.getBoundingClientRect().bottom >= 0) {
                return section.id;
            }
        }
    }
</script>



</body>
</html>
