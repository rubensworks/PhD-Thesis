<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title>Storing and Querying Evolving Knowledge Graphs&lt;br /&gt;on the Web</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" media="all"    href="styles/katex.css" />
  <meta name="citation_title" content="Storing and Querying<br />Evolving Knowledge Graphs<br />on the Web">
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Pieter Colpaert" />
  <meta name="citation_author" content="Erik Mannens" />
  <meta name="citation_author" content="Ruben Verborgh" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Miel Vander Sande" />
  <meta name="citation_author" content="Joachim Van Herwegen" />
  <meta name="citation_author" content="Erik Mannens" />
  <meta name="citation_author" content="Ruben Verborgh" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Joachim Van Herwegen" />
  <meta name="citation_author" content="Miel Vander Sande" />
  <meta name="citation_author" content="Ruben Verborgh" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Ruben Verborgh" />
  <meta name="citation_author" content="Pieter Colpaert" />
  <meta name="citation_author" content="Erik Mannens" />
  <meta name="citation_author" content="Rik Van de Walle" />
  
  <meta name="citation_publication_date" content="2019/07/16" />
</head>

<body prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# as: https://www.w3.org/ns/activitystreams# oa: http://www.w3.org/ns/oa# ldp: http://www.w3.org/ns/ldp#" typeof="schema:CreativeWork sioc:Post prov:Entity">
  <div class="header-wrapper">
<header>
    <h1 id="storing-and-queryingbr-evolving-knowledge-graphsbr-on-the-web">Storing and Querying<br />Evolving Knowledge Graphs<br />on the Web</h1>

    <h2 id="de-opslag-en-bevragingbr-van-evoluerende-kennisgrafenbr-op-het-web">De opslag en bevraging<br />van evoluerende kennisgrafen<br />op het Web</h2>

    <ul id="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    </ul>

    <ul id="affiliations">
      <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
    </ul>

  </header>
</div>

<section id="frontmatter">
  <section id="abstract">
    <h2>Abstract</h2>

    <!-- Context      -->
    <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
<!-- Need         -->
Vestibulum finibus dignissim augue, id pellentesque est facilisis non.
<!-- Task         -->
Donec fringilla dolor non neque iaculis blandit.
<!-- Object       -->
Praesent aliquet eleifend iaculis.
<!-- Findings     -->
Quisque pellentesque at odio ac bibendum.
<!-- Conclusion   -->
Pellentesque imperdiet felis urna, quis facilisis lacus gravida non.
<!-- Perspectives -->
Donec quis lectus eget sem tempor tristique pellentesque in dolor.</p>

  </section>

  <section id="preface">
    <h2>Preface</h2>

    <p class="todo">Write me</p>

  </section>

  <section id="toc">
    <h2>Table of Contents</h2>

    <div id="scholarmarkdown-toc-index"></div>

  </section>

  <section id="acronyms">
    <h2>Acronyms</h2>

    <div id="scholarmarkdown-acronyms-list"></div>

  </section>

  <section id="summary">
    <h2>Summary</h2>

    <p class="todo">Write me</p>

  </section>

</section>

<main>
  <section id="introduction">
    <h2>Introduction</h2>

    <h3 id="introduction-the-web">The Web</h3>

    <h4 id="catalysts-for-human-progress">Catalysts for Human Progress</h4>

    <p>Since the dawn of Mankind, biological evolution has shaped us into social animals.
The social capabilities of humans are however <em>much more evolved</em> than most other other species.
For example, <a href="https://pursuit.unimelb.edu.au/articles/why-we-show-the-whites-of-our-eyes">humans are one of the only animals that have clearly visible eye whites</a>.
This allows people to see what other people are looking at,
which simplifies <em>collaborative</em> tasks.
Furthermore, <a href="https://www.sciencedirect.com/topics/neuroscience/theory-of-mind"><em>theory of mind</em></a> —the ability to understand that others have different perspectives—, <a href="https://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262016056.001.0001/upso-9780262016056">is much more pronounced in humans than other animals</a>, which also strengthens our ability to <em>collaborate</em>.
While our collaborative capabilities were initially limited to physical tasks,
the invention of <em>language</em> and <em>writing</em> allowed us to share <em>knowledge</em> with each other.</p>

    <p>Methods for sharing knowledge are essential catalysts for human progress,
as shared knowledge allows larger groups of people to share goals
and accomplish tasks that would have otherwise been impossible.
Due to our <em>technological</em> progress,
the <em>bandwidth</em> of these methods for sharing knowledge is always increasing,
which is continuously increasing the rate of human and technological progress.</p>

    <p>Throughout the last centuries, we saw three major revolutions in bandwidth.
First, the invention of the printing press in the 15th century
drastically increased rate at which books could be duplicated.
Secondly, there was the invention of radio and television in the 20th century,
As audio and video are cognitively less demanding than reading,
this lowered the barrier for spreading knowledge even further.
Third, we had the development of the internet near the end of the 20th century,
and the invention of the World Wide Web in 1989 on top of that,
which gave us a globally interlinked information space
that is fully <em>open</em> and <em>decentralized</em>, where anyone can say anything.
With the Web, bandwidth for knowledge sharing has become nearly unlimited,
as knowledge no longer has to go through a few large radio or tv stations,
but can now be shared over a virtually unlimited amount of Web pages,
which leads to a more <em>social</em> human species.</p>

    <h4 id="impact-of-the-web">Impact of the Web</h4>

    <p>At the time of writing, the Web is 30 years old.
Considering <a href="http://humanorigins.si.edu/evidence/human-fossils/species/homo-sapiens">our species is believed to be 300,000 years old</a>,
this is just 0.01% of the time we have been around.
To put this in perspective in terms of a human life,
the Web would only be a baby of just under 3 days old,
assuming <a href="https://countrymeters.info/en/Belgium">a life expectancy of 80 years</a>.
This means that the Web <em>just</em> got started,
and it will take a long time for it to mature and achieve its full potential.</p>

    <p>Even in this short amount of time,
the Web has already transformed our world in an unprecedented way.
Most importantly, it has given more than <a href="https://internetworldstats.com/stats.htm">56% of the global population</a>
access to most of all human knowledge behind a finger’s touch.
Secondly, <em>social media</em> has enabled people to communicate with anyone on the planet near-instantly,
and even with multiple people at the same time.
Furthermore, it has <a href="https://ieeexplore.ieee.org/document/8267982">impacted politics</a>
and even <a href="https://www.mic.com/articles/10642/twitter-revolution-how-the-arab-spring-was-helped-by-social-media">caused oppressive regimes to be overthrown</a>.
Next to that, it is also significantly <a href="https://stratechery.com/2015/airbnb-and-the-internet-revolution/">disrupting businesses models that have been around since the industrial revolution, and creating new ones</a>.</p>

    <h4 id="knowledge-graphs">Knowledge Graphs</h4>

    <p>Even though the Web has only existed for a brief window of time,
it already has made a significant impact on world for the better.
Yet, the goal of curiosity-driven researchers is to uncover
what the next steps are to improve the world <em>even more</em>.</p>

    <p>In 2001, Tim Berners-Lee shared his dream <span class="references">[<a href="#ref-1">1</a>]</span> where machines
would be able to help out with our day-to-day tasks
by analyzing data on the Web and acting as <em>intelligent agents</em>.
Back then, the primary goal of the Web was to be <em>human-readable</em>.
In order for this dream to become a reality,
the Web had to become <em>machine-readable</em>.
This Web extension is typically referred to as the <em>Semantic Web</em>.</p>

    <p>Now —almost twenty years later—, several standards and technologies have been developed to make this dream a reality,
<a href="http://iswc2013.semanticweb.org/content/keynote-ramanathan-v-guha.html">In 2013, more than four million Web domains are using these technologies</a>.
Using these Semantic Web technologies, so-called <em>Knowledge Graphs</em> are being constructed by many major companies world-wide,
such as <a href="https://developers.google.com/knowledge-graph/">Google</a> and <a href="https://developer.microsoft.com/en-us/graph/">Microsoft</a>.
These Knowledge Graphs are being used to support tasks that were part of Tim Berners-Lee’s original vision,
such as managing day-to-day tasks with the <a href="https://www.google.com/intl/nl/landing/now/">Google Now assistant</a>.</p>

    <p>The standard for modeling Knowledge Graphs is the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">Resource Description Framework (<abbr title='Resource Description Framework'>RDF</abbr>)</a> <span class="references">[<a href="#ref-2">2</a>]</span>.
Fundamentally, it is based around the concept of <em>triples</em> that are used to make statements about things.
A triple is made up of a <em>subject</em>, <em>predicate</em> and <em>object</em>,
where the <em>subject</em> and <em>object</em> are resources, and the <em>predicate</em> denotes their relationship.
For example, <a href="#introduction-figure-triple">Fig. 1</a> shows an example of a simple triple indicating the nationality of a person.
Multiple resources can combined with each other through multiple triples, which forms a <em>graph</em>.
<a href="#introduction-figure-graph">Fig. 2</a> shows an example of such a graph, which contains <em>knowledge</em> about a person.
In order to look up information within such graphs, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>
was introduced as a standard.
Essentially, <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> allows <abbr title='Resource Description Framework'>RDF</abbr> data to be looked up through combinations of <em>triple patterns</em>,
which are triples where any of its elements can be replace with <em>variables</em> such as <code>?name</code>.
For example, <a href="#introduction-code-sparql">Listing 1</a> contains a <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query that find the names of all people that Alice knows.</p>

    <figure id="introduction-figure-triple">
<img src="img/triple.svg" alt="A triple" />
<figcaption>
        <p><span class="label">Fig. 1:</span> A triple indicating that Alice knows Bob.</p>
      </figcaption>
</figure>

    <figure id="introduction-figure-graph">
<img src="img/graph.svg" alt="A knowledge graph" />
<figcaption>
        <p><span class="label">Fig. 2:</span> A small knowledge graph about Alice.</p>
      </figcaption>
</figure>

    <figure id="introduction-code-sparql" class="listing">
<pre><code>SELECT ?name WHERE {
</code><code>  ?alice knows ?person.
</code><code>  ?person name ?name.
</code><code>}</code></pre>
<figcaption>
        <p><span class="label">Listing 1:</span> A simplified <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query selecting the names of all people that Alice knows.</p>
      </figcaption>
</figure>

    <h4 id="evolving-knowledge-graphs">Evolving Knowledge Graphs</h4>

    <p>As one could expect, the Web is continuously evolving,
and it does to at an increasing rate.
For example, <a href="https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#6907ae2460ba">Google is processing more than <em>40.000 search requests</em> <em>every second</em></a>,
<a href="https://expandedramblings.com/index.php/youtube-statistics/"><em>500 hours of video</em> are being uploaded to YouTube <em>every minute</em></a>,
and <a href="https://blog.hootsuite.com/twitter-statistics/">more than <em>5.000</em> tweets are being sent <em>every seconds</em></a>.</p>

    <p>Statistics like these show that significant research and engineering effort is needed to keep things on the Web working.
Specifically, work is needed to make it possible to
<em>store</em> all of this data as fast as possible,
and to make it <em>searchable</em> for <em>knowledge</em> as soon as possible.
As such, <em>evolving</em> Knowledge Graphs are the main focus of my research.</p>

    <h4 id="decentralized-knowledge-graphs">Decentralized Knowledge Graphs</h4>

    <p>The current <em>centralized</em> Knowledge Graphs do however not match well with the <em>decentralized</em> nature of the Web.
At the time of writing, these new Knowledge Graphs are in the hands of a few large corporations,
and intelligent agents on top of them are restricted to what these corporations allow them to do.
As people depend on the capabilities of these Knowledge Graphs,
large corporations gain significant control over the Web.
In the last couple of years, these centralized powers have proven to be problematic,
for example when <a href="https://fs.blog/2017/07/filter-bubbles/">the flow of information is redirected to influence election results</a>,
when <a href="https://www.theguardian.com/technology/live/2018/apr/10/mark-zuckerberg-testimony-live-congress-facebook-cambridge-analytica">personal information is being misused</a>,
or when <a href="https://quillette.com/2019/06/06/against-big-tech-viewpoint-discrimination/">information is being censored due to idealogical differences</a>.</p>

    <p>For these reasons, there is a massive push for <a href="https://ruben.verborgh.org/articles/redecentralizing-the-web/"><em>re-decentralizing the Web</em></a>,
where people regain <em>ownership</em> of their data.
Decentralization is however a technologically difficult thing.
As people do want ownership of their data, they do not want to give up their intelligent agents.
As such, this decentralization wave requires significant research effort to achieve the same capabilities as these <em>centralized</em> Knowledge Graphs,
which is why this is of important factor within my research.</p>

    <h3 id="introduction-research-question">Research Question</h3>

    <p>The goal of my research is to allow people to <em>publish</em> and <em>find</em> knowledge
without having to depend on large centralized entities,
with a focus on knowledge that <em>evolves</em> over time.
This lead me to the following research question for my PhD:</p>

    <blockquote id="research-question" class="strong">
      <p>How to store and query evolving knowledge graphs on the Web?</p>
    </blockquote>

    <p>During my research, I focused on <em>four</em> main challenges
related to this research question:</p>

    <ol>
      <li><strong>Experimentation requires <em>realistic</em> evolving data.</strong>
 <br />
 In order to <em>evaluate</em> the performance of systems that handle <em>evolving</em> data,
 a flexible method for <em>obtaining</em> such data needs to be available.</li>
      <li><strong>Indexing evolving data involves a <em>trade-off</em> between <em>storage size</em> and <em>lookup efficiency</em>.</strong>
 <br />
 Indexing techniques are used to improve the efficiency of querying,
 but comes at the cost of increased storage.
 As such, it is important to find a good <em>balance</em> between the amount of <em>storage</em>,
 and the amount of <em>querying speedup</em>.</li>
      <li><strong>The Web is highly <em>heterogeneous</em>.</strong>
 <br />
 Before data can be queried from the Web,
 the different <em>formats</em> and <em>interfaces</em> in which data is available need to be considered and harmonized.</li>
      <li><strong>Publishing <em>evolving</em> data via a <em>queryable interface</em> is costly.</strong>
 <br />
 Centralized querying interfaces are hard to scale for an increasing number of concurrent clients,
 especially when the data that is being queried over is continuously evolving.
 New kinds of interfaces and querying algorithms are needed to cope with this scalability issue.</li>
    </ol>

    <h3 id="introduction-outline">Outline</h3>

    <p>Corresponding to my four research challenges,
this thesis is based on the following four peer-reviewed publications:</p>

    <ul>
      <li>Ruben Taelman et al. <a href="https://www.rubensworks.net/raw/publications/2018/podigg.pdf">Generating Public Transport Data based on Population Distributions for <abbr title='Resource Description Framework'>RDF</abbr> Benchmarking</a>.
  <br />In: <em>In Semantic Web Journal</em>. IOS Press, 2019.</li>
      <li>Ruben Taelman et al. <a href="https://rdfostrich.github.io/article-jws2018-ostrich/">Triple Storage for Random-Access Versioned Querying of <abbr title='Resource Description Framework'>RDF</abbr> Archives</a>.
  <br />In: <em>Journal of Web Semantics</em>. Elsevier, 2019.</li>
      <li>Ruben Taelman et al. <a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica: a Modular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Query Engine for the Web</a>.
  <br />In: <em>International Semantic Web Conference</em>. Springer, October 2018.</li>
      <li>Ruben Taelman et al. <a href="https://www.rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf">Continuous Client-side Query Evaluation over Dynamic Linked Data</a>.
  <br />In: <em>The Semantic Web: ESWC 2016 Satellite Events, Revised Selected Papers</em>. Springer, May 2016.</li>
    </ul>

    <p>In <a href="#generating">Section 2</a> a mimicking algorithm (<em><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr></em>) is introduced for generating <em>realistic</em> evolving public transport data,
so that it can be used to benchmark systems that work with evolving data.
This algorithm is based on established concepts for designing public transport networks,
and takes into account population distributions for simulating the flow of vehicles.
Next, in <a href="#storing">Section 3</a>, a storage architecture and querying algorithms are introduced
for managing evolving data.
It has been implemented as a system called <em><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr></em>,
and extensive experimentation shows that this systems introduces a useful trade-off between storage size and querying efficiency.
In <a href="#querying">Section 4</a>, A modular query engine is introduced called <em>Comunica</em> that is able to cope with the heterogeneity of date on the Web.
This engine has been designed to be highly flexible, so that it simplifies research within the query domain,
where new query algorithms can for example be developed in a separate module, and plugged into the engine without much effort.
In <a href="#querying-evolving">Section 5</a>, a publishing interface and accompanying querying algorithm (<em><abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer</em>) is introduced and evaluated
to enable evolving data with a low volatility to be published at a low cost, and queried continuously.
Finally, this work is concluded in <a href="#conclusions">Section 6</a> and future research opportunities are discussed.</p>

  </section>

  
  <section class="sub-paper">
    <h2 id="generating">Generating Synthetic Evolving Data</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="https://pietercolpaert.be/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
      <li><a href="https://www.linkedin.com/in/erikmannens" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/erik_mannens">Erik Mannens</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://www.rubensworks.net/raw/publications/2018/podigg.pdf">Generating Public Transport Data based on Population Distributions for <abbr title='Resource Description Framework'>RDF</abbr> Benchmarking</a></p>

    <section id="generating_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <!--context-->
      <p>When benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems such as public transport route planners,
system evaluation needs to happen under various realistic circumstances,
which requires a wide range of datasets with different properties.
Real-world datasets are almost ideal, as they offer these realistic circumstances,
but they are often hard to obtain and inflexible for testing.
For these reasons, synthetic dataset generators are typically preferred
over real-world datasets due to their intrinsic flexibility.
Unfortunately, many synthetic dataset that are generated within benchmarks are insufficiently realistic,
raising questions about the generalizability of benchmark results to real-world scenarios.
<!--need-->
In order to benchmark geospatial and temporal <abbr title='Resource Description Framework'>RDF</abbr> data management systems
such as route planners
with sufficient external validity and depth,
<!--task-->
we designed <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>,
a highly configurable generation algorithm for synthetic public transport datasets
with realistic geospatial and temporal characteristics
comparable to those of their real-world variants.
The algorithm is inspired by real-world public transit network design
and scheduling methodologies.
<!--object-->
This article discusses the design and implementation of <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>
and validates the properties of its generated datasets.
<!--findings-->
Our findings show that the generator achieves a sufficient level of realism,
based on the existing coherence metric and new metrics we introduce specifically for the public transport domain.
<!--conclusions-->
Thereby, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>
provides a flexible foundation for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems with geospatial and temporal data.</p>

    </section>

    <section id="generating_introduction">
      <h3>Introduction</h3>

      <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">Resource Description Framework (<abbr title='Resource Description Framework'>RDF</abbr>)</a> <span class="references">[<a href="#ref-4">4</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a> <span class="references">[<a href="#ref-5">5</a>]</span> technologies enable distributed use and management of semantic data models.
Datasets with an interoperable domain model can be stored and queried by different data owners in different ways.
In order to discover the strengths and weaknesses of different storage and querying possibilities,
data-driven benchmarks with different sizes of datasets and varying characteristics can be used.</p>

      <p>Regardless of whether existing data-driven benchmarks use real or synthetic datasets,
the <em>external validity</em> of their results can be too limited,
which makes a generalization to other datasets difficult.
Real datasets, on the one hand, are often only scarcely available for testing,
and only cover very specific scenarios,
such that not all aspects of systems can be assessed.
Synthetic datasets, on the other hand, are typically generated by
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>-Benchmark-IJSWIS.pdf&#8221;><em>mimicking algorithms</em></a> <span class="references">[<a href="#ref-6">6</a>, <a href="#ref-7">7</a>, <a href="#ref-8">8</a>, <a href="#ref-9">9</a>]</span>,
which are not always sufficiently realistic <span class="references">[<a href="#ref-10">10</a>]</span>.
Features that are relevant for real-world datasets may not be tested.
As such, conclusions drawn from existing benchmarks
do not always apply to the envisioned real-world scenarios.
One way to get the best of both worlds
is to design mimicking algorithms that generate realistic synthetic datasets.</p>

      <p>The <em>public transport</em> domain provides data with both geospatial and temporal properties,
which makes this an especially interesting source of data for benchmarking.
Its representation as Linked Data is valuable because
1) of the many shared entities, such as stops, routes and trips, across different existing datasets on the Web.
2) These entities can be distributed over different datasets
and 3) benefit from interlinking for the improvement of discoverability.
Synthetic public transport datasets are particularly important and needed
in cases where public transport route planning algorithms are evaluated.
The Linked Connections framework <span class="references">[<a href="#ref-11">11</a>]</span> and Connection Scan Algorithm <span class="references">[<a href="#ref-12">12</a>]</span>
are examples of such public transport route planning systems.
Because of the limited availability of real-world datasets with desired properties,
these systems were evaluated with only a very low number of datasets, respectively one and three datasets.
A synthetic public transport dataset generator would make it easier for researchers
to include a higher number of realistic datasets with various properties in their evaluations,
which would be beneficial to the discovery of new insights from the evaluations.
Network size, network sparsity and temporal range are examples of such properties,
and different combinations of them may not always be available in real datasets,
which motivates the need for generating synthetic, but realistic datasets with these properties.</p>

      <p>Not only are public transport datasets useful for benchmarking route planning systems,
they are also highly useful for benchmarking <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf">geospatial</a> <span class="references">[<a href="#ref-13">13</a>, <a href="#ref-14">14</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1526709.1526856">temporal</a> <span class="references">[<a href="#ref-15">15</a>, <a href="#ref-16">16</a>]</span> <abbr title='Resource Description Framework'>RDF</abbr> systems
due to the intrinsic geospatial and temporal properties of public transport datasets.
While synthetic dataset generators already exist in the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://dx.doi.org/10.1007/978-3-642-41338-4_22">geospatial and temporal domain</a> <span class="references">[<a href="#ref-17">17</a>, <a href="#ref-18">18</a>]</span>,
no systems exist yet that focus on realism, and specifically look into the generation of public transport datasets.
As such, the main topic that we address in this work, is solving the need for realistic public transport datasets
with geospatial and temporal characteristics,
so that they can be used to benchmark <abbr title='Resource Description Framework'>RDF</abbr> data management and route planning systems.
More specifically, we introduce a mimicking algorithm for generating realistic public transport data,
which is the main contribution of this work.</p>

      <p>We observed a significant correlation between transport networks and the population distributions of their geographical areas,
which is why population distributions are the driving factor within our algorithm.
The cause of this correlation is obvious, considering transport networks are frequently used to transport people,
but other – possibly independent – factors exist that influence transport networks as well,
like certain points of interest such as tourist attractions and shopping areas.
Our algorithm is subdivided into five sequential steps,
inspired by existing methodologies from the domains of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">public transit planning</a> <span class="references">[<a href="#ref-19">19</a>]</span>
as a means to improve the realism of the algorithm’s output data.
These steps include the creation of a geospatial region, the placement of stops, edges and routes, and the scheduling of trips.
We provide an implementation of this algorithm, with different parameters to configure the algorithm.
Finally, we confirm the realism of datasets that are generated by this algorithm
using the existing generic structuredness metric <span class="references">[<a href="#ref-10">10</a>]</span>
and new metrics that we introduce, which are specific to the public transport domain.
The notable difference of this work compared to other synthetic dataset generators
is that our generation algorithm specializes in generating public transit networks,
while other generators either focus on other domains, or aim to be more general-purpose.
Furthermore, our algorithm is based on population distributions and existing methodologies from public transit network design.</p>

      <p>In the next section, we introduce the related work on dataset generation,
followed by the background on public transit network design, and transit feed formats in <a href="#generating_public-transit-background">Subsection 2.4</a>.
In <a href="#generating_research-question">Subsection 2.5</a>, we introduce the main research question and hypothesis of this work.
Next, our algorithm is presented in <a href="#generating_methodology">Subsection 2.6</a>, followed by its implementation in <a href="#generating_implementation">Subsection 2.7</a>.
In <a href="#generating_evaluation">Subsection 2.8</a>, we present the evaluation of our implementation,
followed by a discussion and conclusion in <a href="#generating_discussion">Subsection 2.9</a> and <a href="#generating_conclusions">Subsection 2.10</a>.</p>
    </section>

    <section id="generating_related-work">
      <h3>Related Work</h3>

      <p>In this section, we present the related work on spatiotemporal and <abbr title='Resource Description Framework'>RDF</abbr> dataset generation,</p>

      <p>Spatiotemporal database systems store instances that are described using an identifier, a spatial location and a timestamp.
In order to evaluate spatiotemporal indexing and querying techniques with datasets,
automatic means exist to generate such datasets with predictable characteristics <span class="references">[<a href="#ref-20">20</a>]</span>.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">Brinkhoff</a> <span class="references">[<a href="#ref-21">21</a>]</span> argues that moving objects tend to follow a predefined network.
Using this and other statements, he introduces a spatiotemporal dataset generator.
Such a network can be anything over which certain objects can move,
ranging from railway networks to air traffic connections.
The proposed parameter-based generator restricts the existence of the spatiotemporal objects to
a predefined time period <span class="kdmath">$\lbrack t_\text{min},t_\text{max})$</span>.
It is assumed that each edge in the network has a maximum allowed speed and capacity
over which objects can move at a certain speed.
The eventual speed of each object is defined by the maximum speed of its class,
the maximum allowed speed of the edge, and the congestion of the edge based on its capacity.
Furthermore, external events that can impact the movement of the objects, such as weather conditions,
are represented as temporal grids over the network, which apply a <em>decreasing factor</em> on the maximum speed of the objects in certain areas.
The existence of each object that is generated starts at a certain timestamp,
which is determined by a certain function,
and <em>dies</em> when it arrives at its destination.
The starting node of an object can be chosen based on three approaches:</p>

      <ul>
        <li><strong>dataspace-oriented approaches</strong>: Selecting the nearest node to a position picked from a two-dimensional distribution function that maps positions to nodes.</li>
        <li><strong>region-based approaches</strong>: Improvement of the data-space oriented approach where the data space is represented as a collection of cells, each having a certain chance of being the place of a starting node.</li>
        <li><strong>network-based approaches</strong>: Selection of a network node based on a one-dimensional distribution function that assigns a chance to each node.</li>
      </ul>

      <p>Determining the destination node using one of these approaches leads to non-satisfying results.
Instead, the destination is derived from the preferred length of a route.
Each route is determined as the fastest path to a destination, weighed by the external events.
Finally, the results are reported as either textual output, insertion into a database or a figure of the generated objects.
Compared to our work, this approach assumes a predefined network,
while our algorithm also includes the generation of the network.
For our work, we reuse the concepts of object speed and region-based node selection with relation to population distributions.</p>

      <p>In order to improve the testability of Information Discovery Systems,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">generic synthetic dataset generator</a> <span class="references">[<a href="#ref-22">22</a>]</span> was developed
that is able to generate synthetic data based on declarative graph definitions.
This graph is based on objects, attributes and relationships between them.
The authors propose to generate new instances, such as people, based on a set of dependency rules.
They introduce three types of dependencies for the generation of instances:</p>

      <ul>
        <li><strong>independent</strong>: Attribute values that are independent of other instances and attributes.</li>
        <li><strong>intra-record (horizontal) dependencies</strong>: Attribute values depending on other values of the same instance.</li>
        <li><strong>inter-record (vertical) dependencies</strong>: Relationships between different instances.</li>
      </ul>

      <p>Their engine is able to accept such dependencies as part of a semantic graph definition,
and iteratively create new instances to form a synthetic dataset.
This tool however outputs non-<abbr title='Resource Description Framework'>RDF</abbr> <abbr title='Comma separated values'>CSV</abbr> files, which makes it impossible to directly use this system for
the generation of public transport datasets in <abbr title='Resource Description Framework'>RDF</abbr> using existing ontologies.
For our public transport use case, individual entities such as stops, stations and connections
would be possible to generate up to a certain level using this declarative tool.
However, due to the underlying relation to population distributions
and specific restrictions for resembling real datasets,
declarative definitions are too limited.</p>

      <p>The need for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> data management systems is illustrated by the existence of the Linked Data Benchmark Council <span class="references">[<a href="#ref-23">23</a>]</span>
and the <a href="http://project-hobbit.eu/" class="mandatory" data-link-text="http:/​/​project-​hobbit.eu/​"><abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> H2020 EU project</a> for benchmarking of Big Linked Data.
<abbr title='Resource Description Framework'>RDF</abbr> benchmarks are typically based on certain datasets that are used as input to the tested systems.
Many of these datasets are not always very closely related to real datasets <span class="references">[<a href="#ref-10">10</a>]</span>,
which may result in conclusions drawn from benchmarking results that do not translate to system behaviours in realistic settings.</p>

      <p>Duan et al. <span class="references">[<a href="#ref-10">10</a>]</span> argue that the realism of an <abbr title='Resource Description Framework'>RDF</abbr> dataset can be measured
by comparing the <em>structuredness</em> of that dataset with a realistic equivalent.
The authors show that real-world datasets are typically less structured than their synthetic counterparts,
which can results in significantly different benchmarking results,
since this level of structuredness can have an impact on how certain data is stored in <abbr title='Resource Description Framework'>RDF</abbr> data management systems.
This is because these systems may behave differently on datasets with different levels of structuredness,
as they can have certain optimizations for some cases.
In order to measure this structuredness, the authors introduce the <em>coherence</em>
metric of a dataset <span class="kdmath">$D$</span> with a type system <span class="kdmath">$\mathcal{T}$</span> that can be calculated as follows:</p>

      <div class="kdmath">$$
\begin{aligned}
    CH(\mathcal{T}, D) = \sum_{\forall{T \in \mathcal{T}}} WT(<abbr title='Cross-version join'>CV</abbr>(T, D)) * <abbr title='Cross-version join'>CV</abbr>(T, D)
\end{aligned}
$$</div>

      <p>The type system <span class="kdmath">$\mathcal{T}$</span> contains all the <abbr title='Resource Description Framework'>RDF</abbr> types that are present in a dataset.
<span class="kdmath">$<abbr title='Cross-version join'>CV</abbr>(T, D)$</span> represents the <em>coverage</em> of a type <span class="kdmath">$T$</span> in a dataset <span class="kdmath">$D$</span>,
and is calculated as the fraction of type instances that set a value for all its properties.
The factor <span class="kdmath">$WT(<abbr title='Cross-version join'>CV</abbr>(T, D))$</span> is used to weight this sum,
so that the coherence is always a value between 0 and 1, with 1 representing a perfect structuredness.
A maximal coherence means that all instances in the dataset have values for all possible properties in the type system,
which is for example the case in relational databases without optional values.
Based on this metric, the authors introduce a generic method for creating variants of real datasets
with different sizes while maintaining a similar structuredness.
The authors describe a method to calculate the coverage value of this dataset,
which has been <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1700/paper-02.pdf">implemented as a procedure in the Virtuoso <abbr title='Resource Description Framework'>RDF</abbr> store</a> <span class="references">[<a href="#ref-9">9</a>]</span>.
As the goal of our work is to generate <em>realistic</em> <abbr title='Resource Description Framework'>RDF</abbr> public transport datasets,
we will use this metric to compare the realism of generated datasets with real datasets.
As this high-level metric is used to define <em>realism</em> over any kind of <abbr title='Resource Description Framework'>RDF</abbr> dataset,
we will introduce new metrics to validate the realism for specifically the case of public transport datasets.</p>

    </section>

    <section id="generating_public-transit-background">
      <h3>Public Transit Background</h3>

      <p>In this section, we present background on public transit planning that is essential to this work.
We discuss existing public transit network planning methodologies
and formats for exchanging transit feeds.</p>

      <h4 id="public-transit-planning">Public Transit Planning</h4>

      <p>The domain of public transit planning entails the design of public transit networks,
rostering of crews, and all the required steps inbetween.
The goal is to maximize the quality of service for passengers while minimizing the costs for the operator.
Given a public demand and a topological area, this planning process aims to obtain routes, timetables and vehicle and crew assignment.
A survey about 69 existing public transit planning approaches
shows that these processes are typically subdivided into <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">five sequential steps</a> <span class="references">[<a href="#ref-19">19</a>]</span>:</p>

      <ol>
        <li><strong>route design</strong>, the placement of transit routes over an existing network.</li>
        <li><strong>frequencies setting</strong>, the temporal instantiation of routes based on the available vehicles and estimated demand.</li>
        <li><strong>timetabling</strong>, the calculation of arrival and departure times at each stop based on estimated demand.</li>
        <li><strong>vehicle scheduling</strong>, vehicle assignment to trips.</li>
        <li><strong>crew scheduling and rostering</strong>, the assignment of drivers and additional crew to trips.</li>
      </ol>

      <p>In this paper, we only consider the first three steps for our mimicking algorithm,
which lead to all the required information
that is of importance to passengers in a public transit schedule.
We present the three steps from this survey in more detail hereafter.</p>

      <p>The first step, route design, requires the topology of an area and public demand as input.
This topology describes the network in an area, which contains possible stops and edges between these stops.
Public demand is typically represented as <em>origin-destination</em> (OD) matrices,
which contain the number of passengers willing to go from origin stops to destination stops.
Given this input, routes are designed based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives</a> <span class="references">[<a href="#ref-19">19</a>]</span>:</p>

      <ul>
        <li><strong>area coverage</strong>: The percentage of public demand that can be served.</li>
        <li><strong>route and trip directness</strong>: A metric that indicates how much the actual trips from passengers deviate from the shortest path.</li>
        <li><strong>demand satisfaction</strong>: How many stops are close enough to all origin and destination points.</li>
        <li><strong>total route length</strong>: The total distance of all routes, which is typically minimized by operators.</li>
        <li><strong>operator-specific objectives</strong>: Any other constraints the operator has, for example the shape of the network.</li>
        <li><strong>historical background</strong>: Existing routes may influence the new design.</li>
      </ul>

      <p>The next step is the setting of frequencies, which is based on the routes from the previous step, public demand and vehicle availability.
The main objectives in this step are based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">metrics</a> <span class="references">[<a href="#ref-19">19</a>]</span>:</p>

      <ul>
        <li><strong>demand satisfaction</strong>: How many stops are serviced frequently enough to avoid overcrowding and long waiting times.</li>
        <li><strong>number of line runs</strong>: How many times each line is serviced – a trade-off between the operator’s aim for minimization and the public demand for maximization.</li>
        <li><strong>waiting time bounds</strong>: Regulation may put restrictions on minimum and maximum waiting times between line runs.</li>
        <li><strong>historical background</strong>: Existing frequencies may influence the new design.</li>
      </ul>

      <p>The last important step for this work is timetabling, which takes the output from the previous steps as input, together with the public demand.
The objectives for this step are the following:</p>

      <ul>
        <li><strong>demand satisfaction</strong>: Total travel time for passengers should be minimized.</li>
        <li><strong>transfer coordination</strong>: Transfers from one line to another at a certain stop should be taken into account during stop waiting times, including how many passengers are expected to transfer.</li>
        <li><strong>fleet size</strong>: The total amount of available vehicles and their usage will influence the timetabling possibilities.</li>
        <li><strong>historical background</strong>: Existing timetables may influence the new design.</li>
      </ul>

      <h4 id="transit-feed-formats">Transit Feed Formats</h4>

      <p>The de-facto standard for public transport time schedules is
the <a href="https://developers.google.com/transit/gtfs/" class="mandatory" data-link-text="https:/​/​developers.google.com/​transit/​gtfs/​">General Transit Feed Specification (<abbr title='General Transit Feed Specification'>GTFS</abbr>)</a>.
<abbr title='General Transit Feed Specification'>GTFS</abbr> is an exchange format for transit feeds, using a series of <abbr title='Comma separated values'>CSV</abbr> files contained in a zip file.
The specification uses the following terminology to define the rules for a public transit system:</p>

      <ul>
        <li><strong>Stop</strong> is a geospatial location where vehicles stop and passengers can get on or off, such as platform 3 in the train station of Brussels.</li>
        <li><strong>Stop time</strong> indicates a scheduled arrival and departure time at a certain stop.</li>
        <li><strong>Route</strong> is a time-independent collection of stops, describing the sequence of stops a certain vehicle follows in a certain public transit line. For example the train route from Brussels to Ghent.</li>
        <li><strong>Trip</strong> is a collection of stops with their respective stop times, such as the route from Brussels to Ghent at a certain time.</li>
      </ul>

      <p>The \zip file is put online by a public transit operator, to be downloaded by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1007/978-3-642-02094-0_7">route planning</a> <span class="references">[<a href="#ref-24">24</a>]</span> software.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1227161.1227166">Two models are commonly used to then extract these rules into a graph</a> <span class="references">[<a href="#ref-25">25</a>]</span>.
In a <em>time-expanded model</em>, a large graph is modeled with arrivals and departures as nodes and edges connect departures and arrivals together.
The weights on these edges are constant.
In a <em>time-dependent model</em>, a smaller graph is modeled in which vertices are physical stops and edges are transit connections between them.
The weights on these edges change as a function of time.
In both models, Dijkstra and Dijkstra-based algorithms can be used to calculate routes.</p>

      <p>In contrast to these two models, the <em>Connection Scan Algorithm</em> <span class="references">[<a href="#ref-12">12</a>]</span>
takes an ordered array representation of <em>connections</em> as input.
A connection is the actual departure time at a stop and an arrival at the next stop.
These connections can be given a <abbr title='Internationalized Resource Identifier'>IRI</abbr>, and described using <abbr title='Resource Description Framework'>RDF</abbr>, using the Linked Connections <span class="references">[<a href="#ref-11">11</a>]</span> ontology.
For this base algorithm and its derivatives, a connection object is the smallest building block of a transit schedule.</p>

      <p>In our work, generated public transport networks and time schedules
can be serialized to both the <abbr title='General Transit Feed Specification'>GTFS</abbr> format, and <abbr title='Resource Description Framework'>RDF</abbr> datasets using the Linked Connections ontology.</p>

    </section>

    <section id="generating_research-question">
      <h3>Research Question</h3>

      <p>In order to generate public transport networks and schedules,
we start from the hypothesis that both
are correlated with the population distribution within the same area.
More populated areas are expected to have more nearby and more frequent access to public transport,
corresponding to the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">recurring demand satisfaction objective in public transit planning</a> <span class="references">[<a href="#ref-19">19</a>]</span>.
When we calculate the correlation
between the distribution of stops in an area and its population distribution,
we discover a positive correlation of 0.439 for Belgium and 0.459 for the Netherlands (<em>p</em>-values in both cases &lt; 0.00001),
thereby validating our hypothesis with a confidence of 99%.
Because of the continuous population variable and the binary variable indicating whether or not there is a stop,
the correlation is calculated using the <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg-evaluate/blob/master/stats/correlation.r&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg-​evaluate/​blob/​master/​stats/​correlation.r&#8221;>point-biserial correlation coefficient</a>.
For the calculation of these correlations, we ignored the population value outliers.
Following this conclusion, our mimicking algorithm will use such population distributions as input,
and derive public transport networks and trip instances.</p>

      <p>The main objective of a mimicking algorithm is to create <em>realistic</em> data,
so that it can be used to by benchmarks to evaluate systems under realistic circumstances.
We will measure dataset realism in high-level by comparing the levels of structuredness
of real-world datasets and their synthetic variants
using the <em>coherence metric</em> introduced by Duan et al. <span class="references">[<a href="#ref-10">10</a>]</span>.
Furthermore, we will measure the realism of different characteristics within public transport datasets,
such as the location of stops, density of the network of stops, length of routes or the frequency of connections.
We will quantify these aspects by measuring the distance of each aspect between real and synthetic datasets.
These dataset characteristics will be linked with potential evaluation metrics within <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
and tasks to evaluate them.
This generic coherence metric together with domain-specific metrics will provide a way to evaluate dataset realism.</p>

      <p>Based on this, we introduce the following research question for this work:</p>

      <blockquote id="generating_researchquestion">
        <p>Can population distribution data be used to generate realistic synthetic public transport networks and scheduling?</p>
      </blockquote>

      <p>We provide an answer to this question by first introducing an
algorithm for generating public transport networks and their scheduling based on population distributions in <a href="#generating_methodology">Subsection 2.6</a>.
After that, we validate the realism of datasets that were generated using an implementation of this algorithm in <a href="#generating_evaluation">Subsection 2.8</a>.</p>

    </section>

    <section id="generating_methodology">
      <h3>Method</h3>

      <p>In order to formulate an answer to our research question,
we designed a mimicking algorithm that generates realistic synthetic public transit feeds.
We based it on techniques from the domains of public transit planning, spatiotemporal and <abbr title='Resource Description Framework'>RDF</abbr> dataset generation.
We reuse the route design, frequencies setting and timetabling steps from the domain public transit planning,
but prepend this with a network generation phase.</p>

      <p><a href="#generating_fig:methodology:datamodel">Fig. 3</a> shows the model of the generated public transit feeds,
with connections being the primary data element.</p>

      <figure id="generating_fig:methodology:datamodel">
<img src="generating/img/datamodel.svg" alt="<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> data model&#8221; />
<figcaption>
          <p><span class="label">Fig. 3:</span> The resources (rectangle), literals (dashed rectangle) and properties (arrows) used to model the generated public transport data.
Node and text colors indicate vocabularies.</p>
        </figcaption>
</figure>

      <p>We consider different properties in this model based on the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">independent, intra-record or inter-record dependency rules</a> <span class="references">[<a href="#ref-22">22</a>]</span>, as discussed in <a href="#generating_related-work">Subsection 2.3</a>.
The arrival time in a connection can be represented as a fully intra-record dependency,
because it depends on the time it departed and the stops it goes between.
The departure time in a connection is both an intra-record and inter-record dependency,
because it depends on the stop at which it departs,
but also on the arrival time of the connection before it in the trip.
Furthermore, the delay value can be seen as an inter-record dependency,
because it is influenced by the delay value of the previous connection in the trip.
Finally, the geospatial location of a stop depends on the location of its parent station,
so this is also an inter-record dependency.
All other unmentioned properties are independent.</p>

      <p>In order to generate data based on these dependency rules, our algorithm is subdivided in five steps:</p>

      <ol>
        <li><strong>Region</strong>: Creation of a two-dimensional area of cells annotated with population density information.</li>
        <li><strong>Stops</strong>: Placement of stops in the area.</li>
        <li><strong>Edges</strong>: Connecting stops using edges.</li>
        <li><strong>Routes</strong>: Generation of routes between stops by combining edges.</li>
        <li><strong>Trips</strong>: Scheduling of timely trips over routes by instantiating connections.</li>
      </ol>

      <p>These steps are not fully sequential, since stop generation is partially executed before and after edge generation.
The first three steps are required to generate a network,
step 4 corresponds to the route design step in public transit planning
and step 5 corresponds to both the frequencies setting and timetabling.
These steps are explained in the following subsections.</p>

      <h4 id="region">Region</h4>

      <p>In order to create networks, we sample geographic regions in which such networks exist as two-dimensional matrices.
The resolution is defined as a configurable number of cells per square of one latitude by one longitude.
Network edges are then represented as links between these cells.
Because our algorithm is population distribution-based, each cell contains a population density.
These values can either be based on real population information from countries,
or this can be generated based on certain statistical distributions.
For the remainder of this paper, we will reuse the population distribution from Belgium as a running example, as illustrated in <a href="#generating_fig:methodology:region">Fig. 4</a>.</p>

      <figure id="generating_fig:methodology:region">
<img src="generating/img/region.png" alt="Heatmap of the population distribution in Belgium" />
<figcaption>
          <p><span class="label">Fig. 4:</span> Heatmap of the population distribution in Belgium, which is illustrated for each cell
as a scale going from white (low), to red (medium) and black (high).
The actual placement of train stops are indicated as green points.</p>
        </figcaption>
</figure>

      <h4 id="stops">Stops</h4>

      <p>Stop generation is divided into two steps.
First, stops are placed based on population values,
then the edge generation step is initiated
after which the second phase of stop generation is executed where additional stops are created based on the generated edges.</p>

      <p><strong>Population-based</strong>
For the initial placement of stops, our algorithm only takes a population distribution as input.
The algorithm iteratively selects random cells in the two-dimensional area, and tags those cells as stops.
To make it <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">region-based</a> <span class="references">[<a href="#ref-21">21</a>]</span>,
the selection uses a weighted Zipf-like-distribution, where cells with high population values have a higher chance
of being picked than cells with lower values.
The shape of this Zipf curve can be scaled to allow for different stop distributions to be configured.
Furthermore, a minimum distance between stops can be configured, to avoid situations where all stops are placed in highly population areas.</p>

      <p><strong>Edge-based</strong>
Another stop generation phase exists after the edge generation
because real transit networks typically show line artifacts for stop placement.
<a href="#generating_fig:methodology:stopplacementgs">Fig. 6</a> shows the actual train stops in Belgium, which clearly shows line structures.
Stop placement after the first generation phase results can be seen in <a href="#generating_fig:methodology:stopplacementp1">Fig. 7</a>,
which does not show these line structures.
After the second stop generation phase, these line structures become more apparent as can be seen in <a href="#generating_fig:methodology:stopplacementp2">Fig. 13</a>.</p>

      <figure id="generating_fig:methodology:stopplacement">

<figure id="generating_fig:methodology:stopplacementgs" class="subfigure">
<img src="generating/img/stops_gs.png" alt="Real stops" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 5:</span> Real stops with line structures.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp1" class="subfigure">
<img src="generating/img/stops_parameterized_1.png" alt="Generation phase step 1" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 7:</span> Synthetic stops after the first stop generation phase without line structures.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_parameterized_2.png" alt="Generation phase step 2" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 13:</span> Synthetic stops after the second stop generation phase with line structures.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Placement of train stops in Belgium, each dot represents one stop.</p>
        </figcaption>
</figure>

      <p>In this second stop generation phase,
edges are modified so that sufficiently populated areas will be included in paths formed by edges,
as illustrated by <a href="#generating_fig:methodology:stopsphase2">Fig. 9</a>.
Random edges will iteratively be selected, weighted by the edge length measured as
Euclidian distance.
(The Euclidian distance based on geographical coordinates is always used to calculate distances in this work.)
On each edge, a random cell is selected weighed by the population value in the cell.
Next, a weighed random point in a certain area around this point is selected.
This selected point is marked as a stop, the original edge is removed and two new edges are added,
marking the path between the two original edge nodes and the newly selected node.</p>

      <figure id="generating_fig:methodology:stopsphase2">

<figure id="generating_fig:methodology:stopsphase2_1" class="subfigure">
<img src="generating/img/stops_phase2_1.svg" alt="Real stops" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 9:</span> Selecting a weighted random point on the edge.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_2" class="subfigure">
<img src="generating/img/stops_phase2_2.svg" alt="Generation phase step 1" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 11:</span> Defining an area around the selected point.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_3" class="subfigure">
<img src="generating/img/stops_phase2_3.svg" alt="Generation phase step 2" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 12:</span> Choosing a random point within the area, weighted by population value.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_phase2_4.svg" alt="Generation phase step 2" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 13:</span> Modify edges so that the path includes this new point.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Illustration of the second phase of stop generation where edges are modified to include sufficiently populated areas in paths.</p>
        </figcaption>
</figure>

      <h4 id="edges">Edges</h4>
      <p>The next phase in public transit network generation connects the stops that were generated in the previous phase with edges.
In order to simulate real transit network structures, we split up this generation phase into three sequential steps.
In the first step, clusters of nearby stops are formed, to lay the foundation for short-distance routes.
Next, these local clusters are connected with each other, to be able to form long-distance routes.
Finally, a cleanup step is in place to avoid abnormal edge structures in the network.</p>

      <p><strong>Short-distance</strong>
The formation of clusters with nearby stations is done using agglomerative hierarchical clustering.
Initially, each stop is part of a seperate cluster, where each cluster always maintains its centroid.
The clustering step will iteratively try to merge two clusters with their centroid distance below a certain threshold.
This threshold will increase for each iteration, until a maximum value is reached.
The maximum distance value indicates the maximum inter-stop distance for forming local clusters.
When merging two clusters, an edge is added between the closest stations from the respective clusters.
The center location of the new cluster is also recalculated before the next iteration.</p>

      <p><strong>Long-distance</strong>
At this stage, we have several clusters of nearby stops.
Because all stops need to be reachable from all stops, these separate clusters also need to be connected.
This problem is related to the domain of route planning over public transit networks,
in which networks can be decomposed into smaller clusters of nearby stations to improve the efficiency of route planning.
Each cluster contains one or more <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1137/1.9781611974317.2"><a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611974317.2"><em>border stations</em></a></span> <span class="references">[<a href="#ref-26">26</a>]</span>, which are the only points
through which routes can be formed between different clusters.
We reuse this concept of border stations, by iteratively picking a random cluster,
identifying its closest cluster based on the minimal possible stop distance, and connecting their border stations using a new edge.
After that, the two clusters are merged.
The iteration will halt when all clusters are merged and there is only one connected graph.</p>

      <p><strong>Cleanup</strong>
The final cleanup step will make sure that the number of stops that are connected by only one edge are reduced.
In real train networks, the majority of stations are connected with at least more than one other stations.
The two earlier generation steps however generate a significant number of <em>loose stops</em>,
which are connected with only a single other stop with a direct edge.
In this step, these loose stops are identified, and an attempt is made to connect them to other nearby stops as shown in <a href="#generating_alg:methodology:loosestops">Algorithm 1</a>.
For each loose stop, this is done by first identifying the direction of the single edge of the loose stop on line 18.
This direction is scaled by the radius in which to look for stops, and defines the stepsize for the loop the starts on line 20.
This loop starts from the loose stop and iteratively moves the search position in the defined direction, until it finds a random stop in the radius,
or the search distance exceeds the average distance of between the stops in the neighbourhood of this loose stop.
This random stop from line 22 can be determined
by finding all stations that have a distance to the search point that is below the radius, and picking a random stop from this collection.
If such a stop is found, an edge is added from our loose stop to this stop.</p>

      <figure id="generating_alg:methodology:loosestops" class="algorithm numbered">
<pre><code>FUNCTION RemoveLooseStops(S, E, N, O, r)
</code><code>  INPUT:
</code><code>    Set of stops S
</code><code>    Set of edges E between the stops from S
</code><code>    Maximum number N of closest stations to consider
</code><code>    Maximum average distance O around a stop to be considered a loose station
</code><code>    Radius r in which to look for stops.
</code><code>FOREACH s in S with degree of 1 w.r.t. E DO
</code><code>    sx = x coordinate of s
</code><code>    sy = y coordinate of s
</code><code>    C = N closest stations to s in S excluding s
</code><code>    c = closest station to s in S excluding s
</code><code>    cx = x coordinate of c
</code><code>    cy = y coordinate of c
</code><code>    a = average distance between each pair of stops in C
</code><code>    IF a &lt;= O and C not empty THEN
</code><code>        dx= (sx - cx) * r
</code><code>        dy= (sy - cy) * r
</code><code>        ox = sx; oy = sy
</code><code>        WHILE distance between o and s &lt; a DO
</code><code>            ox += dx; oy += dy
</code><code>            s&#39; = random station around o with radius a * r
</code><code>            IF s&#39; exists
</code><code>                add edge between s and s&#39; to E and continue next for-loop iteration
</code></pre>
<figcaption>
          <p><span class="label">Algorithm 1:</span> Reduce the number of loose stops by adding additional edges.</p>
        </figcaption>
</figure>

      <p><a href="#generating_fig:methodology:edges">Fig. 14</a> shows an example of these three steps.
After this phase, a network with stops and edges is available, and the actual transit planning can commence.</p>

      <figure id="generating_fig:methodology:edges">
 
<figure id="generating_fig:methodology:edges1" class="subfigure">
<img src="generating/img/edges_1.svg" alt="Real stops" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 14:</span> Formation of local clusters.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:edges2" class="subfigure">
<img src="generating/img/edges_2.svg" alt="Generation phase step 1" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 16:</span> Connecting clusters through border stations.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:edges3" class="subfigure">
<img src="generating/img/edges_3.svg" alt="Generation phase step 2" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 17:</span> Cleanup of loose stops.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Example of the different steps in the edges generation algorithm.</p>
        </figcaption>
</figure>

      <p><strong>Generator Objectives</strong>
The main guaranteed objective of the edge generator is that the stops form a single connected transit network graph.
This is to ensure that all stops in the network can be reached from any other stop using at least one path through the network.</p>

      <h4 id="routes">Routes</h4>

      <p>Given a network of stops and edges, this phase generates routes over the network.
This is done by creating short and long distance routes in two sequential steps.</p>

      <p><strong>Short-distance</strong>
The goal of the first step is to create short routes where vehicles deliver each passed stop.
This step makes sure that all edges are used in at least one route,
this ensures that each stop can at least be reached from each other stop with one or more transfers to another line.
The algorithm does this by first determining a subset of the largest stops in the network, based on the population value.
The shortest path from each large stop to each other large stop through the network is determined.
if this shortest path is shorter than a predetermined value in terms of the number of edges,
then this path is stored as a route, in which all passed stops are considered as actual stops in the route.
For each edge that has not yet been passed after this, a route is created by iteratively
adding unpassed edges to the route that are connected to the edge until an edge is found that has already been passed.</p>

      <p><strong>Long-distance</strong>
In the next step, longer routes are created, where the transport vehicle not necessarily halts at each passed stop.
This is done by iteratively picking two stops from the list of largest stops using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">network-based method</a> <span class="references">[<a href="#ref-21">21</a>]</span> with each stop having an equal chance to be selected.
A heuristical shortest path algorithm is used to determine a route between these stops.
This algorithm searches for edges in the geographical direction of the target stop.
This is done to limit the complexity of finding long paths through potentially large networks.
A random amount of the largest stops on the path are selected, where the amount
is a value between a minimum and maximum preconfigured route length.
This iteration ends when a predetermined number of routes are generated.</p>

      <p><strong>Generator Objectives</strong>
This algorithm takes into account the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives of route design</a> <span class="references">[<a href="#ref-19">19</a>]</span>, as discussed in <a href="#generating_related-work">Subsection 2.3</a>.
More specifically, by first focusing on the largest stops, a minimal level of <em>area coverage</em> and <em>demand satisfaction</em> is achieved,
because the largest stops correspond to highly populated areas, which therefore satisfies at least a large part of the population.
By determining the shortest path between these largest stops, the <em>route and trip directness</em> between these stops is optimal.
Finally, by not instantiating all possible routes over the network, the <em>total route length</em> is limited to a reasonable level.</p>

      <h4 id="generating_subsec-methodology-trips">Trips</h4>

      <p>A time-agnostic transit network with routes has been generated in the previous steps.
In this final phase, we temporally instantiate routes
by first determining starting times for trips,
after which the following stop times can be calculated based on route distances.
Instead of generating explicit timetables, as is done in typical transit scheduling methodologies,
we create fictional rides of vehicles.
In order to achieve realistic trip times, we approximate real trip time distributions,
with the possibility to encounter delays.</p>

      <p>As mentioned before in <a href="#generating_related-work">Subsection 2.3</a>, each consecutive pair of start and stop time in a trip over an edge corresponds to a connection.
A connection can therefore be represented as a pair of timestamps,
a link to the edge representing the departure and arrival stop,
a link to the trip it is part of,
and its index within this trip.</p>

      <p><strong>Trip Starting Times</strong>
The trips generator iteratively creates new connections until a predefined number is reached.
For each connection, a random route is selected with a larger chance of picking a long route.
Next, a random start time of the connection is determined.
This is done by first picking a random day within a certain range.
After that, a random hour of the day is determined using a preconfigured distribution.
This distribution is derived from the public logs of <a href="https://hello.irail.be" class="mandatory" data-link-text="https:/​/​hello.irail.be">iRail</a>,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">route planning <abbr title='Application Programming Interface'>API</abbr> in Belgium</a> <span class="references">[<a href="#ref-27">27</a>]</span>.
A seperate hourly distribution is used for weekdays and weekends, which is chosen depending on the random day that was determined.</p>

      <p><strong>Stop Times</strong>
Once the route and the starting time have been determined, different stop times across the trip can be calculated.
For this, we take into account the following factors:</p>

      <ul>
        <li>Maximum vehicle speed <span class="kdmath">$\omega$</span>, preconfigured constant.</li>
        <li>Vehicle acceleration <span class="kdmath">$\varsigma$</span>, preconfigured constant.</li>
        <li>Connection distance <span class="kdmath">$\delta$</span>, Euclidian distance between stops in network.</li>
        <li>Stop size <span class="kdmath">$\sigma$</span>, derived from population value.</li>
      </ul>

      <p>For each connection in the trip, the time it takes for a vehicle to move between the two stops over a certain distance is calculated
using the formula in <a href="#generating_math:methodology:distanceduration">Equation 3</a>.
<a href="#generating_math:methodology:timetomaxspeed">Equation 1</a> calculates the required time to reach maximum speed
and <a href="#generating_math:methodology:distancetomaxspeed">Equation 2</a> calculates the required distance to reach maximum speed.
This formula simulates the vehicle speeding up until its maximum speed, and slowing down again until it reaches its destination.
When the distance is too short, the vehicle will not reach its maximum speed,
and just speeds up as long as possible until is has to slow down again to stop in time.</p>

      <figure id="generating_math:methodology:timetomaxspeed" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    T_\omega &= \omega / \varsigma
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 1:</span> Time to reach maximum speed.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:methodology:distancetomaxspeed" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    \delta_\omega &= T_\omega^2 \cdot \varsigma
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 2:</span> Distance to reach maximum speed.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:methodology:distanceduration" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\begin{cases}
    2T_\omega + (\delta - 2 \delta_\omega) / \omega &\text{ if } \delta_\omega < \delta / 2 \\
    \sqrt{2\delta / \varsigma} &\text{ otherwise}
\end{cases}
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 3:</span> Duration for a vehicle to move between two stops.</p>
        </figcaption>
      </figure>

      <p>Not only the connection duration, but also the waiting times of the vehicle at each stop are important for determining the stop times.
These are calculated as a constant minimum waiting time together with a waiting time that increases for larger stop sizes <span class="kdmath">$\sigma$</span>,
this increase is determined by a predefined growth factor.</p>

      <p><strong>Delays</strong>
Finally, each connection in the trip will have a certain chance to encounter a delay.
When a delay is applicable, a delay value is randomly chosen within a certain range.
Next to this, also a cause of the delay is determined from a preconfigured list.
These causes are based on the Traffic Element Events from the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>,
which contains a number of events that are not planned by the network operator such as strikes, bad weather or animal collisions.
Different types of delays can have a different impact factor of the delay value,
for instance, simple delays caused by rush hour would have a lower impact factor than a major train defect.
Delays are carried over to next connections in the trip, with again a chance of encountering additional delay.
Furthermore, these delay values can also be reduced when carried over to the next connection by a certain predetermined factor,
which simulates the attempt to reduce delays by letting vehicles drive faster.</p>

      <p><strong>Generator Objectives</strong>
For trip generation, we take into account several objectives from the
setting of frequencies and timetabling from <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transit planning</a> <span class="references">[<a href="#ref-19">19</a>]</span>.
By instantiating more long distance routes, we aim to increase <em>demand satisfaction</em> as much as possible,
because these routes deliver busy and populated areas, and the goal is to deliver these more frequently.
Furthermore, by taking into account realistic time distributions for trip instantiation, we also adhere to this objective.
Secondly, by ensuring waiting times at each stop that are longer for larger stations,
the <em>transfer coordination</em> objective is taken into account to some extent.</p>

    </section>

    <section id="generating_implementation">
      <h3>Implementation</h3>

      <p>In this section, we discuss the implementation details of <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, based on the generator algorithm introduced in <a href="#generating_methodology">Subsection 2.6</a>.
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is split up into two parts: the main <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> generator, which outputs <abbr title='General Transit Feed Specification'>GTFS</abbr> data, and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC,
which depends on the main generator to output <abbr title='Resource Description Framework'>RDF</abbr> data.
Serialization in <abbr title='Resource Description Framework'>RDF</abbr> using existing ontologies, such as the <a href="http://vocab.gtfs.org/terms" class="mandatory" data-link-text="http:/​/​vocab.gtfs.org/​terms"><abbr title='General Transit Feed Specification'>GTFS</abbr></a>
and <a href="http://semweb.mmlab.be/ns/linkedconnections" class="mandatory" data-link-text="http:/​/​semweb.mmlab.be/​ns/​linkedconnections">Linked Connections ontologies</a>,
allows this inherently linked data to be used within <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
where it can for instance be used for benchmarking purposes.
Providing output in <abbr title='General Transit Feed Specification'>GTFS</abbr> will allow this data to be used directly within all systems
that are able to handle transit feeds, such as route planning systems.
The two generator parts will be explained hereafter, followed by a section on how the generator can be configured using various parameters.</p>

      <h4 id="podigg"><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr></h4>

      <p>The main requirement of our system is the ability to generate realistic
public transport datasets using the mimicking algorithm that was introduced in <a href="#generating_methodology">Subsection 2.6</a>.
This means that given a population distribution of a certain region,
the system must be able to design a network of routes,
and determine timely trips over this network.</p>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is implemented to achieve this goal. It is written in JavaScript using Node.js,
and is available under an open license on <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg&#8221;>GitHub</a>.
In order to make installation and usage more convenient,
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is available as a Node module on the <a href="https://www.npmjs.com/package/podigg" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg">NPM package manager</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg/​">Docker Hub</a> to easily run on any platform.
Every sub-generator that was explained in <a href="#generating_methodology">Subsection 2.6</a>, is implemented as a separate module.
This makes <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> highly modifiable and composable, because different implementations of sub-generators can easily be added and removed.
Furthermore, this flexible composition makes it possible to use real data instead of certain sub-generators.
This can be useful for instance when a certain public transport network is already available, and only the trips and connections need to be generated.</p>

      <p>We designed <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> to be highly configurable
to adjust the characteristics of the generated output across different levels,
and to define a certain <em>seed</em> parameter for producing deterministic output.</p>

      <p>All sub-generators store generated data in-memory, using list-based data structures directly corresponding to the <abbr title='General Transit Feed Specification'>GTFS</abbr> format.
This makes <abbr title='General Transit Feed Specification'>GTFS</abbr> serialization a simple and efficient process.
<a href="#generating_table:implementation:gtfsfiles">Table 1</a> shows the <abbr title='General Transit Feed Specification'>GTFS</abbr> files that are generated by the different <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> modules.
This table does not contain references to the region and edges generator,
because they are only used internally as prerequisites to the later steps.
All required files are created to have a valid <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.
Next to that, the optional file for exceptional service dates is created.
Furthermore, <code>delays.txt</code> is created, which is not part of the <abbr title='General Transit Feed Specification'>GTFS</abbr> specification.
It is an extension we provide in order to serialize delay information about each connection in a trip.
These delays are represented in a <abbr title='Comma separated values'>CSV</abbr> file containing columns for referring to a connection in a trip,
and contains delay values in milliseconds and a certain reason per connection arrival and departure,
as shown in <a href="#generating_listing:example:delays">Listing 2</a>.</p>

      <figure id="generating_table:implementation:gtfsfiles" class="table">

        <table>
          <thead>
            <tr>
              <th>File</th>
              <th>Generator</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong><code>agency.txt</code></strong></td>
              <td><em>Constant</em></td>
            </tr>
            <tr>
              <td><strong><code>stops.txt</code></strong></td>
              <td>Stops</td>
            </tr>
            <tr>
              <td><strong><code>routes.txt</code></strong></td>
              <td>Routes</td>
            </tr>
            <tr>
              <td><strong><code>trips.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><strong><code>stop_times.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><strong><code>calendar.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><code>calendar_dates.txt</code></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><code>delays.txt</code></td>
              <td>Trips</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 1:</span> The <abbr title='General Transit Feed Specification'>GTFS</abbr> files that are written by <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, with their corresponding sub-generators that are responsible for generating the required data.
The files in bold refer to files that are required by the <abbr title='General Transit Feed Specification'>GTFS</abbr> specification.</p>
        </figcaption>
      </figure>

      <figure id="generating_listing:example:delays" class="listing">
<pre><code>trip_id,stop_sequence,delay_departure,delay_arrival,delay_departure_reason         ,delay_arrival_reason
</code><code>100_4  ,0            ,0              ,1405754      ,                               ,td:RepairWork
</code><code>100_6  ,0            ,0              ,1751671      ,                               ,td:BrokenDownTrain
</code><code>100_6  ,1            ,1751671        ,1553820      ,td:BrokenDownTrain             ,td:BrokenDownTrain
</code><code>100_7  ,0            ,2782295        ,0            ,td:TreeAndVegetationCuttingWork,</code></pre>
<figcaption>
          <p><span class="label">Listing 2:</span> Sample of a <code>delays.txt</code> file in a <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.</p>
        </figcaption>
</figure>

      <p>In order to easily observe the network structure in the generated datasets,
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> will always produce a figure accompanying the <abbr title='General Transit Feed Specification'>GTFS</abbr> dataset.
<a href="#generating_fig:generated_example">Fig. 18</a> shows an example of such a visualization.</p>

      <figure id="generating_fig:generated_example">
<img src="generating/img/generated_example.png" alt="Visualization of a generated public transport network based on Belgium's population distribution" />
<figcaption>
          <p><span class="label">Fig. 18:</span> Visualization of a generated public transport network based on Belgium’s population distribution.
Each route has a different color, and dark route colors indicate more frequent trips over them than light colors.
The population distribution is illustrated for each cell as a scale going from white (low), to red (medium) and black (high).
<a href="https://linkedsoftwaredependencies.org/raw/podigg/gen.png" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​podigg/​gen.png">Full image</a></p>
        </figcaption>
</figure>

      <p>Because the generation of large datasets can take a long time depending on the used parameters,
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> has a logging mechanism, which provides continuous feedback to the user about the current status and progress of the generator.</p>

      <p>Finally, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> provides the option to derive realistic public transit queries over the generated network,
aimed at testing the load of route planning systems.
This is done by iteratively selecting two random stops weighed by their size
and choosing a random starting time based on the same time distribution as discussed in <a href="#generating_subsec:methodology:trips"></a>.
This is serialized to a <a href="https://github.com/linkedconnections/benchmark-belgianrail#transit-schedules" class="mandatory" data-link-text="https:/​/​github.com/​linkedconnections/​benchmark-​belgianrail#transit-​schedules">JSON format</a>
that was introduced for benchmarking the Linked Connections route planner <span class="references">[<a href="#ref-11">11</a>]</span>.</p>

      <h4 id="podigg-lc"><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC</h4>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC is an extension of <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, that outputs data in Turtle/<abbr title='Resource Description Framework'>RDF</abbr> using the ontologies shown in <a href="#generating_fig:methodology:datamodel">Fig. 3</a>.
It is also implemented in JavaScript using Node.js, and available under an open license on <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg-lc&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg-​lc&#8221;>GitHub</a>.
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC is also available as a Node module on <a href="https://www.npmjs.com/package/podigg-lc" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg-​lc">NPM</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg-lc/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg-​lc/​">Docker Hub</a>.
For this, we extended the <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/gtfs2lc&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​gtfs2lc&#8221;><abbr title='General Transit Feed Specification'>GTFS</abbr>-LC tool</a> that is able
to convert <abbr title='General Transit Feed Specification'>GTFS</abbr> datasets to <abbr title='Resource Description Framework'>RDF</abbr> using the Linked Connections and <abbr title='General Transit Feed Specification'>GTFS</abbr> ontologies.
The original tool serializes a minimal subset of the <abbr title='General Transit Feed Specification'>GTFS</abbr> data, aimed at being used for Linked Connections route planning over connections.
Our extension also serializes trip, station and route instances, with their relevant interlinking.
Furthermore, our <abbr title='General Transit Feed Specification'>GTFS</abbr> extension for representing delays is also supported, and is serialized
using a new <a href="http://semweb.datasciencelab.be/ns/linked-connections-delay/" class="mandatory" data-link-text="http:/​/​semweb.datasciencelab.be/​ns/​linked-​connections-​delay/​">Linked Connections Delay ontology</a> that we created.</p>

      <h4 id="configuration">Configuration</h4>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> accepts a wide range of parameters that can be used to configure properties of the different sub-generators.
<a href="#generating_table:implementation:params">Table 2</a> shows an overview of the parameters, grouped by each sub-generator.
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC accept these <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg#parameters&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg#parameters&#8221;>parameters</a>
either in a JSON configuration file or via environment variables.
Both <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC produce deterministic output for identical sets of parameters,
so that datasets can easily be reproduced given the configuration.
The <em>seed</em> parameter can be used to introduce pseudo-randomness into the output.</p>

      <figure id="generating_table:implementation:params" class="table">
<table>
    <tr>
        <th></th>
        <th>Name</th>
        <th>Default Value</th>
        <th>Description</th>
    </tr>

    <tr class="row-sep-above">
        <td></td>
        <td><code>seed</code></td>
        <td><code>1</code></td>
        <td>The random seed</td>
    </tr>

    <tr class="row-sep-above">
        <td rowspan="4" class="rotate">Region</td>
        <td><code>region_generator</code></td>
        <td><code>isolated</code></td>
        <td>Name of a region generator. (isolated, noisy or region)</td>
    </tr>
    <tr>
        <td><code>lat_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated latitudes</td>
    </tr>
    <tr>
        <td><code>lon_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated longitudes</td>
    </tr>
    <tr>
        <td><code>cells_per_latlon</code></td>
        <td><code>100</code></td>
        <td>How many cells go in 1 latitude/longitude</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="9" class="rotate">Stops</td>
        <td><code>stops</code></td>
        <td><code>600</code></td>
        <td>How many stops should be generated</td>
    </tr>
    <tr>
        <td><code>min_station_size</code></td>
        <td><code>0.01</code></td>
        <td>Minimum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>max_station_size</code></td>
        <td><code>30</code></td>
        <td>Maximum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells as stops</td>
    </tr>
    <tr>
        <td><code>min_interstop_distance</code></td>
        <td><code>1</code></td>
        <td>Minimum distance between stops in number of cells</td>
    </tr>
    <tr>
        <td><code>factor_stops_post_edges</code></td>
        <td><code>0.66</code></td>
        <td>Factor of stops to generate after edges</td>
    </tr>
    <tr>
        <td><code>edge_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer edges to generate stops on</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells around edges</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_radius</code></td>
        <td><code>2</code></td>
        <td>Radius in number of cells around an edge to select points</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Edges</td>
        <td><code>max_intracluster_distance</code></td>
        <td><code>100</code></td>
        <td>Maximum distance between stops in one cluster</td>
    </tr>
    <tr>
        <td><code>max_intracluster_distance_growthfactor</code></td>
        <td><code>0.1</code></td>
        <td>Power for clustering with more distant stops</td>
    </tr>
    <tr>
        <td><code>post_cluster_max_intracluster_distancefactor</code></td>
        <td><code>1.5</code></td>
        <td>Power for connecting a stop with multiple stops</td>
    </tr>
    <tr>
        <td><code>loosestations_neighbourcount</code></td>
        <td><code>3</code></td>
        <td>Neighbours around a loose station that should define its area</td>
    </tr>
    <tr>
        <td><code>loosestations_max_range_factor</code></td>
        <td><code>0.3</code></td>
        <td>Maximum loose station range relative to the total region size</td>
    </tr>
    <tr>
        <td><code>loosestations_max_iterations</code></td>
        <td><code>10</code></td>
        <td>Maximum iteration number to try to connect one loose station</td>
    </tr>
    <tr>
        <td><code>loosestations_search_radius_factor</code></td>
        <td><code>0.5</code></td>
        <td>Loose station neighbourhood size factor</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="5" class="rotate">Routes</td>
        <td><code>routes</code></td>
        <td><code>1000</code></td>
        <td>The number of routes to generate</td>
    </tr>
    <tr>
        <td><code>largest_stations_fraction</code></td>
        <td><code>0.05</code></td>
        <td>The fraction of stops to form routes between</td>
    </tr>
    <tr>
        <td><code>penalize_station_size_area</code></td>
        <td><code>10</code></td>
        <td>The area in which stop sizes should be penalized</td>
    </tr>
    <tr>
        <td><code>max_route_length</code></td>
        <td><code>10</code></td>
        <td>Maximum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>min_route_length</code></td>
        <td><code>4</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="15" class="rotate">Connections</td>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>connections</code></td>
        <td><code>30000</code></td>
        <td>Number of connections to generate</td>
    </tr>
    <tr>
        <td><code>stop_wait_min</code></td>
        <td><code>60000</code></td>
        <td>Minimum waiting time per stop</td>
    </tr>
    <tr>
        <td><code>stop_wait_size_factor</code></td>
        <td><code>60000</code></td>
        <td>Waiting time to add multiplied by station size</td>
    </tr>
    <tr>
        <td><code>route_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer routes for connections</td>
    </tr>
    <tr>
        <td><code>vehicle_max_speed</code></td>
        <td><code>160</code></td>
        <td>Maximum speed of a vehicle in km/h</td>
    </tr>
    <tr>
        <td><code>vehicle_speedup</code></td>
        <td><code>1000</code></td>
        <td>Vehicle speedup in km/(h$^2$)</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekdays</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekend days</td>
    </tr>
    <tr>
        <td><code>delay_chance</code></td>
        <td><code>0</code></td>
        <td>Chance for a connection delay</td>
    </tr>
    <tr>
        <td><code>delay_max</code></td>
        <td><code>3600000</code></td>
        <td>Maximum delay</td>
    </tr>
    <tr>
        <td><code>delay_choice_power</code></td>
        <td><code>1</code></td>
        <td>Power for selecting larger delays</td>
    </tr>
    <tr>
        <td><code>delay_reasons</code></td>
        <td><code>...<sup>2</sup></code></td>
        <td>Default reasons and chances for delays</td>
    </tr>
    <tr>
        <td><code>delay_reduction_duration_fraction</code></td>
        <td><code>0.1</code></td>
        <td>Maximum part of connection duration to subtract for delays</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Queryset</td>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large starting stations</td>
    </tr>
    <tr>
        <td><code>query_count</code></td>
        <td><code>100</code></td>
        <td>The number of queries to generate</td>
    </tr>
    <tr>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp</td>
    </tr>
    <tr>
        <td><code>max_time_before_departure</code></td>
        <td><code>3600000</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekday</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekend day</td>
    </tr>
</table>
<figcaption>
          <p><span class="label">Table 2:</span> Configuration parameters for the different sub-generators. Time values are represented in milliseconds.
<sup>1</sup> Time distributions are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">public route planning logs</a> <span class="references">[<a href="#ref-27">27</a>]</span>.
<sup>2</sup> Default delays are based on the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>.</p>
        </figcaption>
</figure>

    </section>

    <section id="generating_evaluation">
      <h3>Evaluation</h3>

      <p>In this section, we discuss our evaluation of <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>.
We first evaluate the realism of the generated datasets using a constant seed by comparing its coherence to real datasets,
followed by a more detailed realism evaluation of each sub-generator using distance functions.
Finally, we provide an indicative efficiency and scalability evaluation of the generator and discuss practical dataset sizes.
All scripts that were used for the following evaluation can be found on <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg-evaluate&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg-​evaluate&#8221;>GitHub</a>.
Our experiments were executed on a 64-bit Ubuntu 14.04 machine with 128 GB of memory and a 24-core 2.40 GHz CPU.</p>

      <h4 id="generating_subsec:evaluation:coherence">Coherence</h4>

      <h5 id="metric">Metric</h5>
      <p>In order to determine how closely synthetic <abbr title='Resource Description Framework'>RDF</abbr> datasets resemble their real-world variants in terms of <em>structuredness</em>,
the coherence metric <span class="references">[<a href="#ref-10">10</a>]</span> can be used.
In <abbr title='Resource Description Framework'>RDF</abbr> dataset generation, the goal is to reach a level of structuredness similar to real datasets.
As mentioned before in <a href="#generating_related-work">Subsection 2.3</a>, many synthetic datasets have a level of structuredness that is higher than their real-world counterparts.
Therefore, our coherence evaluation should indicate that our generator is not subject to the same problem.
We have implemented a <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/graph-coherence&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​graph-​coherence&#8221;>command-line tool</a>
to calculate the coherence value for any given input dataset.</p>

      <h5 id="results">Results</h5>
      <p>When measuring the coherence of the Belgian railway, buses and Dutch railway datasets,
we discover high values for both the real-world datasets and the synthetic datasets, as can be seen in <a href="#generating_table:eval:coherence">Table 3</a>.
These nearly maximal values indicate that there is a very high level of structuredness in these real-world datasets.
Most instances have all the possible values, unlike most typical <abbr title='Resource Description Framework'>RDF</abbr> datasets,
which have values around or below 0.6 <span class="references">[<a href="#ref-10">10</a>]</span>.
That is because of the very specialized nature of this dataset, and the fact that they originate
from <abbr title='General Transit Feed Specification'>GTFS</abbr> datasets that have the characteristics of relational databases.
Only a very limited number of classes and predicates are used,
where almost all instances have the same set of attributes.
In fact, these very high coherence values for real-world datasets simplify the process of synthetic dataset generation,
as less attention needs to be given to factors that lead to lower levels of structuredness, such as optional attributes for instances.
When generating synthetic datasets using <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> with the same number of stops, routes and connections for the three gold standards,
we measure very similar coherence values, with differences ranging from 0.08% to 1.64%.
This confirms that <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is able to create datasets with the same high level of structuredness to real datasets of these types,
as it inherits the relational database characteristics from its <abbr title='General Transit Feed Specification'>GTFS</abbr>-centric mimicking algorithm.</p>

      <figure id="generating_table:eval:coherence" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: left"> </th>
              <th style="text-align: right">Belgian railway</th>
              <th style="text-align: right">Belgian buses</th>
              <th style="text-align: right">Dutch railway</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: left"><strong>Real</strong></td>
              <td style="text-align: right">0.9845</td>
              <td style="text-align: right">0.9969</td>
              <td style="text-align: right">0.9862</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Synthetic</strong></td>
              <td style="text-align: right">0.9879</td>
              <td style="text-align: right">0.9805</td>
              <td style="text-align: right">0.9870</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Difference</strong></td>
              <td style="text-align: right">0.0034</td>
              <td style="text-align: right">0.0164</td>
              <td style="text-align: right">0.0008</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 3:</span> Coherence values for three gold standards compared to the values for equivalent synthetic variants.</p>
        </figcaption>
      </figure>

      <h4 id="generating_subsec:evaluation:distance">Distance to Gold Standards</h4>

      <p>While the coherence metric is useful to compare the level of structuredness between datasets,
it does not give any detailed information about how <em>real</em> synthetic datasets are in terms of their <em>distance</em> to the real datasets.
In this case, we are working with public transit feeds with a known structure,
so we can look at the different datasets aspects in more detail.
More specifically, we start from real geographical areas with their population distributions,
and consider the distance functions between stops, edges, routes and trips for the synthetic and gold standard datasets.
In order to check the applicability of <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> to different transport types and geographical areas,
we compare with the gold standard data of the Belgian railway, the Belgian buses and the Dutch railway.
The scripts that were used to derive these gold standards from real-world data
can be found on <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/population-density-generator&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​population-​density-​generator&#8221;>GitHub</a>.</p>

      <p>In order to construct distance functions for the different generator elements, we consider several helper functions.
The function in <a href="#generating_math:eval:closest">Equation 4</a> is used to determine the closest element
in a set of elements <span class="kdmath">$B$</span> to a given element <span class="kdmath">$a$</span>, given a distance function <span class="kdmath">$f$</span>.
The function in <a href="#generating_math:eval:distance">Equation 5</a> calculates the distance between all elements in <span class="kdmath">$A$</span> and all elements in <span class="kdmath">$B$</span>,
given a distance function <span class="kdmath">$f$</span>.
The computational complexity of <span class="kdmath">$\chi$</span> is <span class="kdmath">$O(\|B\| \cdot \kappa(f))$</span>,
where <span class="kdmath">$\kappa(f)$</span> is the cost for one distance calculation for <span class="kdmath">$f$</span>.
The complexity of <span class="kdmath">$\Delta$</span> then becomes <span class="kdmath">$O(\|A\| \cdot \|B\| \cdot \kappa(f))$</span>.</p>

      <figure id="generating_math:eval:closest" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\chi(a, B, f) \coloneqq \text{arg min}_{b \in B} f(a, b)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 4:</span> Function to determine the closest element in a set of elements.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta(A, B, f) \coloneqq &#92;
\dfrac{
      \sum\limits_{a \in A}{f(a, \chi(a, B, f))}
    + \sum\limits_{b \in B}{f(b, \chi(b, A, f))}
    }{\|A\| + \|B\|}
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 5:</span> Function to calculate the distance between all elements in a set of elements.</p>
        </figcaption>
      </figure>

      <h5 id="stops-distance">Stops Distance</h5>
      <p>For measuring the distance between two sets of stops <span class="kdmath">$S_1$</span> and <span class="kdmath">$S_2$</span>,
we introduce the distance function from <a href="#generating_math:stops:distance">Equation 6</a>.
This measures the distance between every possible pair of stops using the Euclidian distance function <span class="kdmath">$d$</span>.
Assuming a constant execution time for <span class="kdmath">$\kappa(d)$</span>,
the computational complexity for <span class="kdmath">$\Delta_\text{s}$</span> is <span class="kdmath">$O(\|S_1\| \cdot \|S_2\|)$</span>.</p>

      <figure id="generating_math:stops:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    \Delta_\text{s}(S_1, S_2) \coloneqq \Delta(S_1, S_2, d)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 6:</span> Function to calculate the distance between two stops.</p>
        </figcaption>
      </figure>

      <h5 id="edges-distance">Edges Distance</h5>
      <p>In order to measure the distance between two sets of edges <span class="kdmath">$E_1$</span> and <span class="kdmath">$E_2$</span>,
we use the distance function from <a href="#generating_math:eval:edges:distance">Equation 7</a>,
which measures the distance between all pairs of edges using the distance function $d_\text{e}$.
This distance function <span class="kdmath">$d_\text{e}$</span>, which is introduced in <a href="#generating_math:eval:edge:distance">Equation 8</a>,
measures the Euclidian distance between the start and endpoints of each edge, and between the different edges,
weighed by the length of the edges.
The constant <span class="kdmath">$1$</span> in <a href="#generating_math:eval:edge:distance">Equation 8</a> is to ensure that the distance between two edges that have an equal length,
but exist at a different position, is not necessarily zero.
The computational cost of <span class="kdmath">$d_\text{e}$</span> can be considered as a constant,
so the complexity of <span class="kdmath">$\Delta_\text{e}$</span> becomes <span class="kdmath">$O(\|E_1\| \cdot \|E_2\|)$</span>.</p>

      <figure id="generating_math:eval:edges:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{e}(E_1, E_2) \coloneqq \Delta(E_1, E_2, d_\text{e})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 7:</span> Function to calculate the distance between two sets of edges.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:edge:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    d_\text{e}(e_1, e_2) \coloneqq & \text{min}\big(
          d(e_1^\text{from}, e_2^\text{from})
        + d(e_1^\text{to}, e_2^\text{to}), &#92;
    &\quad\quad
          d(e_1^\text{from}, e_2^\text{to})
        + d(e_1^\text{to}, e_2^\text{from})\big) &#92;
    &\cdot (d(e_1^\text{from}, e_1^\text{to}) - d(e_2^\text{from}, e_2^\text{to}) + 1)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 8:</span> Function to calculate the distance of an edge.</p>
        </figcaption>
      </figure>

      <h5 id="routes-distance">Routes Distance</h5>
      <p>Similarly, the distance between two sets of routes <span class="kdmath">$R_1$</span> and <span class="kdmath">$R_2$</span> is measured in <a href="#generating_math:eval:routes:distance">Equation 9</a>
by applying <span class="kdmath">$\Delta$</span> for the distance function <span class="kdmath">$d_\text{r}$</span>.
<a href="#generating_math:eval:route:distance">Equation 10</a> introduces this distance function <span class="kdmath">$d_\text{r}$</span> between two routes,
which is calculated by considering the edges in each route and measuring the distance
between those two sets using the distance function <span class="kdmath">$\Delta_\text{e}$</span> from <a href="#generating_math:eval:edges:distance">Equation 7</a>.
By considering the maximum amount of edges per route as <span class="kdmath">$e_\text{max}$</span>,
the complexity of <span class="kdmath">$d_\text{r}$</span> becomes <span class="kdmath">$O(e_\text{max}^2)$</span>
This leads to a complexity of <span class="kdmath">$O(\|R_1\| \cdot \|R_2\| \cdot e_\text{max}^2)$</span> for <span class="kdmath">$\Delta_\text{r}$</span>.</p>

      <figure id="generating_math:eval:routes:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{r}(R_1, R_2) \coloneqq \Delta(R_1, R_2, d_\text{r})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 9:</span> Function to calculate the distance between two sets of routes.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:route:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
d_\text{r}(r_1, r_2) \coloneqq \Delta_\text{e}(r_1^\text{edges}, r_2^\text{edges})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 10:</span> Function to calculate the distance between two routes.</p>
        </figcaption>
      </figure>

      <h5 id="connections-distance">Connections Distance</h5>
      <p>Finally, we measure the distance between two sets of connections <span class="kdmath">$C_1$</span> and <span class="kdmath">$C_2$</span>
using the function from <a href="#generating_math:eval:connections:distance">Equation 11</a>.
The distance between two connections is measured using the function from <a href="#generating_math:eval:connection:distance">Equation 12</a>,
which is done by considering their respective temporal distance weighed by a constant <span class="kdmath">$d_\epsilon$</span> –when serializing time in milliseconds, we set <span class="kdmath">$d_\epsilon$</span> to <span class="kdmath">$60000$</span>.–,
and their geospatial distance using the edge distance function <span class="kdmath">$d_\text{e}$</span>.
The complexity of time calculation in <span class="kdmath">$d_\text{c}$</span> can be considered being constant,
which makes it overall complexity <span class="kdmath">$O(e_\text{max}^2)$</span>.
For <span class="kdmath">$\Delta_\text{c}$</span>, this leads to a complexity of <span class="kdmath">$O(\|C_1\| \cdot \|C_2\| \cdot e_\text{max}^2)$</span>.</p>

      <figure id="generating_math:eval:connections:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{c}(C_1, C_2) \coloneqq \Delta(C_1, C_2, d_\text{c})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 11:</span> Function to calculate the distance between two sets of connections.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:connection:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    d_\text{c}(c_1, c_2) &\coloneqq ((c_1^\text{departureTime} - c_2^\text{departureTime}) &#92;
    &+ (c_1^\text{arrivalTime} - c_2^\text{arrivalTime}) / d_\epsilon) &#92;
    &+ d_\text{e}(c_1, c_2)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 12:</span> Function to calculate the distance between two connections.</p>
        </figcaption>
      </figure>

      <h5 id="computability">Computability</h5>
      <p>When using the introduced functions for calculating the distance between stops, edges, routes or connections,
execution times can become long for a large number of elements because of their large complexity.
When applying these distance functions for realistic numbers of stops, edges, routes and connections,
several optimizations should be done in order to calculate these distances in a reasonable time.
A major contributor for these high complexities is <span class="kdmath">$\chi$</span> for finding the closest element from a set of elements to a given element,
as introduced in <a href="#generating_math:eval:closest">Equation 4</a>.
In practice, we only observed extreme execution times for the respective distance between routes and connections.
For routes, we implemented an optimization, with the same worst-case complexity,
that indexes routes based on their geospatial position, and performs radial search around each route
when the closest one from a set of other routes should be found.
For connections, we consider the linear time dimension when performing binary search for finding the closest connection from a set of elements.</p>

      <h5 id="metrics">Metrics</h5>
      <p>In order to measure the realism of each generator phase, we introduce a <em>realism</em> factor <span class="kdmath">$\rho$</span> for each phase.
These values are calculated by measuring the distance from randomly generated elements to the gold standard,
divided by the distance from the actually generated elements to the gold standard,
as shown below for respectively stops, edges, routes and connections.
We consider these randomly generated elements having the lowest possible level of realism,
so we use these as a weighting factor in our realism values.</p>

      <div class="kdmath">$$
\begin{aligned}
    \rho_\text{s}(S_\text{rand}, S_\text{gen}, S_\text{gs})
    &\coloneqq \Delta_\text{s}(S_\text{rand}, S_\text{gs}) / \Delta_\text{s}(S_\text{gen}, S_\text{gs})&#92;
    \rho_\text{e}(E_\text{rand}, E_\text{gen}, E_\text{gs})
    &\coloneqq \Delta_\text{e}(E_\text{rand}, E_\text{gs}) / \Delta_\text{e}(E_\text{gen}, E_\text{gs})&#92;
    \rho_\text{r}(R_\text{rand}, R_\text{gen}, R_\text{gs})
    &\coloneqq \Delta_\text{r}(R_\text{rand}, R_\text{gs}) / \Delta_\text{r}(R_\text{gen}, R_\text{gs})&#92;
    \rho_\text{c}(C_\text{rand}, C_\text{gen}, C_\text{gs})
    &\coloneqq \Delta_\text{c}(C_\text{rand}, C_\text{gs}) / \Delta_\text{c}(C_\text{gen}, C_\text{gs})
\end{aligned}
$$</div>

      <h5 id="results-1">Results</h5>
      <p>We measured these realism values with gold standards for the Belgian railway, the Belgian buses and the Dutch railway.
In each case, we used an optimal set of <a href="https://github.com/<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>/podigg-evaluate/blob/master/bin/evaluate.js&#8221; class=&#8221;mandatory&#8221; data-link-text=&#8221;https:/​/​github.com/​<abbr title='POpulation DIstribution-​based Gtfs Generator'>PoDiGG</​abbr>/​podigg-​evaluate/​blob/​master/​bin/​evaluate.js&#8221;>parameters</a>
to achieve the most realistic generated output.
<a href="#generating_table:eval:realism">Table 4</a> shows the realism values for the three cases,
which are visualized in <a href="#generating_fig:realism:stops">Fig. 19</a>, <a href="#generating_fig:realism:edges">Fig. 23</a>, <a href="#generating_fig:realism:routes">Fig. 27</a> and <a href="#generating_fig:realism:connections">Fig. 31</a>.
Each value is larger than 1, showing that the generator at least produces data that is closer to the gold standard,
and is therefore more realistic.
The realism for edges is in each case very large, showing that our algorithm produces edges that are very similar to
actual the edge placement in public transport networks according to our distance function.
Next, the realism of stops is lower, but still sufficiently high to consider it as realistic.
Finally, the values for routes and connections show that these sub-generators produce output that is closer
to the gold standard than the random function according to our distance function.
Routes achieve the best level of realism for the Belgian railway case.
For this same case, the connections are however only slightly closer to the gold standard than random placement,
while for the other cases the realism is more significant.
All of these realism values show that <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is able to produce realistic data for different regions and different transport types.</p>

      <figure id="generating_table:eval:realism" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: left"> </th>
              <th style="text-align: right">Belgian railway</th>
              <th style="text-align: right">Belgian buses</th>
              <th style="text-align: right">Dutch railway</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: left"><strong>Stops</strong></td>
              <td style="text-align: right">5.5490</td>
              <td style="text-align: right">297.0888</td>
              <td style="text-align: right">4.0017</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Edges</strong></td>
              <td style="text-align: right">147.4209</td>
              <td style="text-align: right">1633.4693</td>
              <td style="text-align: right">318.4131</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Routes</strong></td>
              <td style="text-align: right">2.2420</td>
              <td style="text-align: right">0.0164</td>
              <td style="text-align: right">1.3095</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Connections</strong></td>
              <td style="text-align: right">1.0451</td>
              <td style="text-align: right">1.5006</td>
              <td style="text-align: right">1.3017</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 4:</span> Realism values for the three gold standards in case of the different sub-generators,
respectively calculated for the stops <span class="kdmath">$\rho_\text{s}$</span>, edges <span class="kdmath">$\rho_\text{e}$</span>, routes <span class="kdmath">$\rho_\text{r}$</span> and connections <span class="kdmath">$\rho_\text{c}$</span>.</p>
        </figcaption>
      </figure>

      <figure id="generating_fig:realism:stops">

<figure id="generating_fig:realism:stops:rand" class="subfigure">
<img src="generating/img/realism/train_be/stops_random.png" alt="Stops Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 19:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gen" class="subfigure">
<img src="generating/img/realism/train_be/stops_parameterized.png" alt="Stops Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 21:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gs" class="subfigure">
<img src="generating/img/realism/train_be/stops_gs.png" alt="Stops Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 22:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Stops for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:edges">

<figure id="generating_fig:realism:edges:rand" class="subfigure">
<img src="generating/img/realism/train_be/edges_random.png" alt="Edges Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 23:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gen" class="subfigure">
<img src="generating/img/realism/train_be/edges_parameterized.png" alt="Edges Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 25:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gs" class="subfigure">
<img src="generating/img/realism/train_be/edges_gs.png" alt="Edges Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 26:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Edges for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:routes">

<figure id="generating_fig:realism:routes:rand" class="subfigure">
<img src="generating/img/realism/train_be/routes_random.png" alt="Routes Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 27:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gen" class="subfigure">
<img src="generating/img/realism/train_be/routes_parameterized.png" alt="Routes Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 29:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gs" class="subfigure">
<img src="generating/img/realism/train_be/routes_gs.png" alt="Routes Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 30:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Routes for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:connections">
<img src="generating/img/realism/train_be/connections_distr.svg" alt="Hourly distribution" />
<figcaption>
          <p><span class="label">Fig. 31:</span> Connections per hour for the Belgian railway case.</p>
        </figcaption>
</figure>

      <h4 id="generating_subsec:evaluation:performance">Performance</h4>

      <h5 id="metrics-1">Metrics</h5>
      <p>While performance is not the main focus of this work,
we provide an indicative performance evaluation in this section in order to discover the bottlenecks and limitations
of our current implementation that could be further investigated and resolved in future work.
We measure the impact of different parameters on the execution times of the generator.
The three main parameters for increasing the output dataset size are the number of stops, routes and connections.
Because the number of edges is implicitly derived from the number of stops in order to reach a connected network,
this can not be configured directly.
In this section, we start from a set of parameters that produces realistic output data that is similar to the Belgian railway case.
We let the value for each of these parameters increase to see the evolution of the execution times and memory usage.
\end{paragraph}</p>

      <h5 id="results-2">Results</h5>
      <p><a href="#generating_fig:performance:times">Fig. 32</a> shows a linear increase in execution times when increasing the routes or connections.
The execution times for stops do however increase much faster, which is caused by the higher complexity of networks that are formed for many stops.
The used algorithms for producing this network graph proves to be the main bottleneck when generating large networks.
Networks with a limited size can however be generated quickly, for any number of routes and connections.
The memory usage results from <a href="#generating_fig:performance:mem">Fig. 33</a> also show a linear increase,
but now the increase for routes and connections is higher than for the stops parameter.
These figures show that stops generation is a more CPU intensive process than routes and connections generation.
These last two are able to make better usage of the available memory for speeding up the process.</p>

      <figure id="generating_fig:performance:times">
<img src="generating/img/performance/times.svg" alt="Execution times" />
<figcaption>
          <p><span class="label">Fig. 32:</span> Execution times when increasing the number of stops, routes or connections.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:performance:mem">
<img src="generating/img/performance/mem.svg" alt="Memory usage" />
<figcaption>
          <p><span class="label">Fig. 33:</span> Memory usage when increasing the number of stops, routes or connections.</p>
        </figcaption>
</figure>

      <h4 id="dataset-size">Dataset size</h4>

      <p>An important aspect of dataset generation is its ability to output various dataset sizes.
In <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, different options are available for tweaking these sizes.
Increasing the time range parameter within the generator increases the number of connections
while the number of stops and routes will remain the same.
When enlarging the geographical area over the same period of time, the opposite is true.
As a rule of thumb, based on the number of triples per connection, stops and routes,
the total number of generated triples per dataset is approximately <span class="kdmath">$7 \cdot \textit{\#connections} + 6 \cdot \textit{\#stops} + \textit{\#routes}$</span>.
For the Belgian railway case, containing 30,011 connections over a period of 9 months,
with 583 stops and 362 routes, this would theoretically result in 213,937 triples.
In practice, we reach 235,700 triples when running with these parameters, which is slightly higher because of the other triples that are not
taken into account for this simplified formula, such as the ones for trips, stations and delays.</p>

    </section>

    <section id="generating_discussion">
      <h3>Discussion</h3>

      <p>In this section, we discuss the main characteristics, the usage within benchmarks and the limitations of this work.
Finally, we mention several <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> use cases.</p>

      <h4 id="characteristics">Characteristics</h4>

      <p>Our main research question on how to generate realistic synthetic public transport networks
has been answered by the introduction of the mimicking algorithm from <a href="#generating_methodology">Subsection 2.6</a>,
based on commonly used practises in transit network design.
This is based on the accepted hypothesis that the population distribution
of an area is correlated with its transport network design and scheduling.
We measured the realism of the generated datasets using the coherence metric in <a href="#generating_subsec:evaluation:coherence">Subsubsection 2.8.1</a>
and more fine-grained realism metrics for different public transport aspects in <a href="#generating_subsec:evaluation:distance">Subsubsection 2.8.2</a>.</p>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, our implementation of the algorithm, accepts a wide range of parameters to configure the mimicking algorithm.
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC are able to output the mimicked data respectively as <abbr title='General Transit Feed Specification'>GTFS</abbr> and <abbr title='Resource Description Framework'>RDF</abbr> datasets,
together with a visualization of the generated transit network.
Our system can be used without requiring any extensive setup or advanced programming skills,
as it consists of simple command lines tools that can be invoked with a number of optional parameters to configure the generator.
Our system is proven to be generalizable to other transport types,
as we evaluated <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> for the bus and train transport type,
and the Belgium and Netherlands geospatial regions in <a href="#generating_subsec:evaluation:distance">Subsubsection 2.8.2</a>.</p>

      <h4 id="usage-within-benchmarks">Usage within Benchmarks</h4>

      <p>A~synthetic dataset generator,
which is one of the main contributions of this work,
forms an essential aspect of benchmarks for (<abbr title='Resource Description Framework'>RDF</abbr>) data management systems <span class="references">[<a href="#ref-23">23</a>, <a href="#ref-28">28</a>]</span>.
Prescribing a concrete benchmark that includes the evaluation of tasks is out of scope.
However,
to provide a guideline on how our dataset generator can be used as part of a benchmark,
we relate the primary elements of public transport datasets to <em>choke points</em> in data management systems,
i.e., key technical challenges in these system.
Below, we list choke points related to <em>storage</em> and <em>querying</em> within data management systems and route planning systems.
For each choke point, we introduce example tasks to evaluate them in the context of public transport datasets.
The querying choke points are inspired by the choke points identified by <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">Petzka et. al. for faceted browsing</a> <span class="references">[<a href="#ref-29">29</a>]</span>.</p>

      <ol>
        <li>Storage of entities.
          <ol>
            <li>Storage of stops, stations, connections, routes, trips and delays.</li>
          </ol>
        </li>
        <li>Storage of links between entities.
          <ol>
            <li>Storage of stops per station.</li>
            <li>Storage of connections for stops.</li>
            <li>Storage of the next connection for each connection.</li>
            <li>Storage of connections per trip.</li>
            <li>Storage of trips per route.</li>
            <li>Storage of a connection per delay.</li>
          </ol>
        </li>
        <li>Storage of literals.
          <ol>
            <li>Storage of latitude, longitude, platform code and code of stops.</li>
            <li>Storage of latitude, longitude and label of stations.</li>
            <li>Storage of delay durations.</li>
            <li>Storage of the start and end time of connections.</li>
          </ol>
        </li>
        <li>Storage of sequences.
          <ol>
            <li>Storage of sequences of connections.</li>
          </ol>
        </li>
        <li>Find instances by property value.
          <ol>
            <li>Find latitude, longitude, platform code or code by stop.</li>
            <li>Find station by stop.</li>
            <li>Find country by station.</li>
            <li>Find latitude, longitude, or label by station.</li>
            <li>Find delay by connection.</li>
            <li>Find next connection by connection.</li>
            <li>Find trip by connection.</li>
            <li>Find route by connection.</li>
            <li>Find route by trip.</li>
          </ol>
        </li>
        <li>Find instances by inverse property value.
          <ol>
            <li><em>Inverse of examples above.</em></li>
          </ol>
        </li>
        <li>Find instances by a combination of properties values.
          <ol>
            <li>Find stops by geospatial location.</li>
            <li>Find stations by geospatial location.</li>
          </ol>
        </li>
        <li>Find instances for a certain property path with a certain value.
          <ol>
            <li>Find the delay value of the connection after a given connection.</li>
            <li>Find the delay values of all connections after a given connection.</li>
          </ol>
        </li>
        <li>Find instances by inverse property path with a certain value.
          <ol>
            <li>Find stops that are part of a certain trip that passes by the stop at the given geospatial location.</li>
          </ol>
        </li>
        <li>Find instances by class, including subclasses.
          <ol>
            <li>Find delays of a certain class.</li>
          </ol>
        </li>
        <li>Find instances with a numerical value within a certain interval.
          <ol>
            <li>Find stops by latitude or longitude range.</li>
            <li>Find stations by latitude or longitude range.</li>
            <li>Find delays with durations within a certain range.</li>
          </ol>
        </li>
        <li>Find instances with a combination of numerical values within a certain interval.
          <ol>
            <li>Find stops by geospatial range.</li>
            <li>Find stations by geospatial range.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value for a certain property path.
          <ol>
            <li>Find connections that pass by stops in a given geospatial range.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value.
          <ol>
            <li>Find connections that occur at a certain time.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value for a certain property path.
          <ol>
            <li>Find trips that occur at a certain time.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain interval.
          <ol>
            <li>Find connections that occur during a certain time interval.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain interval for a certain property path.
          <ol>
            <li>Find trips that occur during a certain time interval.</li>
          </ol>
        </li>
        <li>Find instances with numerical intervals by intervals with property paths.
          <ol>
            <li>Find connections that occur during a certain time interval with stations that have stops in a given geospatial range.</li>
            <li>Find trips that occur during a certain time interval with stops in a given geospatial range.</li>
            <li>Plan a route that gets me from stop A to stop B starting at a certain time.</li>
          </ol>
        </li>
      </ol>

      <p>This list of choke points and tasks can be used
as a basis for benchmarking spatiotemporal data management systems 
using public transport datasets.
For example, <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries can be developed based on these tasks
and executed by systems using a public transport dataset.
For the benchmarking with these tasks, it is essential that the used datasets are realistic,
as discussed in <a href="#generating_subsec:evaluation:distance">Subsubsection 2.8.2</a>.
Otherwise, certain choke points may not resemble the real world.
For example, if an unrealistic dataset would contain only a single trip that goes over all stops,
then finding a route between two given stops could be unrealistically simple.</p>

      <h4 id="limitations-and-future-work">Limitations and Future Work</h4>

      <p>In this section, we discuss the limitations of the current mimicking algorithm and its implementation,
together with further research opportunities.</p>

      <h5 id="memory-usage">Memory Usage</h5>
      <p>The sequential steps in the presented mimicking algorithm require persistence of the intermediary data that is generated in each step.
Currently, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is implemented in such a way that all data is kept in-memory for the duration of the generation, until it is serialized.
When large datasets need to be generated, this requires a larger amount of memory to be allocated to the generator.
Especially for large amounts of routes or connections, where 100 million connections already require almost 10GB of memory to be allocated.
While performance was not the primary concern in this work, in future work, improvements could be made in the future.
A first possible solution would be to use a memory-mapped database for intermediary data,
so that not all data must remain in memory at all times.
An alternative solution would be to modify the mimicking process to a streaming algorithm,
so that only small parts of data need to be kept in memory for datasets of any size.
Considering the complexity of transit networks, a pure streaming algorithm might not be feasible,
because route design requires knowledge of the whole network.
The generation of connections however, could be adapted so that it works as a streaming algorithm.</p>

      <h5 id="realism">Realism</h5>
      <p>We aimed to produce realistic transit feeds by reusing the methodologies learned in public transit planning.
Our current evaluation compares generated output to real datasets, as no similar generators currently exist.
When similar generation algorithms are introduced in the future, this evaluation can be extended to compare their levels of realism.
Our results showed that all sub-generators, except for the trips generator, produced output with a high realism value.
The trips are still closer to real data than a random generator, but this can be further improved in future work.
This can be done by for instance taking into account <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">network capacities</a> <span class="references">[<a href="#ref-19">19</a>]</span>
on certain edges when instantiating routes as trips,
because we currently assume infinite edge capacities, which can result in a large amount of connections over an edge at the same time,
which may not be realistic for certain networks.
Alternatively, we could include other factors in the generation algorithm,
such as the location of certain points of interest, such as shopping areas, schools and tourist spots.
In the future, a study could be done to identify and measure the impact of certain points of interest on transit networks,
which could be used as additional input to the generation algorithm to further improve the level of realism.
Next to this, in order to improve <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transfer coordination</a> <span class="references">[<a href="#ref-19">19</a>]</span>,
possible transfers between trips should be taken into account when generating stop times.
Limiting the network capacity will also lead to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">natural congestion of networks</a> <span class="references">[<a href="#ref-21">21</a>]</span>,
which should also be taken into account for improving the realism.
Furthermore, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">total vehicle fleet size</a> <span class="references">[<a href="#ref-19">19</a>]</span> should be considered,
because we currently assume an infinite number of available vehicles.
It is more realistic to have a limited availability of vehicles in a network,
with the last position of each vehicle being of importance when choosing the next trip for that vehicle.</p>

      <h5 id="alternative-implementations">Alternative Implementations</h5>
      <p>An alternative way of implementing this generator would be to define declarative dependency rules for public transport networks,
based on the work by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">Pengyue et. al.</a> <span class="references">[<a href="#ref-22">22</a>]</span>. This would require a semantic extension to the engine
so that is aware of the relevant ontologies and that it can serialize to one or more <abbr title='Resource Description Framework'>RDF</abbr> formats.
Alternatively, machine learning techniques could be used
to automatically learn the structure and characteristics of real datasets
and create <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/MIC.2008.55">similar realistic synthetic datasets</a> <span class="references">[<a href="#ref-30">30</a>]</span>,
or to <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.08764">create variants of existing datasets</a> <span class="references">[<a href="#ref-31">31</a>]</span>.
The downside of machine learning techniques is however that it is typically more difficult to tweak parameters of automatically learned models
when specific characteristics of the output need to be changed, when compared to a manually implemented algorithm.
Sensitivity analysis could help to determine the impact of such parameters in order to understand the learned models better.</p>

      <h5 id="streaming-extension">Streaming Extension</h5>
      <p>Finally, the temporal aspect of public transport networks is useful for the domain of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf"><abbr title='Resource Description Framework'>RDF</abbr> stream processing</a> <span class="references">[<a href="#ref-32">32</a>]</span>.
Instead of producing single static datasets as output, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> could be adapted to produce <abbr title='Resource Description Framework'>RDF</abbr> streams of connections and delays,
where information about stops and routes are part of the background knowledge.
Such an extension can become part of a benchmark, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf">CityBench</a> <span class="references">[<a href="#ref-33">33</a>]</span> and LSBench <span class="references">[<a href="#ref-18">18</a>]</span>,
for assessing the performance of <abbr title='Resource Description Framework'>RDF</abbr> stream processing systems with temporal and geospatial capabilities.</p>

      <h4 id="podigg-in-use"><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> In Use</h4>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC have been developed for usage within the \hobbit platform.
This platform is being developed within the <abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> project and aims to provide
an environment for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> systems for Big Linked Data.
The platform provides several default dataset generators, including <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>,
which can be used to benchmark systems.</p>

      <p><abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, and its generated datasets are being used in the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf">ESWC Mighty Storage Challenge 2017 and 2018</a> <span class="references">[<a href="#ref-34">34</a>]</span>.
The first task of this challenge consists of <abbr title='Resource Description Framework'>RDF</abbr> data ingestion into triple stores,
and querying over this data.
Because of the temporal aspect of public transport data in the form of connections,
<abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> datasets are fragmented by connection departure time, and transformed to a data stream that can be inserted.
In task 4 of this challenge, the efficiency of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">faceted browsing solutions is benchmarked</a> <span class="references">[<a href="#ref-29">29</a>]</span>.
In this work, a list of choke points are identified regarding <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries on triple stores,
which includes points such as the selection of subclasses and property-path transitions.
Because of the geographical property of public transport data, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> datasets are being used for this benchmark.</p>

      <p>Finally, <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> is being used for creating virtual transit networks of variable size
for the purposes of benchmarking route planning frameworks, such as Linked Connections <span class="references">[<a href="#ref-11">11</a>]</span>.</p>

    </section>

    <section id="generating_conclusions">
      <h3>Conclusions</h3>

      <p>In this article, we introduced a mimicking algorithm for public transport data,
based on steps that are used in real-world transit planning.
Our method splits this process into several sub-generators and uses population distributions of an area as input.
As part of this article, we introduced <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>, a reusable framework that accepts a wide range of parameters to configure the generation algorithm.</p>

      <p>Results show that the structuredness of generated datasets are similar to real public transport datasets.
Furthermore, we introduced several functions for measuring the realism of
synthetic public transport datasets compared to a gold standard on several levels, based on distance functions.
The realism was confirmed for different regions and transport types.
Finally, the execution times and memory usages were measured when increasing the most important parameters,
which showed a linear increase for each parameter, showing that the generator is able to scale to large dataset outputs.</p>

      <p>The public transport mimicking algorithm we introduced, with <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> and <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr>-LC as implementations,
is essential for properly benchmarking the efficiency and performance
of public transport route planning systems under a wide range of realistic, but synthetic circumstances.
Flexible configuration allows datasets of any size to be created
and various characteristics to be tweaked to achieve highly specialized datasets for testing specific use cases.
In general, our dataset generator can be used for the benchmarking of geospatial and temporal <abbr title='Resource Description Framework'>RDF</abbr> data management systems,
and therefore lowers the barrier towards more efficient and performant systems.</p>
    </section>

    <div class="subfooter">
  <section id="generating_acknowledgements">
        <h3 class="no-label-increment">Acknowledgements</h3>

        <p>We wish to thank Henning Petzka for his help with discovering issues and providing useful suggestions for the <abbr title='POpulation DIstribution-based Gtfs Generator'>PoDiGG</abbr> implementation.
The described research activities were funded by the H2020 project <abbr title='Holistic Benchmarking of Big Linked Data'>HOBBIT</abbr> (#688227).</p>

      </section>

</div>
  </section>
  
  <section class="sub-paper">
    <h2 id="storing">Storing Evolving Data</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/miel_vander_sande">Miel Vander Sande</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/joachim_van_herwegen">Joachim Van Herwegen</a></li>
      <li><a href="https://www.ugent.be/ea/idlab/en/members/erik-mannens.htm" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/erik_mannens">Erik Mannens</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://rdfostrich.github.io/article-jws2018-ostrich/">Triple Storage for Random-Access Versioned Querying of <abbr title='Resource Description Framework'>RDF</abbr> Archives</a></p>

    <section id="storing_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <!-- Context      -->
      <p>When publishing Linked Open Datasets on the Web,
most attention is typically directed to their latest version.
Nevertheless, useful information is present in or between previous versions.
<!-- Need         -->
In order to exploit this historical information in dataset analysis,
we can maintain history in <abbr title='Resource Description Framework'>RDF</abbr> archives.
Existing approaches either require much storage space,
or they expose an insufficiently expressive or efficient interface
with respect to querying demands.
<!-- Task         -->
In this article, we introduce an <abbr title='Resource Description Framework'>RDF</abbr> archive indexing technique that is able to store datasets
with a low storage overhead,
by compressing consecutive versions and adding metadata for reducing lookup times.
<!-- Object       -->
We introduce algorithms based on this technique for efficiently evaluating
queries <em>at</em> a certain version, <em>between</em> any two versions, and <em>for</em> versions.
Using the <abbr title='Benchmark of rdf Archives'>BEAR</abbr> <abbr title='Resource Description Framework'>RDF</abbr> archiving benchmark,
we evaluate our implementation, called <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
<!-- Findings     -->
Results show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> introduces a new trade-off regarding storage space, ingestion time, and querying efficiency.
By processing and storing more metadata during ingestion time,
it significantly lowers the average lookup time for versioning queries.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> performs better for many smaller dataset versions
than for few larger dataset versions.
Furthermore, it enables efficient offsets in query result streams,
which facilitates random access in results.
<!-- Conclusion   -->
Our storage technique reduces query evaluation time for versioned queries
through a preprocessing step during ingestion,
which only in some cases increases storage space when compared to other approaches.
This allows data owners to store and query multiple versions of their dataset efficiently,
<!-- Perspectives -->
lowering the barrier to historical dataset publication and analysis.</p>

    </section>

    <section id="storing_introduction">
      <h3>Introduction</h3>

      <p>In the area of data analysis,
there is an ongoing need for maintaining the history of datasets.
Such archives can be used for looking up data at certain points in time,
for requesting evolving changes,
or for checking the temporal validity of these data <span class="references">[<a href="#ref-35">35</a>]</span>.
With the continuously increasing number of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Open Datasets</a> <span class="references">[<a href="#ref-5">5</a>]</span>,
archiving has become an issue for <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><abbr title='Resource Description Framework'>RDF</abbr></a> <span class="references">[<a href="#ref-2">2</a>]</span> data as well.
While the <abbr title='Resource Description Framework'>RDF</abbr> data model itself is atemporal, Linked Datasets typically change over time <span class="references">[<a href="#ref-36">36</a>]</span> on
dataset, schema, and/or instance level <span class="references">[<a href="#ref-37">37</a>]</span>.
Such changes can include additions,
modifications, or deletions of complete datasets, ontologies, and separate facts.
While some evolving datasets, such as DBpedia <span class="references">[<a href="#ref-38">38</a>]</span>,
are published as separate dumps per version,
more direct and efficient access to prior versions is desired.</p>

      <p>Consequently,
<abbr title='Resource Description Framework'>RDF</abbr> archiving systems emerged that, for instance, support query engines that use the standard <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>.
In 2015, however, <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">a survey on archiving Linked Open Data</a> <span class="references">[<a href="#ref-35">35</a>]</span> illustrated the need for improved versioning capabilities,
as current approaches have scalability issues at Web-scale.
They either perform well for versioned query evaluation—at the cost of large storage space requirements—or
require less storage space—at the cost of slower query evaluation.
Furthermore, no existing solution performs well for all versioned query types, namely querying <em>at</em>, <em>between</em>, and <em>for</em> different versions.
An efficient <abbr title='Resource Description Framework'>RDF</abbr> archive solution should have a scalable <em>storage model</em>,
efficient <em>compression</em>, and <em>indexing methods</em> that enable expressive versioned querying <span class="references">[<a href="#ref-35">35</a>]</span>.</p>

      <p>In this article,
we argue that supporting both <abbr title='Resource Description Framework'>RDF</abbr> archiving and <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> at once is difficult to scale due to their combined complexity.
Instead, we propose an elementary but efficient versioned <em>triple pattern</em> index.
Since triple patterns are the basic element of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>,
such indexes can serve as an entry point for query engines.
Our solution is applicable as:
(a) an alternative index with efficient triple-pattern-based access for existing engines, in order to improve the efficiency of more expressive <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries; and
(b) a data source for the Web-friendly <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments</a></span> <span class="references">[<a href="#ref-39">39</a>]</span> (<abbr title='Triple Pattern Fragments'>TPF</abbr>) interface, i.e.,
a Web <abbr title='Application Programming Interface'>API</abbr> that provides access to <abbr title='Resource Description Framework'>RDF</abbr> datasets by triple pattern and partitions the results in pages.
We focus on the performance-critical features of <em>stream-based results</em>, query result <em>offsets</em>, and <em>cardinality estimation</em>.
Stream-based results allow more memory-efficient processing when query results are plentiful.
The capability to efficiently offset (and limit) a large stream reduces processing time if only a subset is needed.
Cardinality estimation is essential for efficient <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">query planning</a></span> <span class="references">[<a href="#ref-39">39</a>, <a href="#ref-40">40</a>]</span> in many query engines.</p>

      <p>Concretely,
this work introduces a storage technique with the following contributions:</p>

      <ul>
        <li>a scalable versioned and compressed <abbr title='Resource Description Framework'>RDF</abbr> <em>index</em> with <em>offset</em> support and result <em>streaming</em>;</li>
        <li>efficient <em>query algorithms</em> to evaluate triple pattern queries and perform cardinality estimation <em>at</em>, <em>between</em>, and <em>for</em> different versions, with optional <em>offsets</em>;</li>
        <li>an open-source <em>implementation</em> of this approach called <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>;</li>
        <li>an extensive <em>evaluation</em> of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> compared to other approaches using an existing <abbr title='Resource Description Framework'>RDF</abbr> archiving benchmark.</li>
      </ul>

      <p>The main novelty of this work is the combination of efficient offset-enabled queries over a new index structure for <abbr title='Resource Description Framework'>RDF</abbr> archives.
We do not aim to compete with existing versioned <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engines—full access to the language can instead be leveraged by different engines,
or by using alternative <abbr title='Resource Description Framework'>RDF</abbr> publication and querying methods such as the <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> interface-based <abbr title='Triple Pattern Fragments'>TPF</abbr> approach.
Optional versioning capabilities are possible for <abbr title='Triple Pattern Fragments'>TPF</abbr> by using <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr></a> <span class="references">[<a href="#ref-41">41</a>]</span>,
or <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/jod2017.pdf">datetime content-negotiation</a> <span class="references">[<a href="#ref-42">42</a>]</span> through Memento <span class="references">[<a href="#ref-43">43</a>]</span>.</p>

      <p>This article is structured as follows.
In the following section, we start by introducing the related work and our problem statement in <a href="#storing_problem-statement">Subsection 3.4</a>.
Next, in <a href="#storing_fundamentals">Subsection 3.5</a>, we introduce the basic concepts of our approach,
followed by our storage approach in <a href="#storing_storage">Subsection 3.6</a>, our ingestion algorithms in <a href="#storing_ingestions">Subsection 3.7</a>,
and the accompanying querying algorithms in <a href="#storing_querying">Subsection 3.8</a>.
After that, we present and discuss the evaluation of our implementation in <a href="#storing_evaluation">Subsection 3.9</a>.
Finally, we present our conclusions in <a href="#storing_conclusions">Subsection 3.10</a>.</p>

    </section>

    <section id="storing_related-work">
      <h3>Related Work</h3>

      <p>In this section, we discuss existing solutions and techniques for indexing and compression in <abbr title='Resource Description Framework'>RDF</abbr> storage, without archiving support.
Then, we compare different <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions.
Finally, we discuss suitable benchmarks and different query types for <abbr title='Resource Description Framework'>RDF</abbr> archives.
This section does not contain an exhaustive list of all relevant solutions and techniques,
instead, only those that are most relevant to this work are mentioned.</p>

      <h4 id="general-rdf-indexing-and-compression">General <abbr title='Resource Description Framework'>RDF</abbr> Indexing and Compression</h4>

      <p><abbr title='Resource Description Framework'>RDF</abbr> storage systems typically use indexing and compression techniques
for reducing query times and storage space.
These systems can either be based on existing database technologies,
such as relational databases <span class="references">[<a href="#ref-44">44</a>]</span> or document stores <span class="references">[<a href="#ref-45">45</a>]</span>,
or on techniques tailored to <abbr title='Resource Description Framework'>RDF</abbr>.
These technologies can even be combined, such as approaches that detect <em>emergent schemas</em> <span class="references">[<a href="#ref-46">46</a>, <a href="#ref-47">47</a>]</span>
in <abbr title='Resource Description Framework'>RDF</abbr> datasets, which allow parts of the data to be stored in relational databases
in order to increase compression and improve the efficiency of query evaluation.
These emergent schemas are recently being exploited as <em>characteristics sets</em>
in native <abbr title='Resource Description Framework'>RDF</abbr> approaches <span class="references">[<a href="#ref-48">48</a>, <a href="#ref-49">49</a>]</span>.
For the remainder of this article, we focus the <abbr title='Resource Description Framework'>RDF</abbr>-specific techniques that have direct relevance to our approach.</p>

      <p><abbr title='Resource Description Framework'>RDF</abbr>-3X <span class="references">[<a href="#ref-40">40</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> storage technique that is based
on a clustered B+Tree with 18 indexes in which triples are sorted lexicographically.
Given that a triple consists of
a subject (S), predicate (P) and object (O),
it includes six indexes for different triple component orders (SPO, SOP, OSP, OPS, PSO and POS),
six aggregated indexes (SP, SO, PS, PO, OS, and OP),
and three one-valued indexes (S, P, and O).
A dictionary is used to compress common triple components.
When evaluating <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries, optimal indexes can be selected based on the query’s triple patterns.
Furthermore, the store allows update operations.
In our storage approach, we will reuse the concept of multiple indexes
and encoding triple components in a dictionary.</p>

      <p>Hexastore <span class="references">[<a href="#ref-50">50</a>]</span> is a similar approach as it uses six different sorted lists,
one for each possible triple component order.
Also, it uses dictionary encoding to compress common triple components.
An alternative is Triplebit <span class="references">[<a href="#ref-51">51</a>]</span>, which is based on a two-dimensional storage matrix.
Columns correspond to predicates, and rows to subjects and objects.
This sparse matrix is compressed and dictionary-encoded to reduce storage requirements.
Furthermore, it uses auxiliary index structures to improve index selection during query evaluation.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://arxiv.org/pdf/1105.4004.pdf">K2-Triples</a> <span class="references">[<a href="#ref-52">52</a>]</span> is another <abbr title='Resource Description Framework'>RDF</abbr> storage technique that uses <em>k2-tree</em>
structures to the data, which results in high compression rates.
These structures allow <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries to be evaluated in memory without decompressing the structures.</p>

      <p>RDFCSA <span class="references">[<a href="#ref-53">53</a>]</span> is a compact <abbr title='Resource Description Framework'>RDF</abbr> storage technique.
It is a <em>self-index</em> that stores the data together with its index, which results in less storage space than raw storage.
Furthermore, it is built on the concept of <em>compressed suffix arrays</em>,
which compresses text while still allowing efficient pattern-based search on it.
RDFCSA requires about twice the storage space compared to K2-Triples, but it is faster for most queries.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr></a> <span class="references">[<a href="#ref-54">54</a>]</span> is a binary <abbr title='Resource Description Framework'>RDF</abbr> representation that is highly compressed
and provides indexing structures that enable efficient querying.
It consists of three main components:</p>

      <dl>
        <dt>Header</dt>
        <dd>metadata describing the dataset</dd>
        <dt>Dictionary</dt>
        <dd>mapping between triple components and unique IDs for reducing storage requirements of triples</dd>
        <dt>Storage</dt>
        <dd>actual triples based on the IDs of the triple components</dd>
      </dl>

      <p>The dictionary component encodes triple components in four subsets.
The first subset consists of triple components that exist both as subject and objects.
The second and third subset respectively consists of the non-common subject and object component.
The last subset consists of the predicate components.
The storage part encodes triple components using the dictionary,
compacts the triples in a sorted predicate and object adjacency list,
and stores these adjacency list in a bitmap structure that efficiently
indicates the borders of these consecutive adjacency list.
By default, <abbr title='Header Dictionary Triples'>HDT</abbr> only stores triples in the SPO-order.
When querying is required, enhanced triple indexes are constructed
to allow any triple pattern to be resolved efficiently based on the <abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr> <span class="references">[<a href="#ref-55">55</a>]</span> approach.
<abbr title='Header Dictionary Triples'>HDT</abbr> archives are read-only, which leads to high efficiency and compressibility,
but makes them unsuitable for cases where datasets change frequently.
Its fast triple pattern queries and high compression rate make it
an appropriate backend storage method for <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><abbr title='Triple Pattern Fragments'>TPF</abbr></a></span> <span class="references">[<a href="#ref-39">39</a>]</span> servers.
Approaches like LOD Laundromat <span class="references">[<a href="#ref-56">56</a>]</span> combine <abbr title='Header Dictionary Triples'>HDT</abbr> and <abbr title='Triple Pattern Fragments'>TPF</abbr> for hosting and publishing
650K+ Linked Datasets containing 38B+ triples, proving its usefulness at large scale.
Because of these reasons, we will reuse <abbr title='Header Dictionary Triples'>HDT</abbr> snapshots as part of our storage solution.</p>

      <h4 id="storing_related-work-archiving"><abbr title='Resource Description Framework'>RDF</abbr> Archiving</h4>

      <p>Linked Open Datasets typically change over time <span class="references">[<a href="#ref-36">36</a>]</span>,
creating a need for <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">maintaining the history of the datasets</a> <span class="references">[<a href="#ref-35">35</a>]</span>.
Hence, <abbr title='Resource Description Framework'>RDF</abbr> archiving has been an active area of research over the last couple of years.
In the domain of non-<abbr title='Resource Description Framework'>RDF</abbr> graph databases, several graph database extensions exist.
These extensions are either <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/datablend/fluxgraph">wrapper-based</a> <span class="references">[<a href="#ref-57">57</a>, <a href="#ref-58">58</a>]</span>, which leads to sub-optimal querying due to the lack of indexing,
or they are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">changing the graph model</a> <span class="references">[<a href="#ref-59">59</a>, <a href="#ref-60">60</a>]</span>, which complicates the writing of queries.
Furthermore, none of the existing non-<abbr title='Resource Description Framework'>RDF</abbr> graph stores offer native versioning capabilities at the time of writing.
We therefore only discuss <abbr title='Resource Description Framework'>RDF</abbr> archiving for the remainder of this section.</p>

      <p>Fernández et al. formally define an <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><em><abbr title='Resource Description Framework'>RDF</abbr> archive</em></a> <span class="references">[<a href="#ref-61">61</a>]</span> as follows:
<em>An <abbr title='Resource Description Framework'>RDF</abbr> archive graph A is a set of version-annotated triples.</em>
Where a <em>version-annotated triple</em> <em>(s, p, o):[i]</em> is defined as <em>an <abbr title='Resource Description Framework'>RDF</abbr> triple (s, p, o) with a label i ∈ N representing the version in which this triple holds.</em>
The set of all <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><abbr title='Resource Description Framework'>RDF</abbr> triples</a> <span class="references">[<a href="#ref-2">2</a>]</span> is defined as <em>(U ∪ B) × U × (U ∪ B ∪ L)</em>,
where <em>U</em>, <em>B</em>, and <em>L</em>, respectively represent the disjoint, infinite sets of URIs, blank nodes, and literals.
Furthermore,
<em>an <abbr title='Resource Description Framework'>RDF</abbr> version of an <abbr title='Resource Description Framework'>RDF</abbr> archive A at snapshot i is the <abbr title='Resource Description Framework'>RDF</abbr> graph A(i) = {(s, p, o)|(s, p, o):[i] ∈ A}.</em>
For the remainder of this article, we use the notation <em>V<sub>i</sub></em> to refer to the <abbr title='Resource Description Framework'>RDF</abbr> version <em>A(i)</em>.</p>

      <p>The DIACHRON data model <span class="references">[<a href="#ref-37">37</a>]</span> introduces the concept of <em>diachronic datasets</em>,
i.e., datasets that contain diachronic entities, which are semantic entities that evolve over time.
This data model formally defines a diachronic dataset as a set of dataset versions together with metadata annotations about this dataset.
Each dataset version is defined as a set of records (i.e., tuples or triples), an associated schema,
temporal information about this version and metadata specific to this version.
Domain data must be reified in order to store it in the DIACHRON model.
Due to the simplicity of <abbr title='Resource Description Framework'>RDF</abbr> archive model compared to the domain-specific DIACHRON data model,
we will use the model of Fernández et al. for the remainder of this document.</p>

      <p>Systems for archiving Linked Open Data are categorized 
into <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">three non-orthogonal storage strategies</a> <span class="references">[<a href="#ref-35">35</a>]</span>:</p>

      <ul>
        <li>The <strong>Independent Copies (<abbr title='Independent copies'>IC</abbr>)</strong> approach creates separate instantiations of datasets for
each change or set of changes.</li>
        <li>The <strong>Change-Based (<abbr title='Change-based'>CB</abbr>)</strong> approach instead only stores change sets between versions.</li>
        <li>The <strong>Timestamp-Based (<abbr title='Timestamp-based'>TB</abbr>)</strong> approach stores the temporal validity of facts.</li>
      </ul>

      <p>In the following sections, we discuss several existing <abbr title='Resource Description Framework'>RDF</abbr> archiving systems, which use either pure <abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> or <abbr title='Timestamp-based'>TB</abbr>, or hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr>.
<a href="#storing_rdf-archive-systems">Table 5</a> shows an overview of the discussed systems.</p>

      <figure id="storing_rdf-archive-systems" class="table">

        <table>
          <thead>
            <tr>
              <th>Name</th>
              <th><abbr title='Independent copies'>IC</abbr></th>
              <th><abbr title='Change-based'>CB</abbr></th>
              <th><abbr title='Timestamp-based'>TB</abbr></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>SemVersion <span class="references">[<a href="#ref-62">62</a>]</span></td>
              <td>✓</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>Cassidy et. al. <span class="references">[<a href="#ref-63">63</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>R&amp;WBase <span class="references">[<a href="#ref-64">64</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>R43ples <span class="references">[<a href="#ref-65">65</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>Hauptman et. al. <span class="references">[<a href="#ref-66">66</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>X-<abbr title='Resource Description Framework'>RDF</abbr>-3X <span class="references">[<a href="#ref-67">67</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf"><abbr title='Resource Description Framework'>RDF</abbr>-TX</a> <span class="references">[<a href="#ref-68">68</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>v-RDFCSA <span class="references">[<a href="#ref-69">69</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Dydra <span class="references">[<a href="#ref-70">70</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>TailR <span class="references">[<a href="#ref-71">71</a>]</span></td>
              <td>✓</td>
              <td>✓</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 5:</span> Overview of <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions with their corresponding storage strategy:
Individual copies (<abbr title='Independent copies'>IC</abbr>), Change-based (<abbr title='Change-based'>CB</abbr>), or Timestamp-based (<abbr title='Timestamp-based'>TB</abbr>).</p>
        </figcaption>
      </figure>

      <h5 id="independent-copies-approaches">Independent copies approaches</h5>
      <p>SemVersion <span class="references">[<a href="#ref-62">62</a>]</span> was one of the first works to look into tracking different versions of <abbr title='Resource Description Framework'>RDF</abbr> graphs.
SemVersion is based on Concurrent Versions System (<abbr title='Concurrent Versions System'>CVS</abbr>) concepts to maintain different versions of ontologies,
such as diff, branching and merging.
Their approach consists of a separation of language-specific features with ontology versioning from general features together with <abbr title='Resource Description Framework'>RDF</abbr> versioning.
Unfortunately, the implementation details on triple storage and retrieval are unknown.</p>

      <h5 id="change-based-approaches">Change-based approaches</h5>
      <p>Based on the Theory of Patches from the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://darcs.net">Darcs software management system</a> <span class="references">[<a href="#ref-72">72</a>]</span>,
Cassidy et. al. <span class="references">[<a href="#ref-63">63</a>]</span> propose to store changes to graphs as a series of patches, which makes it a <abbr title='Change-based'>CB</abbr> approach.
They describe operations on versioned graphs such as reverse, revert and merge.
An implementation of their approach is provided using the Redland python library and MySQL
by representing each patch as named graphs and serializing them in <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/trig/">TriG</a> <span class="references">[<a href="#ref-73">73</a>]</span>.
Furthermore, a preliminary evaluation shows that their implementation is significantly slower
than a native <abbr title='Resource Description Framework'>RDF</abbr> store. They suggest a native implementation of the approach to avoid some of the overhead.</p>

      <p>Im et. al. <span class="references">[<a href="#ref-74">74</a>]</span> propose a <abbr title='Change-based'>CB</abbr> patching system based on a relational database.
In their approach, they use a storage scheme called <em>aggregated deltas</em>
which associates the latest version with each of the previous ones.
While aggregated deltas result in fast delta queries, they introduce much storage overhead.</p>

      <p>R&amp;WBase <span class="references">[<a href="#ref-64">64</a>]</span> is a <abbr title='Change-based'>CB</abbr> versioning system that adds an additional versioning layer to existing quad-stores.
It adds the functionality of tagging, branching and merging for datasets.
The graph element is used to represent the additions and deletions of patches,
which are respectively the even and uneven graph IDs.
Queries are resolved by looking at the highest even graph number of triples.</p>

      <p>Graube et. al. introduce R43ples <span class="references">[<a href="#ref-65">65</a>]</span> which stores change sets as separate named graphs, making it a <abbr title='Change-based'>CB</abbr> system.
It supports the same versioning features as R&amp;WBase and introduces new <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> keywords for these, such as REVISION, BRANCH and TAG.
As reconstructing a version requires combining all change sets that came before,
queries at a certain version are only usable for medium-sized datasets.</p>

      <h5 id="timestamp-based-approaches">Timestamp-based approaches</h5>
      <p>Hauptman et. al. introduce a similar delta-based storage approach <span class="references">[<a href="#ref-66">66</a>]</span>
by storing each triple in a different named graph as a <abbr title='Timestamp-based'>TB</abbr> storage approach.
The identifying graph of each triple is used in a commit graph for <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation at a certain version.
Their implementation is based on Sesame <span class="references">[<a href="#ref-75">75</a>]</span> and Blazegraph <span class="references">[<a href="#ref-76">76</a>]</span> and is slower than snapshot-based approaches, but uses less disk space.</p>

      <p>X-<abbr title='Resource Description Framework'>RDF</abbr>-3X <span class="references">[<a href="#ref-67">67</a>]</span> is an extension of <abbr title='Resource Description Framework'>RDF</abbr>-3X <span class="references">[<a href="#ref-40">40</a>]</span> which adds versioning support using the <abbr title='Timestamp-based'>TB</abbr> approach.
On storage-level, each triple is annotated with a creation and deletion timestamp.
This enables time-travel queries where only triples valid at the given time are returned.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf"><abbr title='Resource Description Framework'>RDF</abbr>-TX</a> <span class="references">[<a href="#ref-68">68</a>]</span> is an in-memory query engine that supports a temporal <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> querying extension.
The system is based on compressed multi-version B+Trees that outperforms similar systems such as X-<abbr title='Resource Description Framework'>RDF</abbr>-3X in terms of querying efficiency.
The required storage space after indexing is similar to that of X-<abbr title='Resource Description Framework'>RDF</abbr>-3X.</p>

      <p>v-RDFCSA <span class="references">[<a href="#ref-69">69</a>]</span> is a self-indexing <abbr title='Resource Description Framework'>RDF</abbr> archive mechanism,
based on the <abbr title='Resource Description Framework'>RDF</abbr> self-index RDFCSA <span class="references">[<a href="#ref-53">53</a>]</span>,
that enables versioning queries on top of compressed <abbr title='Resource Description Framework'>RDF</abbr> archives as a <abbr title='Timestamp-based'>TB</abbr> approach.
They evaluate their approach using the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><abbr title='Benchmark of rdf Archives'>BEAR</abbr></a> <span class="references">[<a href="#ref-61">61</a>]</span> benchmark
and show that they can reduce storage space requirements 60 times compared to raw storage.
Furthermore, they reduce query evaluation times more than an order of magnitude compared to state of the art solutions.</p>

      <p>Dydra <span class="references">[<a href="#ref-70">70</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> graph storage platform with dataset versioning support.
They introduce the REVISION keyword, which is similar to the GRAPH <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> keyword for referring to different dataset versions.
Their implementation is based on B+Trees that are indexed in six ways  GSPO, GPOS, GOSP, SPOG, POSG, OSPG.
Each B+Tree value indicates the revisions in which a particular quad exists, which makes it a <abbr title='Timestamp-based'>TB</abbr> approach.</p>

      <h5 id="hybrid-approaches">Hybrid approaches</h5>
      <p>TailR <span class="references">[<a href="#ref-71">71</a>]</span> is an <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> archive for Linked Data pages based
on the Memento protocol <span class="references">[<a href="#ref-43">43</a>]</span> for retrieving prior versions of certain <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> resources.
It is a hybrid <abbr title='Change-based'>CB</abbr>/<abbr title='Independent copies'>IC</abbr> approach as it starts by storing a dataset snapshot,
after which only deltas are stored for each consecutive version, as shown in <a href="#storing_regular-delta-chain">Fig. 34</a>.
When the chain becomes too long, or other conditions are fulfilled,
a new snapshot is created for the next version to avoid long version reconstruction times.</p>

      <p>Results show that this is an effective way of reducing version reconstruction times <span class="references">[<a href="#ref-71">71</a>]</span>,
in particular for many versions.
Within the delta chain, however, an increase in version reconstruction times can still be observed.
Furthermore, it requires more storage space than pure delta-based approaches.</p>

      <p>The authors’ implementation is based on a relational database system.
Evaluation shows that resource lookup times for any version ranges between
1 and 50 ms for 10 versions containing around 500K triples.
In total, these versions require ~64MB of storage space.</p>

      <figure id="storing_regular-delta-chain">
<img src="storing/img/regular-delta-chain.svg" alt="[regular delta chain]" />
<figcaption>
          <p><span class="label">Fig. 34:</span> Delta chain in which deltas are relative to the previous delta, as is done in TailR <span class="references">[<a href="#ref-71">71</a>]</span>.</p>
        </figcaption>
</figure>

      <h4 id="related-work-benchmarks"><abbr title='Resource Description Framework'>RDF</abbr> Archiving Benchmarks</h4>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><abbr title='Benchmark of rdf Archives'>BEAR</abbr></a> <span class="references">[<a href="#ref-61">61</a>]</span> is a benchmark for <abbr title='Resource Description Framework'>RDF</abbr> archive systems.
The <abbr title='Benchmark of rdf Archives'>BEAR</abbr> benchmark is based on three real-world datasets from different domains:</p>

      <dl>
        <dt><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A</dt>
        <dd>58 weekly snapshots from the Dynamic Linked Data Observatory <span class="references">[<a href="#ref-36">36</a>]</span>. This is the main dataset from the article on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><abbr title='Benchmark of rdf Archives'>BEAR</abbr></a> <span class="references">[<a href="#ref-61">61</a>]</span>.</dd>
        <dt><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B</dt>
        <dd>The 100 most volatile resources from DBpedia Live <span class="references">[<a href="#ref-77">77</a>]</span> over the course of three months
as three different granularities: instant, hour and day.</dd>
        <dt><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-C</dt>
        <dd>Dataset descriptions from the Open Data Portal Watch <span class="references">[<a href="#ref-78">78</a>]</span> project over the course of 32 weeks.</dd>
      </dl>

      <p>The 58 versions of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A contain between 30M and 66M triples per version, with an average change ratio of 31%.
<abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A provides triple pattern queries for three different versioned query types for both result sets with a low and a high cardinality.
The queries are selected in such a way that they will be evaluated over triples of a certain dynamicity,
which requires the benchmarked systems to handle this dynamicity well.
<abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B provides a small collection of triple pattern queries corresponding to the real-world usage of DBpedia.
Finally, <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-C provides 10 complex <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries that were created with the help of Open Data experts.</p>

      <p><abbr title='Benchmark of rdf Archives'>BEAR</abbr> provides baseline <abbr title='Resource Description Framework'>RDF</abbr> archive implementations based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr></a> <span class="references">[<a href="#ref-54">54</a>]</span> and
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://jena.apache.org/">Jena’s</a> <span class="references">[<a href="#ref-79">79</a>]</span> <a href="https://jena.apache.org/documentation/tdb/">TDB store</a>
for the <abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr>, and <abbr title='Timestamp-based'>TB</abbr> approaches, but also hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr>/<abbr title='Change-based'>CB</abbr> approaches.
The hybrid approaches are based on snapshots followed by delta chains, as implemented by TailR <span class="references">[<a href="#ref-71">71</a>]</span>.
Due to <abbr title='Header Dictionary Triples'>HDT</abbr> not supporting quads, the <abbr title='Timestamp-based'>TB</abbr> and <abbr title='Timestamp-based'>TB</abbr>/<abbr title='Change-based'>CB</abbr> approaches could not be implemented in the <abbr title='Header Dictionary Triples'>HDT</abbr> baseline implementations.</p>

      <p>Results show that <abbr title='Independent copies'>IC</abbr> for both Jena and <abbr title='Header Dictionary Triples'>HDT</abbr> requires more storage space than the compressed deltas for the three datasets.
<abbr title='Change-based'>CB</abbr> results in less storage space for both approaches for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B, but not for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-C because that dataset is so dynamic that
the deltas require more storage space than they would in with <abbr title='Independent copies'>IC</abbr>.
Jena-<abbr title='Timestamp-based'>TB</abbr> results in the least storage space of Jena-based approaches,
however,
it fails for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-instant because of the large amount of versions
as Jena is less efficient for many graphs.</p>

      <p>The hybrid approaches are evaluated with different delta chain lengths and expectedly show
that shorter delta chains lead to results similar to <abbr title='Independent copies'>IC</abbr>, and longer delta chains lead are similar to <abbr title='Change-based'>CB</abbr> or <abbr title='Timestamp-based'>TB</abbr>.
The queries for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B show that
<abbr title='Independent copies'>IC</abbr> results in constant evaluation times for any version,
<abbr title='Change-based'>CB</abbr> times increase for each following version,
and <abbr title='Timestamp-based'>TB</abbr> also result in constant times.
The <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches outperform Jena in all cases because of its compressed nature.
The <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr> hybrid approaches similarly show increasing evaluation times for each version,
with a drop each time a new snapshot is created.
The <abbr title='Independent copies'>IC</abbr>/<abbr title='Timestamp-based'>TB</abbr> hybrid Jena approach has slowly increasing evaluation times for each version,
but they are significantly lower than the regular <abbr title='Timestamp-based'>TB</abbr> approach.</p>

      <p>The queries of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-C currently can not be solved by the archiving strategies in a straightforward way,
but they are designed to help foster the development of future <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions.
While queries of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B are just triple pattern queries and therefore do not cover the full <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> spectrum,
they provide the basis for more complex queries, as is proven by the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><abbr title='Triple Pattern Fragments'>TPF</abbr> framework</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>,
which makes them sufficient for benchmarking.</p>

      <p>EvoGen <span class="references">[<a href="#ref-80">80</a>]</span> is an <abbr title='Resource Description Framework'>RDF</abbr> archive systems benchmark that is based on the synthetic <abbr title='Lehigh University Benchmark'>LUBM</abbr> dataset generator <span class="references">[<a href="#ref-81">81</a>]</span>.
It is an extension of the <abbr title='Lehigh University Benchmark'>LUBM</abbr> generator with additional classes and properties for introducing dataset evolution on schema-level.
EvoGen enables the user to tweak parameters of the dataset and query generation process,
for example to change the dataset dynamicity and the number of versions.</p>

      <p>While EvoGen offers more flexibility than <abbr title='Benchmark of rdf Archives'>BEAR</abbr> in terms of configurability.
<abbr title='Benchmark of rdf Archives'>BEAR</abbr> provides real-world datasets and baseline implementations which lowers the barrier towards its usage.
Hence, we will use the <abbr title='Benchmark of rdf Archives'>BEAR</abbr> dataset in this work for benchmarking our system.</p>

      <h4 id="query-atoms">Query atoms</h4>

      <p>The query atoms that will be introduced in this section are based on
the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><abbr title='Resource Description Framework'>RDF</abbr> data model</a> <span class="references">[<a href="#ref-2">2</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language</a> <span class="references">[<a href="#ref-3">3</a>]</span>.
In these models, a <em>triple pattern</em> is defined as <em>(U ∪ V) × (U ∪ V) × (U ∪ L ∪ V)</em>, with <em>V</em> being the infinite set of variables.
A set of triple patterns is called a <em>Basic Graph Pattern</em>, which forms the basis of a <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query.
The evaluation of a <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query <em>Q</em> on an <abbr title='Resource Description Framework'>RDF</abbr> graph <em>G</em> containing <abbr title='Resource Description Framework'>RDF</abbr> triples,
produces a bag of solution mappings <em>[[Q]]<sub>G</sub></em>.</p>

      <p>To cover the retrieval demands in <abbr title='Resource Description Framework'>RDF</abbr> archiving,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">five foundational query types were introduced</a> <span class="references">[<a href="#ref-61">61</a>]</span>,
which are referred to as <em>query atoms</em>:</p>

      <ol>
        <li><strong>Version materialization (<abbr title='Version materialization'>VM</abbr>)</strong> retrieves data using a query <em>Q</em> targeted at a single version <em>V<sub>i</sub></em>.
Formally: <em><abbr title='Version materialization'>VM</abbr>(Q, V<sub>i</sub>) = [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were present in the library yesterday?</em></li>
        <li><strong>Delta materialization (<abbr title='Delta materialization'>DM</abbr>)</strong> retrieves query <em>Q</em>’s result change sets between two versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em><abbr title='Delta materialization'>DM</abbr>(Q, V<sub>i</sub>, V<sub>j</sub>)=(Ω<sup>+</sup>, Ω<sup>−</sup>). With Ω<sup>+</sup> = [[Q]]<sub>V<sub>i</sub></sub> \ [[Q]]<sub>V<sub>j</sub></sub> and Ω<sup>−</sup> = [[Q]]<sub>V<sub>j</sub></sub> \ [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were returned or taken from the library between yesterday and now?</em></li>
        <li><strong>Version query (<abbr title='Version query'>VQ</abbr>)</strong> annotates query <em>Q</em>’s results with the versions (of <abbr title='Resource Description Framework'>RDF</abbr> archive A) in which they are valid.
Formally: <em><abbr title='Version query'>VQ</abbr>(Q, A) = {(Ω, W) | W = {A(i) | Ω=[[Q]]<sub>A(i)</sub>, i ∈ N} ∧ Ω ≠ ∅}</em>.
Example: <em>At what times was book X present in the library?</em></li>
        <li><strong>Cross-version join (<abbr title='Cross-version join'>CV</abbr>)</strong> joins the results of two queries (<em>Q1</em> and <em>Q2</em>) between versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em><abbr title='Version materialization'>VM</abbr>(Q1, V<sub>i</sub>) ⨝ <abbr title='Version materialization'>VM</abbr>(Q2, V<sub>j</sub>)</em>.
Example: <em>What books were present in the library yesterday and today?</em></li>
        <li><strong>Change materialization (<abbr title='Change materialization'>CM</abbr>)</strong> returns a list of versions in which a given query <em>Q</em> produces
consecutively different results.
Formally: <em>{(i, j) | i,j ∈ ℕ, i &lt; j, <abbr title='Delta materialization'>DM</abbr>(Q, A(i), A(j)) = (Ω<sup>+</sup>, Ω<sup>−</sup>), Ω<sup>+</sup> ∪ Ω<sup>−</sup> ≠ ∅, ∄ k ∈ ℕ : i &lt; k &lt; j}</em>.
Example: <em>At what times was book X returned or taken from the library?</em></li>
      </ol>

      <p>There exists a correspondence between these query atoms
and the independent copies (<abbr title='Independent copies'>IC</abbr>), change-based (<abbr title='Change-based'>CB</abbr>), and timestamp-based (<abbr title='Timestamp-based'>TB</abbr>) storage strategies.</p>

      <p>Namely, <abbr title='Version materialization'>VM</abbr> queries are efficient in storage solutions that are based on <abbr title='Independent copies'>IC</abbr>, because there is indexing on version.
On the other hand, <abbr title='Independent copies'>IC</abbr>-based solutions may introduce a large amount of overhead in terms of storage space because each version is stored separately.
Furthermore, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> queries are less efficient for <abbr title='Independent copies'>IC</abbr> solutions.
That is because <abbr title='Delta materialization'>DM</abbr> queries require two fully-materialized versions to be compared on-the-fly,
and <abbr title='Version query'>VQ</abbr> requires <em>all</em> versions to be queried at the same time.</p>

      <p><abbr title='Delta materialization'>DM</abbr> queries can be efficient in <abbr title='Change-based'>CB</abbr> solutions if the query version ranges correspond to the stored delta ranges.
In all other cases, as well as for <abbr title='Version materialization'>VM</abbr> and <abbr title='Version query'>VQ</abbr> queries, the desired versions must be materialized on-the-fly,
which will take increasingly more time for longer delta chains.
<abbr title='Change-based'>CB</abbr> solutions do however typically require less storage space than <abbr title='Version materialization'>VM</abbr> if there is sufficient overlap between each consecutive version.</p>

      <p>Finally, <abbr title='Version query'>VQ</abbr> queries perform well for <abbr title='Timestamp-based'>TB</abbr> solutions because the timestamp annotation directly corresponds to <abbr title='Version query'>VQ</abbr>’s result format.
<abbr title='Version materialization'>VM</abbr> and <abbr title='Delta materialization'>DM</abbr> queries in this case are typically less efficient than for <abbr title='Independent copies'>IC</abbr> approaches, due to the missing version index.
Furthermore, <abbr title='Timestamp-based'>TB</abbr> solutions can require less storage space compared to <abbr title='Version materialization'>VM</abbr> if the change ratio of the dataset is not too large.</p>

      <p>In summary, <abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches can perform well for certain query types, but they can be slow for others.
On the other hand, this efficiency typically comes at the cost of a large storage overhead, as is the case for <abbr title='Independent copies'>IC</abbr>-based approaches.</p>

      <p>DIACHRON QL <span class="references">[<a href="#ref-37">37</a>]</span> is a <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language extension
based on the DIACHRON data model that provides functionality similar to these query atoms
in order to query specific versions, changesets, or all versions.</p>

    </section>

    <section id="storing_problem-statement">
      <h3>Problem statement</h3>

      <p>As mentioned in <a href="#storing_introduction">Subsection 3.2</a>, no <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions exist that allow
efficient triple pattern querying <em>at</em>, <em>between</em>, and <em>for</em> different versions,
in combination with a scalable <em>storage model</em> and efficient <em>compression</em>.
In the context of query engines, streams are typically used to return query results,
on which offsets and limits can be applied to reduce processing time if only a subset is needed.
Offsets are used to skip a certain amount of elements,
while limits are used to restrict the number of elements to a given amount.
As such, <abbr title='Resource Description Framework'>RDF</abbr> archiving solutions should also allow query results to be returned as offsettable streams.
The ability to achieve such stream subsets is limited in existing solutions.</p>

      <p id="storing_researchquestion">This leads us to the following research question:
&gt; How can we store <abbr title='Resource Description Framework'>RDF</abbr> archives to enable efficient <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> triple pattern queries with offsets?</p>

      <p>The focus of this article is evaluating version materialization (<abbr title='Version materialization'>VM</abbr>), delta materialization (<abbr title='Delta materialization'>DM</abbr>), and version (<abbr title='Version query'>VQ</abbr>) queries efficiently,
as <abbr title='Cross-version join'>CV</abbr> and <abbr title='Change materialization'>CM</abbr> queries can be expressed in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">terms of the other ones</a> <span class="references">[<a href="#ref-82">82</a>]</span>.
In total, our research question indentifies the following requirements:</p>

      <ul>
        <li>an efficient <abbr title='Resource Description Framework'>RDF</abbr> archive storage technique;</li>
        <li><abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> triple pattern querying algorithms on top of this storage technique;</li>
        <li>efficient offsetting of the <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr>, and <abbr title='Version query'>VQ</abbr> query result streams.</li>
      </ul>

      <p>In this work, we lower query evaluation times by processing and storing more metadata during ingestion time.
Instead of processing metadata during every lookup, this happens only once per version.
This will increase ingestion times, but will improve the efficiency of performance-critical features
within query engines and Linked Data interfaces, such as querying with offsets.
To this end, we introduce the following hypotheses:</p>

      <ol>
        <li id="storing_hypothesis-qualitative-querying">Our approach shows no influence of the selected versions on the querying efficiency of <abbr title='Version materialization'>VM</abbr> and <abbr title='Delta materialization'>DM</abbr> triple pattern queries.</li>
        <li id="storing_hypothesis-qualitative-ic-storage">Our approach requires <em>less</em> storage space than state-of-the-art <abbr title='Independent copies'>IC</abbr>-based approaches.</li>
        <li id="storing_hypothesis-qualitative-ic-querying">For our approach, querying is <em>slower</em> for <abbr title='Version materialization'>VM</abbr> and <em>equal</em> or <em>faster</em> for <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> than in state-of-the-art <abbr title='Independent copies'>IC</abbr>-based approaches.</li>
        <li id="storing_hypothesis-qualitative-cb-storage">Our approach requires <em>more</em> storage space than state-of-the-art <abbr title='Change-based'>CB</abbr>-based approaches.</li>
        <li id="storing_hypothesis-qualitative-cb-querying">For our approach, querying is <em>equal</em> or <em>faster</em> than in state-of-the-art <abbr title='Change-based'>CB</abbr>-based approaches.</li>
        <li id="storing_hypothesis-qualitative-ingestion">Our approach reduces average query time compared to other non-<abbr title='Independent copies'>IC</abbr> approaches at the cost of increased ingestion time.</li>
      </ol>

    </section>

    <section id="storing_fundamentals">
      <h3>Overview of Approaches</h3>

      <p>In this section, we lay the groundwork for the following sections.
We introduce fundamental concepts
that are required in our storage approach and its accompanying querying algorithms,
which will be explained in <a href="#storing_storage">Subsection 3.6</a> and <a href="#storing_querying">Subsection 3.8</a>, respectively.</p>

      <p>To combine smart use of storage space with efficient processing of <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr>, and <abbr title='Version query'>VQ</abbr> triple pattern queries,
we employ a hybrid approach between the individual copies (<abbr title='Independent copies'>IC</abbr>), change-based (<abbr title='Change-based'>CB</abbr>), and timestamp-based (<abbr title='Timestamp-based'>TB</abbr>) storage techniques (as discussed in <a href="#storing_related-work">Subsection 3.3</a>).
In summary, intermittent <em>fully materialized snapshots</em> are followed by <em>delta chains</em>.
Each delta chain is stored in <em>six tree-based indexes</em>, where values are dictionary-encoded and timestamped
to reduce storage requirements and lookup times.
These six indexes correspond to the combinations for storing three triple component orders
separately for additions and deletions.
The indexes for the three different triple component orders
ensure that any triple pattern query can be resolved quickly.
The additions and deletions are stored separately
because access patterns to additions and deletions in deltas differ between <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr>, and <abbr title='Version query'>VQ</abbr> queries.
To efficiently support inter-delta <abbr title='Delta materialization'>DM</abbr> queries, each addition and deletion value contains a <em>local change</em> flag
that indicates if the change is not relative to the snapshot.
Finally, in order to provide cardinality estimation for any triple pattern,
we store an additional count data structure.</p>

      <p>In the following sections, we discuss the most important distinguishing features of our approach.
We elaborate on the novel hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> storage technique that our approach is based on,
the reason for using multiple indexes,
having local change metadata,
and methods for storing addition and deletion counts.</p>

      <h4 id="storing_snapshot-delta-chain">Snapshot and Delta Chain</h4>

      <p>Our storage technique is partially based on a hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr> approach similar to <a href="#storing_regular-delta-chain">Fig. 34</a>.
To avoid increasing reconstruction times,
we construct the delta chain in an aggregated deltas <span class="references">[<a href="#ref-74">74</a>]</span> fashion:
each delta is <em>independent</em> of a preceding delta and relative to the closest preceding snapshot in the chain, as shown in <a href="#storing_alternative-delta-chain">Fig. 35</a>.
Hence, for any version, reconstruction only requires at most one delta and one snapshot.
Although this does increase possible redundancies within delta chains,
due to each delta <em>inheriting</em> the changes of its preceding delta,
the overhead can be compensated with compression, which we discuss in <a href="#storing_storage">Subsection 3.6</a>.</p>

      <figure id="storing_alternative-delta-chain">
<img src="storing/img/alternative-delta-chain.svg" alt="[alternative delta chain]" />
<figcaption>
          <p><span class="label">Fig. 35:</span> Delta chain in which deltas are relative to the snapshot at the start of the chain, as part of our approach.</p>
        </figcaption>
</figure>

      <h4 id="storing_indexes">Multiple Indexes</h4>

      <p>Our storage approach consists of six different indexes that are used for separately storing additions and deletions
in three different triple component orders, namely: <code>SPO</code>, <code>POS</code> and <code>OSP</code>.
These indexes are B+Trees, thereby, the starting triple for any triple pattern can be found in logarithmic time.
Consequently, the next triples can be found by iterating through the links between each tree leaf.
<a href="#triple-pattern-index-mapping"></a> shows an overview of which triple patterns can be mapped to which index.
In contrast to other approaches <span class="references">[<a href="#ref-40">40</a>, <a href="#ref-50">50</a>]</span> that ensure certain triple orders,
we use three indexes instead of all six possible component orders,
because we only aim to reduce the iteration scope of the lookup tree for any triple pattern.
For each possible triple pattern,
we now have an index that locates the first triple component in logarithmic time,
and identifies the terminating element of the result stream without necessarily having iterate to the last value of the tree.
For some scenarios, it might be beneficial to ensure the order of triples in the result stream,
so that more efficient stream joining algorithms can be used, such as sort-merge join.
If this would be needed, <code>OPS</code>, <code>PSO</code> and <code>SOP</code> indexes could optionally be added
so that all possible triple orders would be available.</p>

      <figure id="storing_triple-pattern-index-mapping" class="table">

        <table>
          <thead>
            <tr>
              <th>Triple pattern</th>
              <th><code>SPO</code></th>
              <th><code>SP?</code></th>
              <th><code>S?O</code></th>
              <th><code>S??</code></th>
              <th><code>?PO</code></th>
              <th><code>?P?</code></th>
              <th><code>??O</code></th>
              <th><code>???</code></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr></strong></td>
              <td><code>SPO</code></td>
              <td><code>SPO</code></td>
              <td><code>OSP</code></td>
              <td><code>SPO</code></td>
              <td><code>POS</code></td>
              <td><code>POS</code></td>
              <td><code>OSP</code></td>
              <td><code>SPO</code></td>
            </tr>
            <tr>
              <td><strong><abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr></strong></td>
              <td><code>SPO</code></td>
              <td><code>SPO</code></td>
              <td><code>SPO</code></td>
              <td><code>SPO</code></td>
              <td><code>OPS</code></td>
              <td><code>PSO</code></td>
              <td><code>OPS</code></td>
              <td><code>SPO</code></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 6:</span> Overview of which triple patterns are queried inside which index in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr>.</p>
        </figcaption>
      </figure>

      <p>Our approach could also act as a dedicated <abbr title='Resource Description Framework'>RDF</abbr> archiving solution
without (necessarily efficient) querying capabilities.
In this case, only a single index would be required, such as <code>SPO</code>, which would reduce the required storage space even further.
If querying would become required afterwards,
the auxiliary <code>OSP</code> and <code>POS</code> indexes could still be derived from this main index
during a one-time, pre-querying processing phase.</p>

      <p>This technique is similar to the <abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr> <span class="references">[<a href="#ref-55">55</a>]</span> extension for <abbr title='Header Dictionary Triples'>HDT</abbr> that adds additional indexes to a basic <abbr title='Header Dictionary Triples'>HDT</abbr> file
to enable faster querying for any triple pattern.
The main difference is that <abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr> uses the indexes <code>OSP</code>, <code>PSO</code> and <code>OPS</code>,
with a different triple pattern to index mapping as shown in <a href="#storing_triple-pattern-index-mapping">Table 6</a>.
We chose our indexes in order to achieve a more balanced distribution from triple patterns to index,
which could lead to improved load balancing between indexes when queries are parallelized.
<abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr> uses <code>SPO</code> for five triple pattern groups, <code>OPS</code> for two and <code>PSO</code> for only a single group.
Our approach uses <code>SPO</code> for 4 groups, <code>POS</code> for two and <code>OSP</code> for two.
Future work is needed to evaluate the distribution for real-world queries.
Additionally, the mapping from patterns <code>S?O</code> to index <code>SPO</code> in <abbr title='hdt Focus on Querying'><abbr title='Header Dictionary Triples'>HDT</abbr>-FoQ</abbr> will lead to suboptimal query evaluation
when a large number of distinct predicates is present.</p>

      <h4 id="storing_local-changes">Local Changes</h4>

      <p>A delta chain can contain multiple instances of the same triple,
since it could be added in one version and removed in the next.
Triples that revert a previous addition or deletion within the same delta chain, are called <em>local changes</em>,
and are important for query evaluation.
Determining the locality of changes can be costly,
thus we pre-calculate this information during ingestion time and store it for each versioned triple,
so that this does not have to happen during query-time.</p>

      <p>When evaluating version materialization queries by combining a delta with its snapshot,
all local changes should be filtered out.
For example, a triple <code>A</code> that was deleted in version 1, but re-added in version 2,
is cancelled out when materializing against version 2.
For delta materialization, these local changes should be taken into account,
because triple <code>A</code> should be marked as a deletion between versions 0 and 1,
but as an addition between versions 1 and 2.
Finally, for version queries, this information is also required
so that the version ranges for each triple can be determined.</p>

      <h4 id="storing_addition-deletion-counts">Addition and Deletion counts</h4>

      <p>Parts of our querying algorithms depend on the ability to efficiently count
the <em>exact</em> number of additions or deletions in a delta.
Instead of naively counting triples by iterating over all of them,
we propose two separate approaches for enabling efficient addition and deletion counting in deltas.</p>

      <p>For additions, we store an additional mapping from triple pattern and version to number of additions
so that counts can happen in constant time by just looking them up in the map.
For deletions, we store additional metadata in the main deletions tree.
Both of these approaches will be further explained in <a href="#storing_storage">Subsection 3.6</a>.</p>

    </section>

    <section id="storing_storage">
      <h3>Hybrid Multiversion Storage</h3>

      <p>In this section, we introduce our hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> storage approach for storing multiple versions of an <abbr title='Resource Description Framework'>RDF</abbr> dataset.
<a href="#storing_storage-overview">Fig. 36</a> shows an overview of the main components.
Our approach consists of an initial dataset snapshot—stored in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr></a> <span class="references">[<a href="#ref-54">54</a>]</span>—followed by a delta chain (similar to TailR <span class="references">[<a href="#ref-71">71</a>]</span>).
The delta chain uses multiple compressed B+Trees for a <abbr title='Timestamp-based'>TB</abbr>-storage strategy (similar to Dydra <span class="references">[<a href="#ref-70">70</a>]</span>),
applies dictionary-encoding to triples, and
stores additional metadata to improve lookup times.
In this section, we discuss each component in more detail.
In the next section, we describe two ingestion algorithms based on this storage structure.</p>

      <figure id="storing_storage-overview">
<img src="storing/img/storage-overview.svg" alt="[storage overview]" />
<figcaption>
          <p><span class="label">Fig. 36:</span> Overview of the main components of our hybrid <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> storage approach.</p>
        </figcaption>
</figure>

      <p>Throughout this section, we will use the example <abbr title='Resource Description Framework'>RDF</abbr> archive from <a href="#storing_example-archive">Table 7</a>
to illustrate the different storage components with.</p>

      <figure id="storing_example-archive" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: right">Version</th>
              <th>Triple</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: right">0</td>
              <td><code>:Bob foaf:name "Bobby"</code></td>
            </tr>
            <tr>
              <td style="text-align: right">1</td>
              <td><code>:Alice foaf:name "Alice"</code></td>
            </tr>
            <tr>
              <td style="text-align: right">1</td>
              <td><code>:Bob foaf:name "Bobby"</code></td>
            </tr>
            <tr>
              <td style="text-align: right">2</td>
              <td><code>:Bob foaf:name "Bob"</code></td>
            </tr>
            <tr>
              <td style="text-align: right">3</td>
              <td><code>:Alice foaf:name "Alice"</code></td>
            </tr>
            <tr>
              <td style="text-align: right">3</td>
              <td><code>:Bob foaf:name "Bob"</code></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 7:</span> Example of a small <abbr title='Resource Description Framework'>RDF</abbr> archive with 4 versions.
We assume that the following URI prefixes: <code>: http://example.org</code>, <code>foaf: http://xmlns.com/foaf/0.1/</code></p>
        </figcaption>
      </figure>

      <h4 id="storing_snapshot-storage">Snapshot storage</h4>

      <p>As mentioned before, the start of each delta chain is a fully materialized snapshot.
In order to provide sufficient efficiency for <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> querying with respect to all versions in the chain,
we assume the following requirements for the snapshot storage:</p>

      <ul>
        <li>Any triple pattern query <em>must</em> be resolvable as triple streams.</li>
        <li>Offsets <em>must</em> be applicable to the result stream of any triple pattern query.</li>
        <li>Cardinality estimation for all triple pattern queries <em>must</em> be possible.</li>
      </ul>

      <p>These requirements are needed for ensuring the efficiency of the querying algorithms that will be introduced in <a href="#querying">Section 4</a>.
For the implementation of snapshots,
existing techniques such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr></a> <span class="references">[<a href="#ref-54">54</a>]</span> fulfill all the requirements.
Therefore,
we do not introduce a new snapshot approach, but use <abbr title='Header Dictionary Triples'>HDT</abbr> in our implementation.
This will be explained further in <a href="#storing_implementation">Subsubsection 3.9.1</a>.</p>

      <h4 id="storing_dictionary">Delta Chain Dictionary</h4>

      <p>A common technique in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Resource Description Framework'>RDF</abbr> indexes</a> <span class="references">[<a href="#ref-54">54</a>, <a href="#ref-40">40</a>, <a href="#ref-51">51</a>]</span> is to use a dictionary for mapping triple components to numerical IDs.
This is done for three main reasons:
1) reduce storage space if triple components are stored multiple times;
2) reducing I/O overhead when retrieving data; and
3) simplify and optimize querying.
As our storage approach essentially stores each triple three or six times,
a dictionary can definitely reduce storage space requirements.</p>

      <p>Each delta chain consists of two dictionaries, one for the snapshot and one for the deltas.
The snapshot dictionary consists of triple components that already existed in the snapshot.
All other triple components are stored in the delta dictionary.
This dictionary is shared between the additions and deletions,
as the dictionary ignores whether or not the triple is an addition or deletion.
How this distinction is made will be explained in <a href="#storing_delta-storage">Subsubsection 3.6.3</a>.
The snapshot dictionary can be optimized and sorted, as it will not change over time.
The delta dictionary is volatile, as each new version can introduce new mappings.</p>

      <p>During triple encoding (i.e., ingestion), the snapshot dictionary will always first be probed for existence of the triple component.
If there is a match, that ID is used for storing the delta’s triple component.
To identify the appropriate dictionary for triple decoding,
a reserved bit is used where <code>1</code> indicates snapshot dictionary
and <code>0</code> indicates the delta dictionary.
The text-based dictionary values can be compressed to reduce storage space further, as they are likely to contain many redundancies.</p>

      <p><a href="#storing_example-delta-storage-dict">Table 8</a> contains example encodings of the triple components.</p>

      <figure id="storing_example-delta-storage-dict" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: right"><code>:Bob</code></th>
              <th style="text-align: right"><code>foaf:name</code></th>
              <th style="text-align: right"><code>"Bobby"</code></th>
              <th style="text-align: right"><code>:Alice</code></th>
              <th style="text-align: right"><code>"Alice"</code></th>
              <th style="text-align: right"><code>"Bob"</code></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: right"><code>S0</code></td>
              <td style="text-align: right"><code>S1</code></td>
              <td style="text-align: right"><code>S2</code></td>
              <td style="text-align: right"><code>D0</code></td>
              <td style="text-align: right"><code>D1</code></td>
              <td style="text-align: right"><code>D2</code></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 8:</span> Example encoding of the triple components from <a href="#storing_example-archive">Table 7</a>.
Instead of the reserved bit, IDs prefixed with <code>S</code> belong to the snapshot dictionary
and those prefixed with <code>D</code> belong to the delta dictionary.</p>
        </figcaption>
      </figure>

      <h4 id="storing_delta-storage">Delta Storage</h4>

      <p>In order to cope with the newly introduced redundancies in our delta chain structure,
we introduce a delta storage method similar to the <abbr title='Timestamp-based'>TB</abbr> storage strategy,
which is able to compress redundancies within consecutive deltas.
In contrast to a regular <abbr title='Timestamp-based'>TB</abbr> approach, which stores plain timestamped triples,
we store timestamped triples annotated with a flag for addition or deletion.
An overview of this storage technique is shown in <a href="#storing_delta-storage-overview">Fig. 37</a>,
which will be explained in detail hereafter.</p>

      <figure id="storing_delta-storage-overview">
<img src="storing/img/delta-storage-overview.svg" alt="[delta storage overview]" />
<figcaption>
          <p><span class="label">Fig. 37:</span> Overview of the components for storing a delta chain.
The value structure for the addition and deletion trees are indicated with the dashed nodes.</p>
        </figcaption>
</figure>

      <p>The additions and deletions of deltas require different metadata in our querying algorithms,
which will be explained in <a href="#storing_querying">Subsection 3.8</a>.
Additions and deletions are respectively stored in separate stores,
which hold all additions and deletions from the complete delta chain.
Each store uses B+Tree data structures,
where a key corresponds to a triple and the value contains version information.
The version information consists of a mapping from version to a local change flag as mentioned in <a href="#storing_local-changes">Subsubsection 3.5.3</a> and,
in case of deletions, also the relative position of the triple inside the delta.
Even though triples can exist in multiple deltas in the same chain,
they will only be stored once.
Each addition and deletion store uses three trees with a different triple component order (SPO, POS and OSP),
as discussed in <a href="#storing_indexes">Subsubsection 3.5.2</a>.</p>

      <p>The relative position of each triple inside the delta to the deletion trees speeds up the process
of patching a snapshot’s triple pattern subset for any given offset.
In fact, seven relative positions are stored for each deleted triple: one for each possible triple pattern (<code>SP?</code>, <code>S?O</code>, <code>S??</code>, <code>?PO</code>, <code>?P?</code>, <code>??O</code>, <code>???</code>),
except for <code>SPO</code> since this position will always be 0 as each triple is stored only once.
This position information serves two purposes:
1) it allows the querying algorithm to exploit offset capabilities of the snapshot store
to resolve offsets for any triple pattern against any version;
and 2) it allows deletion counts for any triple pattern and version to be determined efficiently.
The use of the relative position and the local change flag during querying will be further explained in <a href="#querying">Section 4</a>.</p>

      <figure id="storing_example-delta-storage" class="table">

        <table>
          <thead>
            <tr>
              <th>+</th>
              <th style="text-align: right">V</th>
              <th>L</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>D0 S1 D1</code></td>
              <td style="text-align: right">1</td>
              <td>F</td>
            </tr>
            <tr>
              <td> </td>
              <td style="text-align: right">3</td>
              <td>F</td>
            </tr>
            <tr>
              <td><code>S0 S1 D2</code></td>
              <td style="text-align: right">2</td>
              <td>F</td>
            </tr>
          </tbody>
        </table>

        <table>
          <thead>
            <tr>
              <th>-</th>
              <th style="text-align: right">V</th>
              <th>L</th>
              <th style="text-align: right"><code>SP?</code></th>
              <th style="text-align: right"><code>S?O</code></th>
              <th style="text-align: right"><code>S??</code></th>
              <th style="text-align: right"><code>?PO</code></th>
              <th style="text-align: right"><code>?P?</code></th>
              <th style="text-align: right"><code>??O</code></th>
              <th style="text-align: right"><code>???</code></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>D0 S1 D1</code></td>
              <td style="text-align: right">2</td>
              <td>T</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
            </tr>
            <tr>
              <td><code>S0 S1 S2</code></td>
              <td style="text-align: right">2</td>
              <td>F</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">1</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">1</td>
            </tr>
            <tr>
              <td> </td>
              <td style="text-align: right">3</td>
              <td>F</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
              <td style="text-align: right">0</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 9:</span> Addition and deletion tree contents based on the example from <a href="#storing_example-archive">Table 7</a> using the dictionary encoding from <a href="#storing_example-delta-storage-dict">Table 8</a>.
Column <code>+</code> and <code>-</code> respectively represent the keys of the addition and deletion trees, which contains triples based on the encoded triple components.
The remaining columns represent the values, i.e., a mapping from version (<code>V</code>) to the local change flag (<code>L</code>).
For the deletion trees, values also include the relative positions for all essential triple patterns.</p>
        </figcaption>
      </figure>

      <p><a href="#storing_example-delta-storage">Table 9</a> represent
the addition and deletion tree contents when the triples from the example in <a href="#storing_example-archive">Table 7</a> are stored.
The local change flag is enabled for <code>D0 S1 D1</code> in the deletions tree for version 2, as it was previously added in version 1.
The relative positions in the deletion tree for <code>S0 S1 S2</code> is not the same for versions 2 and 3,
because in version 2, the triple <code>D0 S1 D1</code> also exists as a deletion, and when sorted, this comes before <code>S0 S1 S2</code> for triple patterns <code>?P?</code> and <code>???</code>.</p>

      <h4 id="storing_addition-counts">Addition Counts</h4>

      <p>As mentioned before in <a href="#storing_addition-deletion-counts">Subsubsection 3.5.4</a>,
in order to make the counting of matching addition triples for any triple pattern for any version more efficient,
we propose to store an additional mapping from triple pattern and version to the number of matching additions.
Furthermore, for being able to retrieve the total number of additions across all versions,
we also propose to store this value for all triple patterns.
This mapping must be calculated during ingestion time, so that counts during lookup time for any triple pattern
at any version can be derived in constant time.
For many triples and versions, the number of possible triple patterns can become very large,
which can result in a large mapping store.
To cope with this, we propose to only store the elements where their counts are larger than a certain threshold.
Elements that are not stored will have to be counted during lookup time.
This is however not a problem for reasonably low thresholds,
because the iteration scope in our indexes can be limited efficiently, as mentioned in <a href="#storing_addition-deletion-counts">Subsubsection 3.5.4</a>.
The count threshold introduces a trade-off between the storage requirements and the required triple counting during lookups.</p>

      <h4 id="storing_deletion-counts">Deletion Counts</h4>

      <p>As mentioned in <a href="#storing_delta-storage">Subsubsection 3.6.3</a>, each deletion is annotated with its relative position in all deletions for that version.
This position is exploited to perform deletion counting for any triple pattern and version.
We look up the largest possible triple (sorted alphabetically) for the given triple pattern in the deletions tree,
which can be done in logarithmic time by navigating in the tree to the largest possible match for the given triple pattern.
If this does not result in a match for the triple pattern, no matches exist for the given triple pattern, and the count is zero.
Otherwise, we take one plus the relative position of the matched deletion for the given triple pattern.
Because we have queried the largest possible triple for that triple pattern in the given version,
this will be the last deletion in the list, so this position corresponds to the total number of deletions in that case.</p>

      <p>For example, when we want to determine the deletion count for <code>? foaf:name ?</code> (encoded: <code>? S1 ?</code>) in version 2
using the deletion tree contents from <a href="#storing_example-delta-storage">Table 9</a>,
we will find <code>S0 S1 S2</code> as largest triple in version 2.
This triple has relative position <code>1</code> for <code>?P?</code>, so the total deletion count is <code>2</code> for this pattern.
This is correct, as we have indeed two triples matching this pattern, namely <code>D0 S1 D1</code> and <code>S0 S1 S2</code>.</p>

      <h4 id="storing_metadata">Metadata</h4>

      <p>Querying algorithms have to be able to detect the total number of versions across all delta chains.
Therefore,
we must store metadata regarding the delta chain version ranges.
Assuming that version identifiers are numerical, a mapping can be maintained from version ID to delta chain.
Additionally, a counter of the total number of versions must be maintained for when the last version must be identified.</p>

    </section>

    <section id="storing_ingestions">
      <h3>Changeset Ingestion Algorithms</h3>

      <p>In this section, we discuss two ingestion algorithms: a memory-intensive batch algorithm and a memory-efficient streaming algorithm.
These algorithms both take a changeset—containing additions and deletions—as input,
and append it as a new version to the store.
Note that the ingested changesets are regular changesets: they are relative to one another according to <a href="#storing_regular-delta-chain">Fig. 34</a>.
Furthermore, we assume that the ingested changesets are <em>valid</em> changesets:
they don’t contain impossible triple sequences such as a triple that is removed in two versions without having an addition in between.
During ingestion, they will be transformed to the alternative delta chain structure as shown in <a href="#storing_alternative-delta-chain">Fig. 35</a>.
Within the scope of this article, we only discuss ingestion of deltas in a single delta chain following a snapshot.</p>

      <p>Next to ingesting the added and removed triples,
an ingestion algorithm for our storage approach must be able to calculate
the appropriate metadata for the store as discussed in <a href="#storing_delta-storage">Subsubsection 3.6.3</a>.
More specifically, an ingestion algorithm has the following requirements:</p>
      <ul>
    <li>addition triples must be stored in all addition trees;</li>
    <li>additions and deletions must be annotated with their version;</li>
    <li>additions and deletions must be annotated with being a local change or not;</li>
    <li>deletions must be annotated with their relative position for all triple patterns.</li>
</ul>

      <h4 id="storing_batch-ingestion">Batch Ingestion</h4>

      <p>Our first algorithm to ingest data into the store naively loads everything in memory,
and inserts the data accordingly.
The advantage of this algorithm is its simplicity and the possibility to do straightforward optimizations during ingestion.
The main disadvantage is the high memory consumption requirement for large versions.</p>

      <p>Before we discuss the actual batch ingestion algorithm,
we first introduce an in-memory changeset merging algorithm,
which is required for the batch ingestion.
<a href="#storing_algorithm-ingestion-batch-merge">Algorithm 2</a> contains the pseudocode of this algorithm.
First, all contents of the original changeset are copied into the new changeset (line 3).
After that, we iterate over all triples of the second changeset (line 4).
If the changeset already contained the given triple (line 5), the local change flag is negated.
Otherwise, the triple is added to the new changeset, and the local change flag is set to <code>false</code> (line 9,10).
Finally, in both cases the addition flag of the triple in the new changeset is copied from the second changeset (line 12).</p>

      <figure id="storing_algorithm-ingestion-batch-merge" class="algorithm numbered">
<pre><code>mergeChangesets(changesetOriginal, changesetIngest) {
</code><code>  changesetNew = new Changeset()
</code><code>  changesetNew.addAll(changesetOriginal)
</code><code>  for (triple : changesetIngest.getTriples()) {
</code><code>    if (changesetOriginal.contains(triple)) {
</code><code>      localChange = !changesetOriginal.isLocalChange(triple)
</code><code>      changesetNew.setLocalChange(triple, localChange)
</code><code>    } else {
</code><code>      changesetNew.add(triple)
</code><code>      changesetNew.setLocalChange(triple, false)
</code><code>    }
</code><code>    changesetNew.setAddition(triple, changesetIngest.isAddition(triple))
</code><code>  }
</code><code>  return changesetNew
</code><code>}</code></pre>
<figcaption>
          <p><span class="label">Algorithm 2:</span> In-memory changeset merging algorithm</p>
        </figcaption>
</figure>

      <p>Because our querying algorithms require the relative position of each deletion within a changeset to be stored,
we have to calculate these positions during ingestion.
We do this using the helper function <code>calculatePositions(triple)</code>.
This function depends on external mappings that persist over the duration of the ingestion phase
that map from triple to a counter for each possible triple pattern.
When this helper function is called for a certain triple,
we increment the counters for the seven possible triple patterns of the triple.
For the triple itself, we do not maintain a counter, as its value is always 1.
Finally, the function returns a mapping for the current counter values of the seven triple patterns.</p>

      <p>The batch ingestion algorithm starts by reading a complete changeset stream in-memory, sorting it in SPO order,
and encoding all triple components using the dictionary.
After that, it loads the changeset from the previous version in memory,
which is required for merging it together with the new changeset using the algorithm from <a href="#storing_algorithm-ingestion-batch-merge">Algorithm 2</a>.
After that, we have the new changeset loaded in memory.
Now, we load each added triple into the addition trees, together with their version and local change flag.
After that, we load each deleted triple into the deletion trees
with their version, local change flag and relative positions.
These positions are calculated using <code>calculatePositions(triple)</code>.
For the sake of completeness, we included the batch algorithm in pseudo-code in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-algorithms" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​algorithms">Appendix D</a>.</p>

      <p>Even though this algorithm is straightforward,
it can require a large amount of memory for large changesets and long delta chains.
The theoretical time complexity of this algorithm is <code>O(P + N log(N))</code> (<code>O(P + N)</code> if the new changeset is already sorted),
with <code>P</code> the number of triples in the previous changeset,
and <code>N</code> the number of triples in the new changeset.</p>

      <h4 id="storing_streaming-ingestion">Streaming Ingestion</h4>

      <p>Because of the unbounded memory requirements of the <a href="#storing_batch-ingestion">batch ingestion algorithm</a>,
we introduce a more complex streaming ingestion algorithm.
Just like the batch algorithm, it takes a changeset stream as input,
with the additional requirement that the stream’s values must be sorted in SPO-order.
This way the algorithm can assume a consistent order and act as a sort-merge join operation.
Just as for the batch algorithm, we included this algorithm in pseudo-code in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-algorithms" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​algorithms">Appendix D</a>.</p>

      <p>In summary, the algorithm performs a sort-merge join over three streams in SPO-order:
1) the stream of <em>input</em> changeset elements that are encoded using the dictionary when each element is read,
2) the existing <em>deletions</em> over all versions
and 3) the existing <em>additions</em> over all versions.
The algorithm iterates over all streams together, until all of them are finished.
The smallest triple (string-based) over all stream heads is handled in each iteration,
and can be categorized in seven different cases where these stream heads are indicated by <em>input</em>, <em>deletion</em> and <em>addition</em>, respectively:</p>

      <ol>
<li>
          <p><strong><em>Deletion</em> is strictly smaller than both <em>input</em> and <em>addition</em>.</strong>
<br />
The current deletion is the smallest element.
The unchanged deletion information can be copied to the new version.
New relative positions must be calculated in this and all other cases where deletions are added.</p>
        </li>
<li>
          <p><strong><em>Addition</em> is strictly smaller than both <em>input</em> and <em>deletion</em>.</strong>
<br />
Similar to the previous case, the current addition is now the smallest element,
and its information can be copied to the new version.</p>
        </li>
<li>
          <p><strong><em>Input</em> is strictly smaller than both <em>addition</em> and <em>deletion</em>.</strong>
<br />
A triple is added or removed that was not present before,
so it can respectively be added as a non-local change addition or a non-local change deletion.</p>
        </li>
<li>
          <p><strong><em>Input</em> and <em>deletion</em> are equal, but strictly smaller than <em>addition</em>.</strong>
<br />
In this case, the new triple already existed in the previous version as a deletion.
If the new triple is an addition, it must be added as a local change.</p>
        </li>
<li>
          <p><strong><em>Input</em> and <em>addition</em> are equal, but strictly smaller than <em>deletion</em>.</strong>
<br />
Similar as in the previous case, the new triple now already existed as an addition.
So the triple must be deleted as a local change if the new triple is a deletion.</p>
        </li>
<li>
          <p><strong><em>Addition</em> and <em>deletion</em> are equal, but strictly smaller than <em>input</em>.</strong>
<br />
The triple existed as both an addition and deletion at some point.
In this case, we copy over the one that existed at the latest version, as it will still apply in the new version.</p>
        </li>
<li>
          <p><strong><em>Addition</em>,  <em>deletion</em>, and <em>input</em> are equal.</strong>
<br />
Finally, the triple already existed as both an addition and deletion,
and is equal to our new triple.
This means that if the triple was an addition in the previous version, it becomes a deletion, and the other way around,
and the local change flag can be inherited.</p>
        </li>
</ol>

      <p>The theoretical memory requirement for this algorithm is much lower than the <a href="#storing_batch-ingestion">batch variant</a>.
That is because it only has to load at least three triples, i.e., the heads of each stream, in memory, instead of the complete new changeset.
Furthermore, we still need to maintain the relative position counters for the deletions in all triple patterns.
While these counters could also become large, a smart implementation could perform memory-mapping
to avoid storing everything in memory.
The lower memory requirements come at the cost of a higher logical complexity, but an equal time complexity (assuming sorted changesets).</p>

    </section>

    <section id="storing_querying">
      <h3>Versioned Query Algorithms</h3>

      <p>In this section, we introduce algorithms for performing <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> triple pattern queries
based on the storage structure introduced in <a href="#storing_storage">Subsection 3.6</a>.
Each of these querying algorithms are based on result streams, enabling efficient offsets and limits,
by exploiting the index structure from <a href="#storing_storage">Subsection 3.6</a>.
Furthermore, we provide algorithms to provide count estimates for each query.</p>

      <h4 id="version-materialization">Version Materialization</h4>

      <p>Version Materialization (<abbr title='Version materialization'>VM</abbr>) is the most straightforward versioned query type,
it allows you to query against a certain dataset version.
In the following, we start by introducing our <abbr title='Version materialization'>VM</abbr> querying algorithm,
after we give a simple example of this algorithm.
After that, we prove the correctness of our <abbr title='Version materialization'>VM</abbr> algorithm and introduce a corresponding algorithm to provide count estimation for <abbr title='Version materialization'>VM</abbr> query results.</p>

      <h5 id="query">Query</h5>

      <p><a href="#storing_algorithm-querying-vm">Algorithm 3</a> introduces an algorithm for <abbr title='Version materialization'>VM</abbr> triple pattern queries based on our storage structure.
It starts by determining the snapshot on which the given version is based (line 2).
After that, this snapshot is queried for the given triple pattern and offset.
If the given version is equal to the snapshot version, the snapshot iterator can be returned directly (line 3).
In all other cases, this snapshot offset could only be an estimation,
and the actual snapshot offset can be larger if deletions were introduced before the actual offset.</p>

      <p>Our algorithm returns a stream where triples originating from the snapshot always
come before the triples that were added in later additions.
Because of that, the mechanism for determining the correct offset in the
snapshot, additions and deletions streams can be split up into two cases.
The given offset lies within the range of either snapshot minus deletion triples or within the range of addition triples.
At this point, the additions and deletions streams are initialized to the start position for the given triple pattern and version.</p>

      <figure id="storing_algorithm-querying-vm" class="algorithm numbered">
<pre><code>queryVm(store, tp, version, originalOffset) {
</code><code>  snapshot = store.getSnapshot(version).query(tp, originalOffset)
</code><code>  if (snapshot.getVersion() = version) {
</code><code>    return snapshot
</code><code>  }
</code><code>  
</code><code>  additions = store.getAdditionsStream(tp, version)
</code><code>  deletions = store.getDeletionStream(tp, version)
</code><code>  offset = 0
</code><code>  
</code><code>  if (originalOffset &lt; snapshot.count(tp) - deletions.exactCount(tp)) {
</code><code>    do {
</code><code>      snapshot.offset(originalOffset + offset)
</code><code>      offsetTriple = snapshot.peek()
</code><code>      deletions = store.getDeletionsStream(tp, version, offsetTriple)
</code><code>      offset = deletions.getOffset(tp)
</code><code>    } while (snapshot.getCurrentOffset() != originalOffset + offset)
</code><code>  }
</code><code>  else {
</code><code>    snapshot.offset(snapshot.count(tp))
</code><code>    additions.offset(originalOffset - snapshot.count(tp)
</code><code>        + deletions.exactCount(tp))
</code><code>  }
</code><code>  
</code><code>  return PatchedSnapshotIterator(snapshot, deletions, additions)
</code><code>}</code></pre>
<figcaption>
          <p><span class="label">Algorithm 3:</span> Version Materialization algorithm for triple patterns that produces a triple stream with an offset in a given version.</p>
        </figcaption>
</figure>

      <p>In the first case, when the offset lies within the snapshot and deletions range (line 11),
we enter a loop that converges to the actual snapshot offset based on the deletions
for the given triple pattern in the given version.
This loop starts by determining the triple at the current offset position in the snapshot (line 13, 14).
We then query the deletions tree for the given triple pattern and version (line 15),
filter out local changes, and use the snapshot triple as offset.
This triple-based offset is done by navigating through the tree to the smallest triple before or equal to the offset triple.
We store an additional offset value (line 16), which corresponds to the current numerical offset inside the deletions stream.
As long as the current snapshot offset is different from the sum of the original offset and the additional offset,
we continue iterating this loop (line 17), which will continuously increase this additional offset value.</p>

      <p>In the second case (line 19), the given offset lies within the additions range.
Now, we terminate the snapshot stream by offsetting it after its last element (line 20),
and we relatively offset the additions stream (line 21).
This offset is calculated as the original offset subtracted with the number of snapshot triples incremented with the number of deletions.</p>

      <p>Finally, we return a simple iterator starting from the three streams (line 25).
This iterator performs a sort-merge join operation that removes each triple from the snapshot that also appears in the deletion stream,
which can be done efficiently because of the consistent <code>SPO</code>-ordering.
Once the snapshot and deletion streams have finished,
the iterator will start emitting addition triples at the end of the stream.
For all streams, local changes are filtered out because locally changed triples
are cancelled out for the given version as explained in <a href="#storing_local-changes">Subsubsection 3.5.3</a>,
so they should not be returned in materialized versions.</p>

      <h5 id="example">Example</h5>

      <p>We can use the deletion’s position in the delta as offset in the snapshot
because this position represents the number of deletions that came before that triple inside the snapshot given a consistent triple order.
<a href="#storing_query-vm-example">Table 10</a> shows simplified storage contents where triples are represented as a single letter,
and there is only a single snapshot and delta.
In the following paragraphs, we explain the offset convergence loop of the algorithm in function of this data for different offsets.</p>

      <figure id="storing_query-vm-example" class="table">

        <table>
          <thead>
            <tr>
              <th>Snapshot</th>
              <th>A</th>
              <th>B</th>
              <th>C</th>
              <th>D</th>
              <th>E</th>
              <th>F</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Deletions</td>
              <td> </td>
              <td>B</td>
              <td> </td>
              <td>D</td>
              <td>E</td>
              <td> </td>
            </tr>
            <tr>
              <td>Positions</td>
              <td> </td>
              <td>0</td>
              <td> </td>
              <td>1</td>
              <td>2</td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 10:</span> Simplified storage contents example where triples are represented as a single letter.
The snapshot contains six elements, and the next version contains three deletions.
Each deletion is annotated with its position.</p>
        </figcaption>
      </figure>

      <h6 id="offset-0"><em>Offset 0</em></h6>
      <p>For offset zero, the snapshot is first queried for this offset,
which results in a stream starting from <code>A</code>.
Next, the deletions are queried with offset <code>A</code>, which results in no match,
so the final snapshot stream starts from <code>A</code>.</p>

      <h6 id="offset-1"><em>Offset 1</em></h6>
      <p>For an offset of one, the snapshot stream initially starts from <code>B</code>.
After that, the deletions stream is offset to <code>B</code>, which results in a match.
The original offset (1), is increased with the position of <code>B</code> (0) and the constant 1,
which results in a new snapshot offset of 2.
We now apply this new snapshot offset.
As the snapshot offset has changed, we enter a second iteration of the loop.
Now, the head of the snapshot stream is <code>C</code>.
We offset the deletions stream to <code>C</code>, which again results in <code>B</code>.
As this offset results in the same snapshot offset,
we stop iterating and use the snapshot stream with offset 2 starting from <code>C</code>.</p>

      <h6 id="offset-2"><em>Offset 2</em></h6>
      <p>For offset 2, the snapshot stream initially starts from <code>C</code>.
After querying the deletions stream, we find <code>B</code>, with position 0.
We update the snapshot offset to 2 + 0 + 1 = 3,
which results in the snapshot stream with head <code>D</code>.
Querying the deletions stream results in <code>D</code> with position 1.
We now update the snapshot offset to 2 + 1 + 1 = 4, resulting in a stream with head <code>E</code>.
We query the deletions again, resulting in <code>E</code> with position 2.
Finally, we update the snapshot offset to 2 + 2 + 1 = 5 with stream head <code>F</code>.
Querying the deletions results in the same <code>E</code> element,
so we use this last offset in our final snapshot stream.</p>

      <h5 id="estimated-count">Estimated count</h5>

      <p>In order to provide an estimated count for <abbr title='Version materialization'>VM</abbr> triple pattern queries,
we introduce a straightforward algorithm that depends on the efficiency of the snapshot to provide count estimations for a given triple pattern.
Based on the snapshot count for a given triple pattern, the number of deletions for that version and triple pattern
are subtracted and the number of additions are added.
These last two can be resolved efficiently, as we precalculate
and store expensive addition and deletion counts as explained in <a href="#storing_addition-counts">Subsubsection 3.6.4</a> and <a href="#storing_deletion-counts">Subsubsection 3.6.5</a>.</p>

      <h5 id="correctness">Correctness</h5>

      <p>In this section, we provide a proof that <a href="#storing_algorithm-querying-vm">Algorithm 3</a> results in the correct stream offset
for any given version and triple pattern. We do this by first introducing a set of notations,
followed by several lemmas and corollaries, which lead up to our final theorem proof.</p>

      <p><strong>Notations</strong>:</p>

      <p>We will make use of bracket notation to indicate lists (ordered sets):</p>

      <ul>
        <li><code>A[i]</code> is the element at position <code>i</code> from the list <code>A</code>.</li>
        <li><code>A + B</code> is the concatenation of list <code>A</code> followed by list <code>B</code>.</li>
      </ul>

      <p>Furthermore, we will use the following definitions:</p>

      <ul>
        <li><code>snapshot(tp, version)</code> is the ordered list of triples matching the given triple pattern <code>tp</code> in the corresponding snapshot, from here on shortened to <code>snapshot</code>.</li>
        <li><code>additions(version)</code> and <code>deletions(version)</code> are the corresponding ordered additions and deletions for the given version, from here on shortened to <code>additions</code> and <code>deletions</code>.</li>
        <li><code>originalOffset</code> is how much the versioned list should be shifted, from here on shortened to <code>ori</code>.</li>
        <li><code>PatchedSnapshotIterator(snapshot, deletions, additions)</code> is a function that returns the list <code>snapshot\deletions + additions</code>.</li>
      </ul>

      <p>The following definitions correspond to elements from the loop on lines 12-17:</p>

      <ul>
        <li><code>deletions(x)</code> is the ordered list <code>{d | d ∈ deletions, d ≥ x}</code>, with <code>x</code> a triple.</li>
        <li><code>offset(x) = |deletions| - |deletions(x)|</code>, with <code>x</code> a triple.</li>
        <li><code>t(i)</code> is the triple generated at line 13-14 for iteration <code>i</code>.</li>
        <li><code>off(i)</code> is the offset generated at line 16 for iteration <code>i</code>.</li>
      </ul>

      <p><strong>Lemma 1</strong>: <code>off(n) ≥ off(n-1)</code><br />
<em>Proof</em>:<br />
We prove this by induction over the iterations of the loop.
For <code>n=1</code> this follows from line 9 and <code>∀ x offset(x) ≥ 0.</code></p>

      <p>For <code>n+1</code> we know by induction that <code>off(n) ≥ off(n-1)</code>.
Since <code>snapshot</code> is ordered, <code>snapshot[ori + off(n)] ≥ snapshot[ori + off(n-1)]</code>.
From lines 13-14 follows that <code>t(n) = snapshot[ori + off(n-1)]</code>,
together this gives <code>t(n+1) ≥ t(n)</code>.</p>

      <p>From this, we get:</p>

      <ul>
        <li><code>{d | d ∈ deletions, d ≥ t(n+1)} ⊆ {d | d ∈ deletions, d ≥ t(n)}</code></li>
        <li><code>deletions(t(n+1)) ⊆ deletions(t(n))</code></li>
        <li><code>|deletions(t(n+1))| ≤ |deletions(t(n))|</code></li>
        <li><code>|deletions| - |deletions(t(n+1))| ≥ |deletions| - |deletions(t(n))|</code></li>
        <li><code>offset(t(n+1)) ≥ offset(t(n))</code></li>
      </ul>

      <p>Together with lines 15-16 this gives us <code>off(n+1) ≥ off(n)</code>.</p>

      <p><strong>Corollary 1</strong>: The loop on lines 12-17 always terminates.<br />
<em>Proof</em>:<br />
Following the definitions, the end condition of the loop is <code>ori + off(n) = ori + off(n+1)</code>.
From Lemma 1 we know that <code>off</code> is a non-decreasing function.
Since <code>deletions</code> is a finite list of triples, there is an upper limit for <code>off</code> (<code>|deletions|</code>),
causing <code>off</code> to stop increasing at some point which triggers the end condition.</p>

      <p><strong>Corollary 2</strong>: When the loop on lines 12-17 terminates, <code>offset = |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> and <code>ori + offset &lt; |snapshot|</code><br />
<em>Proof</em>:<br />
The first part follows from the definition of <code>deletions</code> and <code>offset</code>.
The second part follows from <code>offset ≤ |deletions|</code> and line 11.</p>

      <p><strong>Theorem 1</strong>: queryVm returns a sublist of <code>(snapshot\deletions + additions)</code>, starting at the given offset.<br />
<em>Proof</em>:<br />
If the given version is equal to a snapshot, there are no additions or deletions so this follows directly from lines 2-4.</p>

      <p>Following the definition of <code>deletions</code>, <code>∀ x ∈ deletions: x ∈ snapshot</code> and thus <code>|snapshot\deletions| = |snapshot| - |deletions|</code>.</p>

      <p>Due to the ordered nature of <code>snapshot</code> and <code>deletions</code>, if <code>ori &lt; |snapshot\deletions|</code>, version<code>[ori] = snapshot[ori + |D|]</code> with <code>D = {d | d ∈ deletions, d &lt; snapshot[ori + |D|]}</code>.
Due to <code>|snapshot\deletions| = |snapshot| - |deletions|</code>, this corresponds to the if-statement on line 11.
From Corollary 1 we know that the loop terminates
and from Corollary 2 and line 13 that snapshot points to the element at position
<code>ori + |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> which,
together with <code>additions</code> starting at index 0 and line 25,
returns the requested result.</p>

      <p>If <code>ori ≥ |snapshot\deletions|</code>, <code>version[ori] = additions[ori - |snapshot\deletions|]</code>.
From lines 20-22 follows that <code>snapshot</code> gets emptied and <code>additions</code> gets shifted for the remaining required elements <code>(ori - |snapshot\deletions|)</code>, which then also returns the requested result on line 25.</p>

      <h4 id="delta-materialization">Delta Materialization</h4>

      <p>The goal of delta materialization (<abbr title='Delta materialization'>DM</abbr>) queries is to query the triple differences between two versions.
Furthermore, each triple in the result stream is annotated with either being an addition or deletion between the given version range.
Within the scope of this work, we limit ourselves to delta materialization within a single snapshot and delta chain.
Because of this, we distinguish between two different cases for our <abbr title='Delta materialization'>DM</abbr> algorithm
in which we can query triple patterns between a start and end version,
the start version of the query can either correspond to the snapshot version or it can come after that.
Furthermore, we introduce an equivalent algorithm for estimating the number of results for these queries.</p>

      <h5 id="query-1">Query</h5>

      <p>For the first query case, where the start version corresponds to the snapshot version,
the algorithm is straightforward.
Since we always store our deltas relative to the snapshot,
filtering the delta of the given end version based on the given triple pattern directly corresponds to the desired result stream.
Furthermore, we filter out local changes, as we are only interested in actual change with respect to the snapshot.</p>

      <p>For the second case, the start version does not correspond to the snapshot version.
The algorithm iterates over the triple pattern iteration scope of the addition and deletion trees in a sort-merge join-like operation,
and only emits the triples that have a different addition/deletion flag for the two versions.</p>

      <h5 id="estimated-count-1">Estimated count</h5>

      <p>For the first case, the start version corresponds to the snapshot version.
The estimated number of results is then the number of snapshot triples for the pattern summed up with the exact umber of deletions and additions for the pattern.</p>

      <p>In the second case the start version does not correspond to the snapshot version.
We estimate the total count as the sum of the additions and deletions for the given triple pattern in both versions.
This may only be a rough estimate, but will always be an upper bound, as the triples that were changed twice within the version range and negate each other
are also counted.
For exact counting, this number of negated triples should be subtracted.</p>

      <h4 id="version-query">Version Query</h4>

      <p>For version querying (<abbr title='Version query'>VQ</abbr>), the final query atom, we have to retrieve all triples across all versions,
annotated with the versions in which they exist.
In this work, we again focus on version queries for a single snapshot and delta chain.
For multiple snapshots and delta chains, the following algorithms can simply be applied once for each snapshot and delta chain.
In the following sections, we introduce an algorithm for performing triple pattern version queries
and an algorithm for estimating the total number of matching triples for the former queries.</p>

      <h5 id="query-2">Query</h5>

      <p>Our version querying algorithm is again based on a sort-merge join-like operation.
We start by iterating over the snapshot for the given triple pattern.
Each snapshot triple is queried within the deletion tree.
If such a deletion value can be found, the versions annotation contains all versions except for the versions
for which the given triple was deleted with respect to the given snapshot.
If no such deletion value was found, the triple was never deleted,
so the versions annotation simply contains all versions of the store.
Result stream offsetting can happen efficiently as long as the snapshot allows efficient offsets.
When the snapshot iterator is finished, we iterate over the addition tree in a similar way.
Each addition triple is again queried within the deletions tree
and the versions annotation can equivalently be derived.</p>

      <h5 id="estimated-count-2">Estimated count</h5>

      <p>Calculating the number of unique triples matching any triple pattern version query is trivial.
We simply retrieve the count for the given triple pattern in the given snapshot
and add the number of additions for the given triple pattern over all versions.
The number of deletions should not be taken into account here,
as this information is only required for determining the version annotation in the version query results.</p>

    </section>

    <section id="storing_evaluation">
      <h3>Evaluation</h3>

      <p>In this section, we evaluate our proposed storage technique and querying algorithms.
We start by introducing <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, an implementation of our proposed solution.
After that, we describe the setup of our experiments, followed by presenting our results.
Finally, we discuss these results.</p>

      <h4 id="storing_implementation">Implementation</h4>

      <p><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> stands for <em>Offset-enabled STore for TRIple CHangesets</em>,
and it is a software implementation of the storage and querying techniques described in this article
It is implemented in C/C++ and available on <a href="https://zenodo.org/record/883008" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883008">GitHub</a> under an open license.
In the scope of this work, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> currently supports a single snapshot and delta chain.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> uses <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr></a> <span class="references">[<a href="#ref-54">54</a>]</span> as snapshot technology as it conforms to all the <a href="#storing_snapshot-storage">requirements</a> for our approach.
Furthermore, for our indexes we use <a href="http://fallabs.com/kyotocabinet/" class="mandatory" data-link-text="http:/​/​fallabs.com/​kyotocabinet/​">Kyoto Cabinet</a>,
which provides a highly efficient memory-mapped B+Tree implementation with compression support.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> immediately generates the main <code>SPO</code> index and the auxiliary <code>OSP</code> and <code>POS</code> indexes.
In future work, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> could be modified to only generate the main index and delay auxiliary index generation to a later stage.
Memory-mapping is required so that not all data must be loaded in-memory when queries are evaluated,
which would not always be possible for large datasets.
For our delta dictionary, we extend <abbr title='Header Dictionary Triples'>HDT</abbr>’s dictionary implementation with adjustments to make it work with unsorted triple components.
We compress this delta dictionary with <a href="http://www.gzip.org/">gzip</a>, which requires decompression during querying and ingestion.
Finally, for storing our addition counts, we use the Hash Database of Kyoto Cabinet, which is also memory-mapped.</p>

      <p>We provide a developer-friendly C/C++ <abbr title='Application Programming Interface'>API</abbr> for ingesting and querying data based on an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store.
Additionally, we provide command-line tools for ingesting data into an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store,
or evaluating <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> or <abbr title='Version query'>VQ</abbr> triple pattern queries for any given limit and offset against a store.
Furthermore, we implemented <a href="https://zenodo.org/record/883010" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883010">Node JavaScript bindings</a> that
expose the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> <abbr title='Application Programming Interface'>API</abbr> for ingesting and querying to JavaScript applications.
We used these bindings to <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">expose an <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store</a>
containing a dataset with 30M triples in 10 versions using <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><abbr title='Triple Pattern Fragments'>TPF</abbr></a></span> <span class="references">[<a href="#ref-39">39</a>]</span>, with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr> feature</a> <span class="references">[<a href="#ref-41">41</a>]</span>.</p>

      <h4 id="experimental-setup">Experimental Setup</h4>

      <p>As mentioned before in <a href="#storing_related-work-benchmarks"></a>, we evaluate our approach using the <abbr title='Benchmark of rdf Archives'>BEAR</abbr> benchmark.
We chose for this benchmark because it provides a complete set of tools and data for benchmarking <abbr title='Resource Description Framework'>RDF</abbr> versioning systems,
containing datasets, queries and easy-to-use engines to compare with.</p>

      <p>We extended the existing <abbr title='Benchmark of rdf Archives'>BEAR</abbr> implementation for the evaluation of offsets.
We did this by implementing custom offset features into each of the <abbr title='Benchmark of rdf Archives'>BEAR</abbr> approaches.
Only for <abbr title='Version materialization'>VM</abbr> queries in <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> an efficient implementation (<abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>+) could be made because of <abbr title='Header Dictionary Triples'>HDT</abbr>’s native offset capabilities.
In all other cases, naive offsets had to be implemented by iterating over the result stream
until a number of elements equal to the desired offset were consumed.
This modified implementation is available on <a href="https://github.com/rdfostrich/bear/tree/ostrich-eval-journal" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​bear/​tree/​ostrich-​eval-​journal">GitHub</a>.
To test the scalability of our approach for datasets with few and large versions, we use the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A benchmark.
We use the ten first versions of the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset, which contains 30M to 66M triples per version.
This dataset was compiled from the <a href="http://swse.deri.org/dyldo/">Dynamic Linked Data Observatory</a>.
To test for datasets with many smaller versions, we use <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B with the daily and hourly granularities.
The daily dataset contains 89 versions and the hourly dataset contains 1,299 versions,
both of them have around 48K triples per version.
We did not evaluate <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-instant, because <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires increasingly
more time for each new version ingestion, as will be shown in the next section.
As <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly with 1,299 versions already takes more than three days to ingest,
the 21,046 versions from <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-instant would require too much time to ingest.
Our experiments were executed on a 64-bit
Ubuntu 14.04 machine with 128 GB of memory and a
24-core 2.40 GHz CPU.</p>

      <p>For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, we use all 7 of the provided querysets, each containing at most 50 triple pattern queries,
once with a high result cardinality and once with a low result cardinality.
These querysets correspond to all possible triple pattern materializations, except for triple patterns where each component is blank.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B, only two querysets are provided, those that correspond to <code>?P?</code> and <code>?PO</code> queries.
The number of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B queries is more limited, but they are derived from real-world DBpedia queries
which makes them useful for testing real-world applicability.
All of these queries are evaluated as <abbr title='Version materialization'>VM</abbr> queries on all versions,
as <abbr title='Delta materialization'>DM</abbr> between the first version and all other versions,
and as <abbr title='Version query'>VQ</abbr>.</p>

      <p>For a complete comparison with other approaches, we re-evaluated <abbr title='Benchmark of rdf Archives'>BEAR</abbr>’s Jena and <abbr title='Header Dictionary Triples'>HDT</abbr>-based <abbr title='Resource Description Framework'>RDF</abbr> archive implementations.
More specifically, we ran all <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A queries against Jena with the <abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr>, <abbr title='Timestamp-based'>TB</abbr> and hybrid <abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> implementation,
and <abbr title='Header Dictionary Triples'>HDT</abbr> with the <abbr title='Independent copies'>IC</abbr> and <abbr title='Change-based'>CB</abbr> implementations
using the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset for ten versions.
We did the same for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B with the daily and hourly dataset.
After that, we evaluated <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> for the same queries and datasets.
We were not able to extend this benchmark with other similar systems such as X-<abbr title='Resource Description Framework'>RDF</abbr>-3X, <abbr title='Resource Description Framework'>RDF</abbr>-TX and Dydra,
because the source code of systems was either not publicly available,
or the system would require additional implementation work to support the required query interfaces.</p>

      <p>Additionally, we evaluated the ingestion rates and storage sizes for all approaches.
Furthermore, we compared the ingestion rate for the two different ingestion algorithms of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
The batch-based algorithm expectedly ran out of memory for larger amounts of versions,
so we used the streaming-based algorithm for all further evaluations.</p>

      <p>Finally, we evaluated the offset capabilities of <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>
by comparing it with custom offset implementations for the other approaches.
We evaluated the blank triple pattern query with offsets ranging from 2 to 4,096 with a limit of 10 results.</p>

      <h4 id="results-3">Results</h4>

      <p>In this section, we present the results of our evaluation.
We report the ingestion results, compressibility, query evaluation times for all cases and offset result.
All raw results and the scripts that were used to process them are available on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.</p>

      <h5 id="ingestion">Ingestion</h5>

      <p><a href="#storing_results-ingestion-size">Table 11</a> and <a href="#storing_results-ingestion-time">Table 12</a>
respectively show the storage requirements and ingestion times for the different approaches for the three different benchmarks.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, the <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches outperform <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in terms of ingestion time, they are about two orders of magniture faster.
Only <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> requires slightly less storage space.
The Jena-based approaches ingest one order of magnitude faster than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, but require more storage space.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires less storage space than all other approaches except for <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> at the cost of slower ingestion.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly, only <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> and Jena-<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> require about 8 to 4 times less space than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> even requires less storage space than gzip on raw N-Triples.</p>

      <p>As mentioned in <a href="#storing_addition-counts">Subsubsection 3.6.4</a>, we use a threshold to define which addition count values should be stored,
and which ones should be evaluated at query time.
For our experiments, we fixed this count threshold at 200,
which has been empirically determined through various experiments as a good value.
For values higher than 200, the addition counts started having a noticable impact on the performance of count estimation.
This threshold value means that when a triple pattern has 200 matching additions,
then this count will be stored.
<a href="#storing_results-addition-counts">Table 13</a> shows that the storage space of the addition count datastructure
in the case of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly is insignificant compared to the total space requirements.
However, for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily, addition counts take up 37.05% of the total size with still an acceptable absolute size,
as the addition and deletion trees require relatively less space,
because of the lower amount of versions.
Within the scope of this work, we use this fixed threshold of 200.
We consider investigating the impact of different threshold levels and methods for dynamically determining optimal levels future work.</p>

      <p><a href="#storing_results-ostrich-ingestion-rate-beara">Fig. 38</a> shows linearly increasing ingestion rate for each consecutive version for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A,
while <a href="#storing_results-ostrich-ingestion-size-beara">Fig. 39</a> shows corresponding linearly increasing storage sizes.
Analogously, <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 40</a> shows the ingestion rate for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly,
which increases linearly until around version 1100, after which it increases significantly.
<a href="#storing_results-ostrich-ingestion-size-bearb-hourly">Fig. 41</a> shows near-linearly increasing storage sizes.</p>

      <p><a href="#storing_results-ostrich-ingestion-rate-beara-compare">Fig. 42</a> compares the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A ingestion rate of the streaming and batch algorithms.
The streaming algorithm starts of slower than the batch algorithm but grows linearly,
while the batch algorithm consumes a large amount of memory, resulting in slower ingestion after version 8 and an out-of-memory error after version 10.</p>

      <figure id="storing_results-ingestion-size" class="table">

        <table>
          <thead>
            <tr>
              <th>Approach</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Raw (N-Triples)</td>
              <td style="text-align: right">46,069.76</td>
              <td style="text-align: right">556.44</td>
              <td style="text-align: right">8,314.86</td>
            </tr>
            <tr>
              <td>Raw (gzip)</td>
              <td style="text-align: right">3,194.88</td>
              <td style="text-align: right">30.98</td>
              <td style="text-align: right">466.35</td>
            </tr>
            <tr>
              <td><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr></td>
              <td style="text-align: right">3,102.72</td>
              <td style="text-align: right">12.32</td>
              <td style="text-align: right">187.46</td>
            </tr>
            <tr>
              <td> </td>
              <td style="text-align: right">+1,484.80</td>
              <td style="text-align: right">+4.55</td>
              <td style="text-align: right">+263.13</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Independent copies'>IC</abbr></td>
              <td style="text-align: right">32,808.96</td>
              <td style="text-align: right">415.32</td>
              <td style="text-align: right">6,233.92</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Change-based'>CB</abbr></td>
              <td style="text-align: right">18,216.96</td>
              <td style="text-align: right">42.82</td>
              <td style="text-align: right">473.41</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Timestamp-based'>TB</abbr></td>
              <td style="text-align: right">82,278.4</td>
              <td style="text-align: right">23.61</td>
              <td style="text-align: right">3,678.89</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr></td>
              <td style="text-align: right">31,160.32</td>
              <td style="text-align: right">22.83</td>
              <td style="text-align: right">53.84</td>
            </tr>
            <tr>
              <td><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr></td>
              <td style="text-align: right">5,335.04</td>
              <td style="text-align: right">142.08</td>
              <td style="text-align: right">2,127.57</td>
            </tr>
            <tr>
              <td> </td>
              <td style="text-align: right">+1,494.69</td>
              <td style="text-align: right">+6.53</td>
              <td style="text-align: right">+98.88</td>
            </tr>
            <tr>
              <td><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr></td>
              <td style="text-align: right"><em>2,682.88</em></td>
              <td style="text-align: right"><em>5.96</em></td>
              <td style="text-align: right"><em>24.39</em></td>
            </tr>
            <tr>
              <td> </td>
              <td style="text-align: right">+802.55</td>
              <td style="text-align: right">+0.25</td>
              <td style="text-align: right">+0.75</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 11:</span> Storage sizes for each of the <abbr title='Resource Description Framework'>RDF</abbr> archive approaches in MB with <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
The additional storage size for the auxiliary <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='Header Dictionary Triples'>HDT</abbr> indexes are provided as separate rows.
The lowest sizes per dataset are indicated in italics.</p>
        </figcaption>
      </figure>

      <figure id="storing_results-ingestion-time" class="table">

        <table>
          <thead>
            <tr>
              <th>Approach</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr></td>
              <td style="text-align: right">2,256</td>
              <td style="text-align: right">12.36</td>
              <td style="text-align: right">4,497.32</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Independent copies'>IC</abbr></td>
              <td style="text-align: right">443</td>
              <td style="text-align: right">8.91</td>
              <td style="text-align: right">142.26</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Change-based'>CB</abbr></td>
              <td style="text-align: right">226</td>
              <td style="text-align: right">9.53</td>
              <td style="text-align: right">173.48</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Timestamp-based'>TB</abbr></td>
              <td style="text-align: right">1,746</td>
              <td style="text-align: right">0.35</td>
              <td style="text-align: right">70.56</td>
            </tr>
            <tr>
              <td>Jena-<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr></td>
              <td style="text-align: right">679</td>
              <td style="text-align: right">0.35</td>
              <td style="text-align: right">0.65</td>
            </tr>
            <tr>
              <td><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr></td>
              <td style="text-align: right">34</td>
              <td style="text-align: right">0.39</td>
              <td style="text-align: right">5.89</td>
            </tr>
            <tr>
              <td><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr></td>
              <td style="text-align: right"><em>18</em></td>
              <td style="text-align: right"><em>0.02</em></td>
              <td style="text-align: right"><em>0.07</em></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 12:</span> Ingestion times for each of the <abbr title='Resource Description Framework'>RDF</abbr> archive approaches with <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
The lowest times per dataset are indicated in italics.</p>
        </figcaption>
      </figure>

      <figure id="storing_results-addition-counts" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily</th>
              <th style="text-align: right"><abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: right">13.69 (0.29%)</td>
              <td style="text-align: right">6.25 (37.05%)</td>
              <td style="text-align: right">15.62 (3.46%)</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 13:</span> Storage sizes of the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> addition count component in MB with <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
The percentage of storage space that this component requires compared to the complete store is indicated between brackets.</p>
        </figcaption>
      </figure>

      <figure id="storing_results-ostrich-ingestion-rate-beara">
<img src="storing/img/results-ostrich-ingestion-rate-beara.svg" alt="[bear-a ostrich ingestion rate]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 38:</span> <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingestion durations for each consecutive <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A version in minutes for an increasing number of versions,
showing a lineair growth.</p>
        </figcaption>
</figure>

      <figure id="storing_results-ostrich-ingestion-size-beara">
<img src="storing/img/results-ostrich-ingestion-size-beara.svg" alt="[bear-a ostrich ingestion sizes]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 39:</span> Cumulative <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store sizes for each consecutive <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A version in GB for an increasing number of versions,
showing a lineair growth.</p>
        </figcaption>
</figure>

      <figure id="storing_results-ostrich-ingestion-rate-bearb-hourly">
<img src="storing/img/results-ostrich-ingestion-rate-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion rate]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 40:</span> <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingestion durations for each consecutive <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly version in minutes for an increasing number of versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-ostrich-ingestion-size-bearb-hourly">
<img src="storing/img/results-ostrich-ingestion-size-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion sizes]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 41:</span> Cumulative <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> store sizes for each consecutive <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly version in GB for an increasing number of versions.</p>
        </figcaption>
</figure>

      <figure id="results-ostrich-compressability" class="table">

        <table>
          <thead>
            <tr>
              <th>Format</th>
              <th>Dataset</th>
              <th style="text-align: right">Size</th>
              <th style="text-align: right">gzip</th>
              <th style="text-align: right">Savings</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>N-Triples</strong></td>
              <td>A</td>
              <td style="text-align: right">46,069.76</td>
              <td style="text-align: right">3,194.88</td>
              <td style="text-align: right">93.07%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-hourly</td>
              <td style="text-align: right">8,314.86</td>
              <td style="text-align: right">466.35</td>
              <td style="text-align: right">94.39%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-daily</td>
              <td style="text-align: right">556.44</td>
              <td style="text-align: right">30.98</td>
              <td style="text-align: right">94.43%</td>
            </tr>
            <tr>
              <td><strong><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr></strong></td>
              <td>A</td>
              <td style="text-align: right">3,117.64</td>
              <td style="text-align: right">2,155.13</td>
              <td style="text-align: right">95.32%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-hourly</td>
              <td style="text-align: right">187.46</td>
              <td style="text-align: right">34.92</td>
              <td style="text-align: right">99.58%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-daily</td>
              <td style="text-align: right">12.32</td>
              <td style="text-align: right">3.35</td>
              <td style="text-align: right">99.39%</td>
            </tr>
            <tr>
              <td><strong><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr></strong></td>
              <td>A</td>
              <td style="text-align: right">5,335.04</td>
              <td style="text-align: right">1,854.48</td>
              <td style="text-align: right">95.97%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-hourly</td>
              <td style="text-align: right">2,127.57</td>
              <td style="text-align: right">388.02</td>
              <td style="text-align: right">95.33%</td>
            </tr>
            <tr>
              <td> </td>
              <td>B-daily</td>
              <td style="text-align: right">142.08</td>
              <td style="text-align: right">25.69</td>
              <td style="text-align: right">95.33%</td>
            </tr>
            <tr>
              <td><strong><abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr></strong></td>
              <td>A</td>
              <td style="text-align: right"><em>2,682.88</em></td>
              <td style="text-align: right"><em>856.39</em></td>
              <td style="text-align: right"><em>98.14%</em></td>
            </tr>
            <tr>
              <td> </td>
              <td>B-hourly</td>
              <td style="text-align: right"><em>24.39</em></td>
              <td style="text-align: right"><em>2.86</em></td>
              <td style="text-align: right"><em>99.96%</em></td>
            </tr>
            <tr>
              <td> </td>
              <td>B-daily</td>
              <td style="text-align: right"><em>5.96</em></td>
              <td style="text-align: right"><em>1.14</em></td>
              <td style="text-align: right"><em>99.79%</em></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 14:</span> Compressability using gzip for all <abbr title='Benchmark of rdf Archives'>BEAR</abbr> datasets using <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> and natively as N-Triples.
The columns represent the original size (MB), the resulting size after applying gzip (MB), and the total space savings.
The lowest sizes are indicated in italics.</p>
        </figcaption>
      </figure>

      <figure id="storing_results-ostrich-ingestion-rate-beara-compare">
<img src="storing/img/results-ostrich-ingestion-rate-beara-compare.svg" alt="[Comparison of ostrich ingestion algorithms]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 42:</span> Comparison of the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> stream and batch-based ingestion durations.</p>
        </figcaption>
</figure>

      <h5 id="compressibility">Compressibility</h5>

      <p><a href="#storing_results-ostrich-compressability"></a> presents the compressibility of datasets without auxiliary indexes,
showing that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and the <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches significantly improve compressibility compared to the original N-Triples serialization.
We omitted the results from the Jena-based approaches in this table,
as all compressed sizes were in all cases two to three times larger than the N-Triples compression.</p>

      <h5 id="query-evaluation">Query Evaluation</h5>

      <p>Figures <a href="#storing_results-beara-vm-sumary">10</a>, <a href="#storing_results-beara-dm-summary">11</a> and <a href="#storing_results-beara-vq-summary">12</a> respectively
summarize the <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> query durations of all <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A queries on the ten first versions of the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset for the different approaches.
<abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> clearly outperforms all other approaches in all cases,
while the Jena-based approaches are orders of magnitude slower than the <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in all cases.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is about two times faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> for <abbr title='Version materialization'>VM</abbr> queries, and slightly slower for both <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> queries.
For <abbr title='Delta materialization'>DM</abbr> queries, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> does however continuously become slower for larger versions, while the lookup times for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> remain constant.
From version 7, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-a" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​a">Appendix A</a> contains more detailed plots for each <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A queryset,
in which we can see that all approaches collectively become slower for queries with a higher result cardinality,
and that predicate-queries are also significantly slower for all approaches.</p>

      <figure id="storing_results-beara-vm-sumary">
<img src="storing/img/query/results_beara-vm-summary.svg" alt="[bear-a vm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 43:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A <abbr title='Version materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-beara-dm-summary">
<img src="storing/img/query/results_beara-dm-summary.svg" alt="[bear-a dm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 44:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A <abbr title='Delta materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-beara-vq-summary">
<img src="storing/img/query/results_beara-vq-summary.svg" alt="[bear-a vq]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 45:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A <abbr title='Version query'>VQ</abbr> query results for all triple patterns.</p>
        </figcaption>
</figure>

      <p>Figures <a href="#storing_results-bearb-daily-vm-sumary">13</a>, <a href="#storing_results-bearb-daily-dm-summary">14</a> and <a href="#storing_results-bearb-daily-vq-summary">15</a>
contain the query duration results for the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B queries on the complete <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily dataset for the different approaches.
Jena-based approaches are again slower than both the <abbr title='Header Dictionary Triples'>HDT</abbr>-based ones and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
For <abbr title='Version materialization'>VM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is slower than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>, but faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, which becomes slower for larger versions.
For <abbr title='Delta materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> for the second half of the versions, and slightly faster <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>.
The difference between <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is however insignificant in this case, as can be seen in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-daily" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​daily">Appendix B</a>.
For <abbr title='Version query'>VQ</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is significantly faster than all other approaches.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-daily" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​daily">Appendix B</a> contains more detailed plots for this case,
in which we can see that predicate-queries are again consistently slower for all approaches.</p>

      <figure id="storing_results-bearb-daily-vm-sumary">
<img src="storing/img/query/results_bearb-daily-vm-summary.svg" alt="[bear-b-daily vm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 46:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily <abbr title='Version materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-bearb-daily-dm-summary">
<img src="storing/img/query/results_bearb-daily-dm-summary.svg" alt="[bear-b-daily dm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 47:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily <abbr title='Delta materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-bearb-daily-vq-summary">
<img src="storing/img/query/results_bearb-daily-vq-summary.svg" alt="[bear-b-daily vq]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 48:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily <abbr title='Version query'>VQ</abbr> query results for all triple patterns.</p>
        </figcaption>
</figure>

      <p>Figures <a href="#storing_results-bearb-hourly-vm-sumary">16</a>, <a href="#storing_results-bearb-hourly-dm-summary">17</a> and <a href="#storing_results-hourly-daily-vq-summary">18</a>
show the query duration results for the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B queries on the complete <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly dataset for all approaches.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> again outperforms Jena-based approaches in all cases.
<abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> is faster for <abbr title='Version materialization'>VM</abbr> queries than <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, but <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> is significantly slower, except for the first 100 versions.
For <abbr title='Delta materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is comparable to <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>, and faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, except for the first 100 versions.
Finally, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms all <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches for <abbr title='Version query'>VQ</abbr> queries by almost an order of magnitude.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-bear-b-hourly" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​bear-​b-​hourly">Appendix C</a> contains the more detailed plots
with the same conclusion as before that predicate-queries are slower.</p>

      <figure id="storing_results-bearb-hourly-vm-sumary">
<img src="storing/img/query/results_bearb-hourly-vm-summary.svg" alt="[bear-b-hourly vm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 49:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly <abbr title='Version materialization'>VM</abbr> query results for all triple patterns for all versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-bearb-hourly-dm-summary">
<img src="storing/img/query/results_bearb-hourly-dm-summary.svg" alt="[bear-b-hourly dm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 50:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly <abbr title='Delta materialization'>DM</abbr> query results for all triple patterns from version 0 to all other versions.</p>
        </figcaption>
</figure>

      <figure id="storing_results-bearb-hourly-vq-summary">
<img src="storing/img/query/results_bearb-hourly-vq-summary.svg" alt="[bear-b-hourly vq]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 51:</span> Median <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly <abbr title='Version query'>VQ</abbr> query results for all triple patterns.</p>
        </figcaption>
</figure>

      <h5 id="offset">Offset</h5>

      <p>From our evaluation of offsets, <a href="#storing_results-offset-vm">Fig. 52</a> shows that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> offset evaluation remain below 1ms,
while other approaches grow beyond that for larger offsets, except for <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>+.
<abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, Jena-<abbr title='Change-based'>CB</abbr> and Jena-<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> are not included in this and the following figures
because they require full materialization before offsets can be applied, which is expensive and therefore take a very long time to evaluate.
For <abbr title='Delta materialization'>DM</abbr> queries, all approaches have growing evaluating times for larger offsets including <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, as can be seen in <a href="#storing_results-offset-dm">Fig. 53</a>.
Finally, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> has <abbr title='Version query'>VQ</abbr> evaluation times that are approximately independent of the offset value,
while other approaches again have growing evaluation times, as shown in <a href="#storing_results-offset-vq">Fig. 54</a>.</p>

      <figure id="storing_results-offset-vm">
<img src="storing/img/query/results_offsets-vm.svg" alt="[Offsets vm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 52:</span> Median <abbr title='Version materialization'>VM</abbr> query results for different offsets over all versions in the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset.</p>
        </figcaption>
</figure>

      <figure id="storing_results-offset-dm">
<img src="storing/img/query/results_offsets-dm.svg" alt="[Offsets dm]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 53:</span> Median <abbr title='Delta materialization'>DM</abbr> query results for different offsets between version 0 and all other versions in the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset.</p>
        </figcaption>
</figure>

      <figure id="storing_results-offset-vq">
<img src="storing/img/query/results_offsets-vq.svg" alt="[Offsets vq]" height="200em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 54:</span> Median <abbr title='Version query'>VQ</abbr> query results for different offsets in the <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A dataset.</p>
        </figcaption>
</figure>

      <h4 id="discussion">Discussion</h4>

      <p>In this section, we interpret and discuss the results from previous section.
We discuss the ingestion, compressbility, query evaluation, offset efficiency and test our hypotheses.</p>

      <h5 id="ingestion-1">Ingestion</h5>

      <p>For all evaluated cases, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires less storage space than most non-<abbr title='Change-based'>CB</abbr> approaches.
The <abbr title='Change-based'>CB</abbr> and <abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> approaches in most cases outperform <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> in terms of storage space efficiency due
to the additional metadata that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> stores per triple.
Because of this, most other approaches require less time to ingest new data.
These timing results should however be interpreted correctly,
because all other approaches receive their input data in the appropriate format (<abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr>, <abbr title='Timestamp-based'>TB</abbr>, <abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr>),
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> does not.
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> must convert <abbr title='Change-based'>CB</abbr> input at runtime to the alternative <abbr title='Change-based'>CB</abbr> structure where deltas are relative to the snapshot,
which explains the larger ingestion times.
As an example, <a href="#storing_triples-bearb-hourly-altcb">Fig. 55</a> shows the number of triples in each <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly version
where the deltas have been transformed to the alternative delta structure that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> uses.
Just like the first part of <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 40</a>, this graph also increases linearly,
which indicates that the large number of triples that need to be handled for long delta chains is one of the main bottlenecks for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>.
This is also the reason why <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> has memory issues during ingestion at the end of such chains.
One future optimization could be to maintain the last version of each chain in a separate index for faster patching.
Or a new ingestion algorithm could be implemented that accepts input in the correct alternative <abbr title='Change-based'>CB</abbr> format.
Alternatively, a new snapshot could dynamically be created when ingestion time becomes too large,
which could for example for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly take place around version 1000.</p>

      <figure id="storing_triples-bearb-hourly-altcb">
<img src="storing/img/triples-bearb-hourly-altcb.svg" alt="[bear-b-hourly alternative cb]" height="150em" class="plot" />
<figcaption>
          <p><span class="label">Fig. 55:</span> Total number of triples for each <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly version when converted to the alternative <abbr title='Change-based'>CB</abbr> structure used by <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>,
i.e., each triple is an addition or deletion relative to the <em>first</em> version instead of the <em>previous</em> version.</p>
        </figcaption>
</figure>

      <p>The <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly datasets indicate the limitations of the ingestion algorithm in our system.
The results for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingests slowly for many very large versions,
but it is still possible because of the memory-efficient streaming algorithm.
The results for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly show that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> should not be used when the number of versions is very large.
Furthermore, for each additional version in a dataset, the ingestion time increases.
This is a direct consequence of our alternative delta chain method where all deltas are relative to a snapshot.
That is the reason why when new deltas are inserted,
the previous one must be fully materialized by iterating over all existing triples,
because no version index exists.</p>

      <p>In <a href="#storing_results-ostrich-ingestion-rate-bearb-hourly">Fig. 40</a>, we can observe large fluctuations in ingestion time around version 1,200 of <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
This is caused by the large amount of versions that are stored for each tree value.
Since each version requires a mapping to seven triple pattern indexes and one local change flag in the deletion tree,
value sizes become non-negligible for large amounts of versions.
Each version value requires 28 uncompressed bytes,
which results in more than 32KB for a triple in 1,200 versions.
At that point, the values start to form a bottleneck as only 1,024 elements
can be loaded in-memory using the default page cache size of 32MB,
which causes a large amount of swapping.
This could be solved by either tweaking the B+Tree parameters for this large amount of versions,
reducing storage requirements for each value,
or by dynamically creating a new snapshot.</p>

      <p>We compared the streaming and batch-based ingestion algorithm in <a href="#storing_results-ostrich-ingestion-rate-beara-compare">Fig. 42</a>.
The batch algorithm is initially faster because most operations can happen in memory,
while the streaming algorithm only uses a small fraction of that memory,
which makes the latter usable for very large datasets that don’t fit in memory.
In future work, a hybrid between the current streaming and batch algorithm could be investigated,
i.e., a streaming algorithm with a larger buffer size, which is faster, but doesn’t require unbounded amounts of memory.</p>

      <h5 id="compressibility-1">Compressibility</h5>

      <p>As shown in <a href="#storing_results-ostrich-compressability"></a>,
when applying gzip directly on the raw N-Triples input already achieves significant space savings.
However, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> and <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> are able to reduce the required storage space <em>even further</em> when they are used as a pre-processing step before applying gzip.
This shows that these approaches are better—storage-wise—for the archival of versioned datasets.
This table also shows that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets with more versions are more prone to space savings
using compression techniques like gzip compared to <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets with fewer versions.</p>

      <h5 id="query-evaluation-1">Query Evaluation</h5>

      <p>The results from previous section show that the <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> query evaluation efficiency is faster than all Jena-based approaches,
mostly faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, and mostly slower than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>.
<abbr title='Version materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are always slower than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>,
because <abbr title='Header Dictionary Triples'>HDT</abbr> can very efficiently query a single materialized snapshot in this case,
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more operations for materializing.
<abbr title='Version materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are however always faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, because the latter has to reconstruct complete delta chains,
while <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> only has to reconstruct a single delta relative to the snapshot.
For <abbr title='Delta materialization'>DM</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is slower or comparable to <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>, slower than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> for early versions, but faster for later versions.
This slowing down of <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> for <abbr title='Delta materialization'>DM</abbr> queries is again caused by reconstruction of delta chains.
For <abbr title='Version query'>VQ</abbr> queries, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms all other approaches for datasets with larger amounts of versions.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, which contains only 10 versions in our case,
the <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches are slightly faster because only a small amount of versions need to be iterated.</p>

      <h5 id="offsets">Offsets</h5>

      <p>One of our initial requirements was to design a system that allows efficient offsetting of <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> result streams.
As shown in last section, for both <abbr title='Version materialization'>VM</abbr> and <abbr title='Version query'>VQ</abbr> queries, the lookup times for various offsets remain approximately constant.
For <abbr title='Version materialization'>VM</abbr> queries, this can fluctuate slightly for certain offsets due to the loop section inside the <abbr title='Version materialization'>VM</abbr> algorithm
for determining the starting position inside the snapshot and deletion tree.
For <abbr title='Delta materialization'>DM</abbr> queries, we do however observe an increase in lookup times for larger offsets.
That is because the current <abbr title='Delta materialization'>DM</abbr> algorithm naively offsets these streams by iterating
over the stream until a number of elements equal to the desired offset have been consumed.
Furthermore, other <abbr title='Independent copies'>IC</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches outperform <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>’s <abbr title='Delta materialization'>DM</abbr> result stream offsetting.
This introduces a new point of improvement for future work,
seeing whether or not <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> would allow more efficient <abbr title='Delta materialization'>DM</abbr> offsets by adjusting either the algorithm or the storage format.</p>

      <h5 id="hypotheses">Hypotheses</h5>

      <p>In <a href="#storing_problem-statement">Subsection 3.4</a>, we introduced six hypotheses, which we will validate in this section based on our experimental results.
We will only consider the comparison between <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='Header Dictionary Triples'>HDT</abbr>-based approaches,
as <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> outperforms the Jena-based approaches for all cases in terms of lookup times.
These validations were done using R, for which the source code can be found on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.
Tables containing p-values of the results can be found in <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#appendix-tests" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#appendix-​tests">Appendix E</a>.</p>

      <p>For our <a href="#storing_hypothesis-qualitative-querying">first hypothesis</a>, we expect <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> lookup times to remain independent of version for <abbr title='Version materialization'>VM</abbr> and <abbr title='Delta materialization'>DM</abbr> queries.
We validate this hypothesis by building a linear regression model with as response the lookup time,
and as factors version and number of results.
The <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-1" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​1">appendix</a> contains the influence of each factor, which shows that for all cases,
we can accept the null hypothesis that the version factor has no influence on the models with a confidence of 99%.
Based on these results, we <em>accept</em> our <a href="#storing_hypothesis-qualitative-querying">first hypothesis</a>.</p>

      <p><a href="#storing_hypothesis-qualitative-ic-storage">Hypothesis 2</a> states that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>less</em> storage space than <abbr title='Independent copies'>IC</abbr>-based approaches,
and <a href="#storing_hypothesis-qualitative-ic-querying">Hypothesis 3</a> correspondingly states that
query evaluation is <em>slower</em> for <abbr title='Version materialization'>VM</abbr> and <em>faster</em> or <em>equal</em> for <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr>.
Results from previous section showed that for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly,
<abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>less</em> space than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr>, which means that we <em>accept</em> Hypothesis 2.
In order to validate that query evaluation is slower for <abbr title='Version materialization'>VM</abbr> but faster or equal for <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr>,
we compared the means using the two-sample t-test, for which the results can be found in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-2" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​2">appendix</a>.
In all cases, the means are not equal with a confidence of 95%.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> is faster for <abbr title='Version materialization'>VM</abbr> queries, but slower for <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> queries.
For <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Independent copies'>IC</abbr> is faster for all query types.
We therefore <em>reject</em> Hypothesis 3, as it does not apply for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, but it is valid for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
This means that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> typically requires less storage space than <abbr title='Independent copies'>IC</abbr>-based approaches,
and outperforms other approaches in terms of querying efficiency
unless the number of versions is small or for <abbr title='Version materialization'>VM</abbr> queries.</p>

      <p>In <a href="#storing_hypothesis-qualitative-cb-storage">Hypothesis 4</a>, we stated that <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires <em>more</em>
storage space than <abbr title='Change-based'>CB</abbr>-based approaches,
and in <a href="#storing_hypothesis-qualitative-cb-querying">Hypothesis 5</a> that query evaluation is <em>faster</em> or <em>equal</em>.
In all cases <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more storage space than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr>, which is why we <em>accept</em> Hypothesis 4.
For the query evaluation, we again compare the means in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-3" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​3">appendix</a> using the same test.
In <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, <abbr title='Version query'>VQ</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are not faster for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A, and <abbr title='Version materialization'>VM</abbr> queries in <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> are not faster for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily,
which is why we <em>reject</em> Hypothesis 5.
However, only one in three query atoms are not fulfilled, and <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is faster than <abbr title='Header Dictionary Triples'>HDT</abbr>-<abbr title='Change-based'>CB</abbr> for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
In general, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> requires more storage space than <abbr title='Change-based'>CB</abbr>-based approaches,
and query evaluation is faster unless the number of versions is low.</p>

      <p>Finally, in our <a href="#storing_hypothesis-qualitative-ingestion">last hypothesis</a>,
we state that average query evaluation times are lower than other non-<abbr title='Independent copies'>IC</abbr> approaches at the cost of increased ingestion times.
In all cases, the ingestion time for <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is higher than the other approaches,
and as shown in the <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#hypo-test-3" class="mandatory" data-link-text="https:/​/​rdfostrich.github.io/​article-​jws2018-​ostrich/​#hypo-​test-​3">appendix</a>, query evaluation times for non-<abbr title='Independent copies'>IC</abbr> approaches are lower for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly.
This means that we <em>reject</em> Hypothesis 6 because it only holds for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-hourly and not for <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-A and <abbr title='Benchmark of rdf Archives'>BEAR</abbr>-B-daily.
In general, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> ingestion is slower than other approaches,
but improves query evaluation time compared to other non-<abbr title='Independent copies'>IC</abbr> approaches,
unless the number of versions is low.</p>

      <p>In this section, we accepted three of the six hypotheses.
As these are statistical hypotheses, these do not necessarily indicate negative results of our approach.
Instead, they allow us to provide general guidelines on where our approach can be used effectively, and where not.</p>

    </section>

    <section id="storing_conclusions">
      <h3>Conclusions</h3>

      <p>In this article, we introduced an <abbr title='Resource Description Framework'>RDF</abbr> archive storage method with accompanied algorithms for evaluating <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr>, and <abbr title='Version query'>VQ</abbr> queries,
with efficient result offsets.
Our novel storage technique is a hybrid of the <abbr title='Independent copies'>IC</abbr>/<abbr title='Change-based'>CB</abbr>/<abbr title='Timestamp-based'>TB</abbr> approaches, because we store sequences of snapshots followed by delta chains.
The evaluation of our <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> implementation shows that this technique offers a new trade-off in terms of ingestion time, storage size and lookup times.
By preprocessing and storing additional data during ingestion, we can reduce lookup times for <abbr title='Version materialization'>VM</abbr>, <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> queries compared to <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches.
Our approach requires less storage space than <abbr title='Independent copies'>IC</abbr> approaches, at the cost of slightly slower <abbr title='Version materialization'>VM</abbr> queries, but comparable <abbr title='Delta materialization'>DM</abbr> queries.
Furthermore, our technique is faster than <abbr title='Change-based'>CB</abbr> approaches, at the cost of more storage space.
Additionally, <abbr title='Version query'>VQ</abbr> queries become increasingly more efficient for datasets with larger amounts of versions compared to <abbr title='Independent copies'>IC</abbr>, <abbr title='Change-based'>CB</abbr> and <abbr title='Timestamp-based'>TB</abbr> approaches.
Our current implementation supports a single snapshot and delta chain
as a proof of concept,
but production environments would normally incorporate more frequent snapshots,
balancing between storage and querying requirements.</p>

      <p>With lookup times of 1ms or less in most cases, <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> is an ideal candidate for Web querying,
as the network latency will typically be higher than that.
At the cost of increased ingestion times, lookups are fast.
Furthermore, by reusing the highly efficient <abbr title='Header Dictionary Triples'>HDT</abbr> format for snapshots,
existing <abbr title='Header Dictionary Triples'>HDT</abbr> files can directly be loaded by <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>
and patched with additional versions afterwards.</p>

      <p><abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> fulfills the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">requirements</a> <span class="references">[<a href="#ref-82">82</a>]</span> for a backend <abbr title='Resource Description Framework'>RDF</abbr> archive storage solution
for supporting versioning queries in the <abbr title='Triple Pattern Fragments'>TPF</abbr> framework.
Together with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr></a> <span class="references">[<a href="#ref-41">41</a>]</span> interface feature, <abbr title='Resource Description Framework'>RDF</abbr> archives can be queried on the Web at a low cost,
as demonstrated on <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">our public <abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr> entrypoint</a>.
<abbr title='Triple Pattern Fragments'>TPF</abbr> only requires triple pattern indexes with count metadata,
which means that <abbr title='Triple Pattern Fragments'>TPF</abbr> clients are able to evaluate full <abbr title='Version materialization'>VM</abbr> <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries using <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> and <abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr>.
In future work, the <abbr title='Triple Pattern Fragments'>TPF</abbr> client will be extended to also support <abbr title='Delta materialization'>DM</abbr> and <abbr title='Version query'>VQ</abbr> <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries.</p>

      <p>With <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>, we provide a technique for publishing and querying <abbr title='Resource Description Framework'>RDF</abbr> archives at Web-scale.
Several opportunities exist for advancing this technique in future work,
such as improving the ingestion efficiency, increasing the <abbr title='Delta materialization'>DM</abbr> offset efficiency,
and supporting dynamic snapshot creation.
Solutions could be based on existing
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://users.ics.forth.gr/~fgeo/files/ER14.pdf">cost models</a> <span class="references">[<a href="#ref-83">83</a>]</span> for determining whether a new snapshot or delta
should be created based on quantified time and space parameters.
Furthermore, branching and merging of different version chains can be investigated.</p>

      <p>Our approach succeeds in reducing the cost for publishing <abbr title='Resource Description Framework'>RDF</abbr> archives on the Web.
This lowers the barrier towards intelligent clients in the Semantic Web <span class="references">[<a href="#ref-1">1</a>]</span> that require <em>evolving</em> data,
with the goal of time-sensitive querying over the ever-evolving Web of data.</p>

    </section>

    <div class="subfooter">
  <section id="storing_acknowledgements">
        <h3 class="no-label-increment">Acknowledgements</h3>

        <p>We would like to thank Christophe Billiet for providing his insights into temporal databases.
We thank Giorgos Flouris for his comments on the structure and contents of this article,
and Javier D. Fernández for his help in setting up and running the <abbr title='Benchmark of rdf Archives'>BEAR</abbr> benchmark.
The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

      </section>

</div>
  </section>
  
  <section class="sub-paper">
    <h2 id="querying">Querying on the Web</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/joachim_van_herwegen">Joachim Van Herwegen</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/miel_vander_sande">Miel Vander Sande</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica: a Modular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Query Engine for the Web</a></p>

    <section id="querying_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <!-- Context      -->
      <p>Query evaluation over Linked Data sources has become a complex story,
given the multitude of algorithms and techniques
for single- and multi-source querying,
as well as the heterogeneity of Web interfaces
through which data is published online.
<!-- Need         -->
Today’s query processors are insufficiently adaptable
to test multiple query engine aspects in combination,
such as evaluating the performance of a certain join algorithm
over a federation of heterogeneous interfaces.
The Semantic Web research community is in need of a flexible query engine
that allows plugging in new components
such as different algorithms,
new or experimental <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> features,
and support for new Web interfaces.
<!-- Task         -->
We designed and developed a Web-friendly and modular meta query engine
called <em>Comunica</em>
that meets these specifications.
<!-- Object       -->
In this article,
we introduce this query engine
and explain the architectural choices behind its design.
<!-- Findings     -->
We show how its modular nature makes it an ideal research platform
for investigating new kinds of Linked Data interfaces and querying algorithms.
<!-- Conclusion   -->
Comunica facilitates the development, testing, and evaluation
of new query processing capabilities,
both in isolation and in combination with others.
<!-- Perspectives --></p>

    </section>

    <section id="querying_introduction">
      <h3>Introduction</h3>

      <p>Linked Data on the Web exists in many shapes and forms—and
so do the processors we use to query data from one or multiple sources.
For instance,
engines that query <abbr title='Resource Description Framework'>RDF</abbr> data using the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> language</a> <span class="references">[<a href="#ref-3">3</a>]</span>
employ <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675"><em>different algorithms</em></a></span> <span class="references">[<a href="#ref-84">84</a>, <a href="#ref-85">85</a>]</span>
and support <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2"><a href="https://doi.org/10.1007/978-3-642-02184-8_2"><em>different language extensions</em></a></span> <span class="references">[<a href="#ref-86">86</a>, <a href="#ref-87">87</a>]</span>.
Furthermore,
Linked Data is increasingly published through <em>different Web interfaces</em>,
such as
data dumps, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/DesignIssues/LinkedData.html">Linked Data documents</a> <span class="references">[<a href="#ref-88">88</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints</a> <span class="references">[<a href="#ref-89">89</a>]</span>
and <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (<abbr title='Triple Pattern Fragments'>TPF</abbr>) interfaces</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>.
This has led to entirely different query evaluation strategies,
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">server-side</a> <span class="references">[<a href="#ref-89">89</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based</a> <span class="references">[<a href="#ref-90">90</a>]</span>,
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">shared client–server query processing</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>,
and
client-side (by downloading data dumps and loading them locally).</p>

      <p>The resulting variety of implementations
suffers from two main problems:
a lack of <em>sustainability</em>
and a lack of <em>comparability</em>.
Alternative query algorithms and features
are typically either implemented as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf"><em>forks</em> of existing software packages</a> <span class="references">[<a href="#ref-91">91</a>, <a href="#ref-92">92</a>, <a href="#ref-93">93</a>]</span>
or as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf"><em>independent</em> engines</a> <span class="references">[<a href="#ref-94">94</a>]</span>.
This practice has limited sustainability:
forks are often not merged into the main software distribution
and hence become abandoned;
independent implementations require a considerable upfront cost
and also risk abandonment more than established engines.
Comparability is also limited:
forks based on older versions of an engine
cannot meaningfully be evaluated against newer forks,
and evaluating <em>combinations</em> of cross-implementation features—such as
different algorithms on different interfaces—is
not possible without code adaptation.
As a result, many interesting comparisons are never performed
because they are too costly to implement and maintain.
For example,
it is currently unknown
how the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf">Linked Data Eddies algorithm</a> <span class="references">[<a href="#ref-94">94</a>]</span>
performs over a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>
of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF interfaces</a></span> <span class="references">[<a href="#ref-95">95</a>]</span>.
Another example is that the effects of various <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">optimizations and extensions for <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces</a> <span class="references">[<a href="#ref-91">91</a>, <a href="#ref-92">92</a>, <a href="#ref-93">93</a>, <a href="#ref-94">94</a>, <a href="#ref-95">95</a>, <a href="#ref-41">41</a>, <a href="#ref-96">96</a>, <a href="#ref-97">97</a>]</span>
have only been evaluated in isolation,
whereas certain combinations will likely prove complementary.</p>

      <p>In order to handle the increasing heterogeneity of Linked Data on the Web,
as well as various solutions for querying it,
there is a need for a flexible and modular query engine
to experiment with all of these techniques—both separately and in combination.
In this article, we introduce <em>Comunica</em> to realize this vision.
It is a highly modular meta engine for federated <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation
over heterogeneous interfaces,
including <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints, and data dumps.
Comunica aims to serve as a flexible research platform for
designing, implementing, and evaluating
new and existing Linked Data querying and publication techniques.</p>

      <p>Comunica differs from existing query processors on different levels:</p>

      <ol>
        <li>The <strong>modularity</strong> of the Comunica meta query engine allows for
<em>extensions</em> and <em>customization</em> of algorithms and functionality.
Users can build and fine-tune a concrete engine
by wiring the required modules through an <abbr title='Resource Description Framework'>RDF</abbr> configuration document.
By publishing this document,
experiments can repeated and adapted by others.</li>
        <li>Within Comunica, multiple <strong>heterogeneous interfaces</strong> are first-class citizens. This enables federated querying over heterogeneous sources and makes it for example possible to evaluate queries over any combination of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints, <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, datadumps, or other types of interfaces.</li>
        <li>Comunica is implemented using <strong>Web-based technologies</strong> in JavaScript, which enables usage through browsers, the command line, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> protocol</a> <span class="references">[<a href="#ref-89">89</a>]</span>, or any Web or JavaScript application.</li>
      </ol>

      <p>Comunica and its default modules are publicly available
on GitHub and the npm package manager under the open-source MIT license
(canonical citation: <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo">https:/​/​zenodo.org/record/1202509#.Wq9GZhNuaHo</a>).</p>

      <p>This article is structured as follows.
In the next section, we discuss the related work, followed by the main features of Comunica in <a href="#querying_features">Subsection 4.4</a>.
After that, we introduce the architecture of Comunica in <a href="#querying_architecture">Subsection 4.5</a>, and its implementation in <a href="#querying_implementation">Subsection 4.6</a>.
Next, we compare the performance of different Comunica configurations with the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client in <a href="#querying_comparison-tpf-client">Subsection 4.7</a>.
Finally, <a href="#querying_conclusions">Subsection 4.8</a> concludes and discusses future work.</p>

    </section>

    <section id="querying_related-work">
      <h3>Related Work</h3>

      <p>In this section, we illustrate the many possible degrees of freedom for <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation,
and show that they are hard to combine, which is the problem we aim to solve with Comunica.
We first discuss the <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language, its engines, and algorithms.
After that, we discuss alternative Linked Data publishing interfaces, and their connection to querying.
Finally, we discuss the software design patterns that are essential in the architecture of Comunica.</p>

      <h4 id="the-different-facets-of-sparql">The Different Facets of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></h4>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></a> <span class="references">[<a href="#ref-3">3</a>]</span> is the W3C-recommended <abbr title='Resource Description Framework'>RDF</abbr> query language.
The traditional way to implement a <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query processor
is to use it as an interface to an underlying database,
resulting in a so-called <a property="schema:citation http://purl.org/spar/cito/citeAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><em><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoint</em></a> <span class="references">[<a href="#ref-89">89</a>]</span>.
This is similar to how an SQL interface
provides access to a relation database.
The internal storage can either be a native <abbr title='Resource Description Framework'>RDF</abbr> store, e.g., AllegroGraph <span class="references">[<a href="#ref-98">98</a>]</span> and Blazegraph <span class="references">[<a href="#ref-76">76</a>]</span>,
or a non-<abbr title='Resource Description Framework'>RDF</abbr> store, e.g., Virtuoso <span class="references">[<a href="#ref-44">44</a>]</span> uses a object-relational database management system.</p>

      <p>Various algorithms have been proposed for optimized <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation.
Some algorithms for example use the concept of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675">query rewriting</a></span> <span class="references">[<a href="#ref-84">84</a>]</span> based on algebraic equivalent query operations,
others have proposed the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1367497.1367578"><a href="http://doi.acm.org/10.1145/1367497.1367578">optimization of Basic Graph Pattern evaluation</a></span> <span class="references">[<a href="#ref-85">85</a>]</span> using selectivity estimation of triple patterns.</p>

      <p>In order to evaluate <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries over datasets of different storage types,
<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query frameworks were developed, such as
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://jena.apache.org/">Jena (ARQ)</a> <span class="references">[<a href="#ref-79">79</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdflib.readthedocs.io/en/stable/">RDFLib</a> <span class="references">[<a href="#ref-99">99</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/linkeddata/rdflib.js">rdflib.js</a> <span class="references">[<a href="#ref-100">100</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/antoniogarrote/rdfstore-js">rdfstore-js</a> <span class="references">[<a href="#ref-101">101</a>]</span>.
Jena is a Java framework, RDFLib is a python package, and rdflib.js and rdfstore-js are JavaScript modules.
Jena—or more specifically the ARQ <abbr title='Application Programming Interface'>API</abbr>—and RDFLib are fully <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1</a> <span class="references">[<a href="#ref-3">3</a>]</span> compliant.
rdflib.js and rdfstore-js both support a subset of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1.
These <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engines support in-memory models or other sources,
such as Jena TDB in the case of ARQ.
Most of the query algorithms are tightly coupled to these frameworks,
which makes swapping out query algorithms for specific query operators hard or sometimes even impossible.
Furthermore, complex things such as federated querying over heterogeneous interfaces are difficult to implement using these frameworks,
as they are not supported out-of-the-box.
This issue of modularity and heterogeneity are two of the main problems we aim to solve within Comunica.
The differences between Comunica and existing frameworks will be explained in more detail in <a href="#features"></a>.</p>

      <p>The <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments client</a></span> <span class="references">[<a href="#ref-39">39</a>]</span> (also known as Client.js or <code>ldf-client</code>) is a client-side <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engine
that retrieves data over <abbr title='Hypertext Transfer Protocol'>HTTP</abbr>
through <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (<abbr title='Triple Pattern Fragments'>TPF</abbr>) interfaces</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">Different algorithms</a> <span class="references">[<a href="#ref-91">91</a>, <a href="#ref-96">96</a>, <a href="#ref-97">97</a>]</span> for this client and
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/iswc2015-amf.pdf"><abbr title='Triple Pattern Fragments'>TPF</abbr> interface extensions</a> <span class="references">[<a href="#ref-92">92</a>, <a href="#ref-93">93</a>, <a href="#ref-95">95</a>, <a href="#ref-41">41</a>]</span> have been proposed to reduce effort of server or client in some way.
All of these efforts are however implemented and evaluated in isolation.
Furthermore, the implementations are tied to <abbr title='Triple Pattern Fragments'>TPF</abbr> interface, which makes it impossible to use them for other types of datasources and interfaces.
With Comunica, we aim to solve this by modularizing query operation implementations into separate modules,
so that they can be plugged in and combined in different ways, on top of different datasources and interfaces.</p>

      <p>With Semantic Web technologies providing the capability
to integrate data from different sources,
<em>federated query processing</em> has been an active area of research.
However, most of the existing frameworks require <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints on every source.
The <abbr title='Triple Pattern Fragments'>TPF</abbr> Client instead federates over <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces,
and achieves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">similar performance compared to the state of the art</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>
despite its usage of a more lightweight interface.
However, no frameworks exist that enable federation over heterogeneous interfaces,
such as the federation over any combination of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints and <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.
With Comunica, we aim to fill this gap.
In addition dataset-centric approaches,
alternative methods such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based query evaluation</a> <span class="references">[<a href="#ref-90">90</a>]</span> exist
to query a web of Linked Data documents.</p>

      <h4 id="linked-data-fragments">Linked Data Fragments</h4>

      <p>In order to formally capture the heterogeneity of different Web interfaces to publish <abbr title='Resource Description Framework'>RDF</abbr> data,
the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Linked Data Fragment</a></span> <span class="references">[<a href="#ref-39">39</a>]</span> (LDF) conceptual framework
uniformly characterizes responses of Web interfaces to <abbr title='Resource Description Framework'>RDF</abbr>-based knowledge graphs.
The simplest type of LDF is a <em>data dump</em>—it is the response of a single <abbr title='Hypertext Transfer Protocol'>HTTP</abbr> requests for a complete <abbr title='Resource Description Framework'>RDF</abbr> dataset.
Other types of LDFs includes responses of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints,
<abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, and Linked Data documents.</p>

      <p>Existing LDF research highlights that,
when it comes to publishing datasets on the Web, there is no silver bullet:
no single interface works well in all situations,
as each one involves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">trade-offs</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>.
As such, data publishers must choose the type of interface that matches their intended use case, target audience and infrastructure.
This however complicates client-side engines that need to retrieve data from the resulting heterogeneity of interfaces.
As shown by the <abbr title='Triple Pattern Fragments'>TPF</abbr> approach, interfaces can be self-descriptive and expose one or more <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.07108">features</a> <span class="references">[<a href="#ref-102">102</a>]</span>,
to describe their functionality using a common vocabulary <span class="references">[<a href="#ref-103">103</a>, <a href="#ref-104">104</a>]</span>.
This allows clients without prior knowledge of the exact inputs and outputs of an interface
to discover its usage at runtime.</p>

      <p>A design goal of Comunica is to
facilitate interaction with any current and future interface
within the LDF framework,
both in single-source and federated scenarios.</p>

      <h4 id="software-design-patterns">Software Design Patterns</h4>

      <p>In the following, we discuss three software design patterns that are relevant to the modular design of the Comunica engine.</p>

      <h5 id="publishsubscribe-pattern">Publish–subscribe pattern</h5>

      <p>The <em>publish-subscribe</em> <span class="references">[<a href="#ref-105">105</a>]</span> design pattern involves passing <em>messages</em> between <em>publishers</em> and <em>subscribers</em>.
Instead of programming publishers to send messages directly to subscribers, they are programmed to <em>publish</em> messages to certain <em>categories</em>.
Subscribers can <em>subscribe</em> to these categories which will cause them to receive these published messages, without requiring prior knowledge of the publishers.
This pattern is useful for decoupling software components from each other,
and only requiring prior knowledge of message categories.
We use this pattern in Comunica for allowing different implementations of certain tasks to subscribe to task-specific buses.</p>

      <h5 id="actor-model">Actor Model</h5>

      <p>The <em>actor</em> model <span class="references">[<a href="#ref-106">106</a>]</span> was designed as a way to achieve highly parallel systems consisting of many independent <em>agents</em>
communicating using messages, similar to the publish–subscribe pattern.
An actor is a computational unit that performs a specific task, acts on messages, and can send messages to other actors.
The main advantages of the actor model are that actors can be independently made to implement certain specific tasks based on messages,
and that these can be handled asynchronously.
These characteristics are highly beneficial to the modularity that we want to achieve with Comunica.
That is why we use this pattern in combination with the publish–subscribe pattern to let each implementation of a certain task correspond to a separate actor.</p>

      <h5 id="mediator-pattern">Mediator pattern</h5>

      <p>The <em>mediator</em> <span class="references">[<a href="#ref-107">107</a>]</span> pattern is able to reduce coupling between software components that interact with each other,
and to easily change the interaction if needed.
This can be achieved by encapsulating the interaction between software components in a mediator component.
Instead of the components having to interact with each other directly,
they now interact through the mediator.
These components therefore do not require prior knowledge of each other,
and different implementations of these mediators can lead to different interaction results.
In Comunica, we use this pattern to handle actions when multiple actors are able to solve the same task,
by for example choosing the <em>best</em> actor for a task, or by combining the solutions of all actors.</p>

    </section>

    <section id="querying_features">
      <h3>Requirement analysis</h3>

      <p>In this section, we discuss the main requirements and features of the Comunica framework
as a research platform for <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation.
Furthermore, we discuss each feature based on the availability in related work.
The main feature requirements of Comunica are the following:</p>

      <dl>
        <dt><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation</dt>
        <dd>The engine should be able to interpret, process and output results for <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries.</dd>
        <dt>Modularity</dt>
        <dd>Different independent modules should contain the implementation of specific tasks, and they should be combinable in a flexible framework. The configurations should be describable in <abbr title='Resource Description Framework'>RDF</abbr>.</dd>
        <dt>Heterogeneous interfaces</dt>
        <dd>Different types of datasource interfaces should be supported, and it should be possible to add new types independently.</dd>
        <dt>Federation</dt>
        <dd>The engine should support federated querying over different interfaces.</dd>
        <dt>Web-based</dt>
        <dd>The engine should run in Web browsers using native Web technologies.</dd>
      </dl>

      <p>In <a href="#querying_features-comparison">Table 15</a>, we summarize the availability of these features in similar works.</p>

      <figure id="querying_features-comparison" class="table">

        <table>
          <thead>
            <tr>
              <th>Feature</th>
              <th><abbr title='Triple Pattern Fragments'>TPF</abbr> Client</th>
              <th>ARQ</th>
              <th>RDFLib</th>
              <th>rdflib.js</th>
              <th>rdfstore-js</th>
              <th>Comunica</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></td>
              <td>✓(1)</td>
              <td>✓</td>
              <td>✓</td>
              <td>✓(1)</td>
              <td>✓(1)</td>
              <td>✓(1)</td>
            </tr>
            <tr>
              <td>Modularity</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Heterogeneous interfaces</td>
              <td> </td>
              <td>✓(2,3)</td>
              <td>✓(2,3)</td>
              <td>✓(3)</td>
              <td>✓(3)</td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Federation</td>
              <td>✓</td>
              <td>✓(4)</td>
              <td>✓(4)</td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Web-based</td>
              <td>✓</td>
              <td> </td>
              <td> </td>
              <td>✓</td>
              <td>✓</td>
              <td>✓</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 15:</span> Comparison of the availability of the main features of Comunica in similar works.
(1) A subset of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 is implemented.
(2) Querying over <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints, other types require implementing an internal storage interface.
(3) Downloading of dumps.
(4) Federation only over <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints using the SERVICE keyword.</p>
        </figcaption>
      </figure>

      <h4 id="sparql-query-evaluation"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation</h4>

      <p>The recommended way of querying within <abbr title='Resource Description Framework'>RDF</abbr> data, is using the <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query language.
All of the discussed frameworks support at least the parsing and execution of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries, and reporting of results.</p>

      <h4 id="modularity">Modularity</h4>

      <p>Adding new functionality or changing certain operations in Comunica should require minimal to no changes to existing code.
Furthermore, the Comunica environment should be developer-friendly, including well documented APIs and auto-generation of stub code.
In order to take full advantage of the Linked Data stack, modules in Comunica must be describable, configurable and wireable in <abbr title='Resource Description Framework'>RDF</abbr>.
By registering or excluding modules from a configuration file, the user is free to choose how heavy or lightweight the query engine will be.
Comunica’s modular architecture will be explained in <a href="#querying_architecture">Subsection 4.5</a>.
ARQ, RDFLib, rdflib.js and rdfstore-js only support customization by implementing a custom query engine programmatically to handle operators.
They do not allow plugging in or out certain modules.</p>

      <h4 id="heterogeneous-interfaces">Heterogeneous interfaces</h4>

      <p>Due to the existence of different types of Linked Data Fragments for exposing Linked Datasets,
Comunica should support <em>heterogeneous</em> interfaces types, including self-descriptive Linked Data interfaces such as <abbr title='Triple Pattern Fragments'>TPF</abbr>.
This <abbr title='Triple Pattern Fragments'>TPF</abbr> interface is the only interface that is supported by the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Additionally, Comunica should also enable querying over other sources,
such as <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints and data dumps in <abbr title='Resource Description Framework'>RDF</abbr> serializations.
The existing <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> frameworks mostly support querying against <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints,
local graphs, and specific storage types using an internal storage adapter.</p>

      <h4 id="federation">Federation</h4>

      <p>Next to the different type of Linked Data Fragments for exposing Linked Datasets,
data on the Web is typically spread over <em>different</em> datasets, at different locations.
As mentioned in <a href="#querying_related-work">Subsection 4.3</a>, federated query processing is a way to query over the combination of such datasets,
without having to download the complete datasets and querying over them locally.
The <abbr title='Triple Pattern Fragments'>TPF</abbr> client supports federated query evaluation over its single supported interface type, i.e., <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.
ARQ and RDFLib only support federation over <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints using the SERVICE keyword.
Comunica should enable <em>combined</em> federated querying over its supported heterogeneous interfaces.</p>

      <h4 id="web-based">Web-based</h4>

      <p>Comunica must be built using native Web technologies, such as JavaScript and <abbr title='Resource Description Framework'>RDF</abbr> configuration documents.
This allows Comunica to run in different kinds of environments, including Web browsers, local (JavaScript) runtime engines and command-line interfaces,
just like the <abbr title='Triple Pattern Fragments'>TPF</abbr>-client, rdflib.js and rdfstore-js.
ARQ and RDFLib are able to run in their language’s runtime and via a command-line interface, but not from within Web browsers.
ARQ would be able to run in browsers using a custom Java applet, which is not a native Web technology.</p>

    </section>

    <section id="querying_architecture">
      <h3>Architecture</h3>

      <p>In this section, we discuss the design and architecture of the Comunica meta engine,
and show how it conforms to the <em>modularity</em> feature requirement.
In summary, Comunica is collection of small modules that, when wired together,
are able to perform a certain task, such as evaluating <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries.
We first discuss the customizability of Comunica at design-time,
followed by the flexibility of Comunica at run-time.
Finally, we give an overview of all modules.</p>

      <h4 id="customizable-wiring-at-design-time-through-dependency-injection">Customizable Wiring at Design-time through Dependency Injection</h4>

      <p>There is no such thing as <em>the</em> Comunica engine,
instead, Comunica is a meta engine that can be <em>instantiated</em> into different engines based on different configurations.
Comunica achieves this customizability at design-time using the concept of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://martinfowler.com/articles/injection.html"><em>dependency injection</em></a> <span class="references">[<a href="#ref-108">108</a>]</span>.
Using a configuration file, which is created before an engine is started,
components for an engine can be <em>selected</em>, <em>configured</em> and <em>combined</em>.
For this, we use the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://componentsjs.readthedocs.io/en/latest/">Components.js</a> <span class="references">[<a href="#ref-109">109</a>]</span> JavaScript dependency injection framework,
This framework is based on semantic module descriptions and configuration files
using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">Object-Oriented Components ontology</a> <span class="references">[<a href="#ref-110">110</a>]</span>.</p>

      <h5 id="description-of-individual-software-components">Description of Individual Software Components</h5>

      <p>In order to refer to Comunica components from within configuration files,
we semantically describe all Comunica components using the Components.js framework in JSON-LD <span class="references">[<a href="#ref-111">111</a>]</span>.
<a href="#querying_config-actor">Listing 3</a> shows an example of the semantic description of an <abbr title='Resource Description Framework'>RDF</abbr> parser.</p>

      <h5 id="description-of-complex-software-configurations">Description of Complex Software Configurations</h5>

      <p>A specific instance of a Comunica engine
can be <em>initialized</em> using Components.js configuration files
that describe the wiring between components.
For example, <a href="#querying_config-parser">Listing 4</a> shows a configuration file of an engine that is able to parse N3 and JSON-LD-based documents.
This example shows that, due to its high degree of modularity,
Comunica can be used for other purposes than a query engine,
such as building a custom <abbr title='Resource Description Framework'>RDF</abbr> parser.</p>

      <p>Since many different configurations can be created,
it is important to know which one was used for a specific use case or evaluation.
For that purpose,
the <abbr title='Resource Description Framework'>RDF</abbr> documents that are used to instantiate a Comunica engine
can be <a property="schema:citation http://purl.org/spar/cito/citeAsEvidence" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">published as Linked Data</a> <span class="references">[<a href="#ref-110">110</a>]</span>.
They can then serve as provenance
and as the basis for derived set-ups or evaluations.</p>

      <figure id="querying_config-actor" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;npmd:@comunica/actor-rdf-parse-n3&quot;,
</code><code>  &quot;components&quot;: [
</code><code>    {
</code><code>      &quot;@id&quot;:            &quot;crpn3:Actor/RdfParse/N3&quot;,
</code><code>      &quot;@type&quot;:          &quot;Class&quot;,
</code><code>      &quot;extends&quot;:        &quot;cbrp:Actor/RdfParse&quot;,
</code><code>      &quot;requireElement&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;comment&quot;:        &quot;An actor that parses Turtle-like RDF&quot;,
</code><code>      &quot;parameters&quot;: [
</code><code>        {
</code><code>          &quot;@id&quot;: &quot;caam:Actor/AbstractMediaTypedFixed/mediaType&quot;,
</code><code>          &quot;default&quot;: [ &quot;text/turtle&quot;, &quot;application/n-triples&quot; ]
</code><code>        }
</code><code>      ]
</code><code>    }
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 3:</span> Semantic description of a component that is able to parse N3-based <abbr title='Resource Description Framework'>RDF</abbr> serializations.
This component has a single parameter that allows media types to be registered that this parser is able to handle.
In this case, the component has four default media types.</p>
        </figcaption>
</figure>

      <figure id="querying_config-parser" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;http://example.org/myrdfparser&quot;,
</code><code>  &quot;@type&quot;: &quot;Runner&quot;,
</code><code>  &quot;actors&quot;: [
</code><code>    { &quot;@type&quot;: &quot;ActorInitRdfParse&quot;,
</code><code>      &quot;mediatorRdfParse&quot;: {
</code><code>        &quot;@type&quot;: &quot;MediatorRace&quot;,
</code><code>        &quot;cc:Mediator/bus&quot;: { &quot;@id&quot;: &quot;cbrp:Bus/RdfParse&quot; }
</code><code>      } },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseJsonLd&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 4:</span> Comunica configuration of <code>ActorInitRdfParse</code> for parsing an <abbr title='Resource Description Framework'>RDF</abbr> document in an unknown serialization.
This actor is linked to a mediator with a bus containing two <abbr title='Resource Description Framework'>RDF</abbr> parsers for specific serializations.</p>
        </figcaption>
</figure>

      <h4 id="flexibility-at-run-time-using-the-actormediatorbus-pattern">Flexibility at Run-time using the Actor–Mediator–Bus Pattern</h4>

      <p>Once a Comunica engine has been configured and initialized,
components can interact with each other in a flexible way using the <em>actor</em> <span class="references">[<a href="#ref-106">106</a>]</span>,
<em>mediator</em> <span class="references">[<a href="#ref-107">107</a>]</span>, and <em>publish–subscribe</em> <span class="references">[<a href="#ref-105">105</a>]</span> patterns.
Any number of <em>actor</em>, <em>mediator</em> and <em>bus</em> modules can be created,
where each actor interacts with mediators, that in turn invoke other actors that are registered to a certain bus.</p>

      <p><a href="#querying_actor-mediator-bus">Fig. 56</a> shows an example logic flow between actors through a mediator and a bus.
The relation between these components, their phases and the chaining of them will be explained hereafter.</p>

      <figure id="querying_actor-mediator-bus">
<img src="querying/img/actor-mediator-bus.svg" alt="[actor-mediator-bus pattern]" class="figure-narrow" />
<figcaption>
          <p><span class="label">Fig. 56:</span> Example logic flow where Actor 0 requires an <em>action</em> to be performed.
This is done by sending the action to the Mediator, which sends a <em>test action</em> to Actors 1, 2 and 3 via the Bus.
The Bus then sends all <em>test replies</em> to the Mediator,
which chooses the best actor for the action, in this case Actor 3.
Finally, the Mediator sends the original action to Actor 3, and returns its response to Actor 0.</p>
        </figcaption>
</figure>

      <h5 id="relation-between-actors-and-buses">Relation between Actors and Buses</h5>

      <p>Actors are the main computational units in Comunica, and buses and mediators form the <em>glue</em> that ties them together and makes them interactable.
Actors are responsible for being able to accept certain messages
via the bus to which they are subscribed,
and for responding with an answer.
In order to avoid a single high-traffic bus for all message types which could cause performance issues,
separate buses exist for different message types.
<a href="#querying_relation-actor-bus">Fig. 57</a> shows an example of how actors can be registered to buses.</p>

      <figure id="querying_relation-actor-bus">
<img src="querying/img/relation-actor-bus.svg" alt="[relation between actors and buses]" class="figure-narrow" />
<figcaption>
          <p><span class="label">Fig. 57:</span> An example of two different buses each having two subscribed actors.
The left bus has different actors for parsing triples in a certain <abbr title='Resource Description Framework'>RDF</abbr> serialization to triple objects.
The right bus has actors that join query bindings streams together in a certain way.</p>
        </figcaption>
</figure>

      <h5 id="mediators-handle-actor-run-and-test-phases">Mediators handle Actor Run and Test Phases</h5>

      <p>Each mediator is connected to a single bus, and its goal is to determine and invoke the <em>best</em> actor for a certain task.
The definition of ‘<em>best</em>’ depends on the mediator, and different implementations can lead to different choices in different scenarios.
A mediator works in two phases: the <em>test</em> phase and the <em>run</em> phase.
The test phase is used to check under which conditions the action can be performed in each actor on the bus.
This phase must always come before the <em>run</em> phase, and is used to select which actor is best suited to perform a certain task under certain conditions.
If such an actor is determined, the <em>run</em> phase of a single actor is initiated.
This <em>run</em> phase takes this same type of message, and requires to <em>effectively act</em> on this message,
and return the result of this action.
<a href="#querying_run-test-phases">Fig. 58</a> shows an example of a mediator invoking a run and test phase.</p>

      <figure id="querying_run-test-phases">
<img src="querying/img/run-test-phases.svg" alt="[mediators handle actor run and test phases]" />
<figcaption>
          <p><span class="label">Fig. 58:</span> Example sequence diagram of a mediator that chooses the fastest actor
on a parse bus with two subscribed actors.
The first parser is very fast but requires a lot of memory,
while the second parser is slower, but requires less memory.
Which one is best, depends on the use case and is determined by the Mediator.
The mediator first calls the <em>tests</em> the actors for the action, and then <em>runs</em> the action using the <em>best</em> actor.</p>
        </figcaption>
</figure>

      <h4 id="modules">Modules</h4>

      <p>At the time of writing, Comunica consists of 79 different modules.
This consists of 13 buses, 3 mediator types, 57 actors and 6 other modules.
In this section, we will only discuss the most important actors and their interactions.</p>

      <p>The main bus in Comunica is the <em>query operation</em> bus, which consists of 19 different actors
that provide at least one possible implementation of the typical <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> operations such as quad patterns, basic graph patterns (BGPs), unions, projects, …
These actors interact with each other using streams of <em>quad</em> or <em>solution mappings</em>,
and act on a query plan expressed in in <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> algebra</a> <span class="references">[<a href="#ref-3">3</a>]</span>.</p>

      <p>In order to enable heterogeneous sources to be queried in a federated way,
we allow a list of sources, annotated by type, to be passed when a query is initiated.
These sources are passed down through the chain of query operation actors,
until the quad pattern level is reached.
At this level, different actors exist for handling a single source of a certain type,
such as <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints, local or remote datadumps.
In the case of multiple sources, one actor exists that implements a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation algorithm defined for <abbr title='Triple Pattern Fragments'>TPF</abbr></a></span> <span class="references">[<a href="#ref-39">39</a>]</span>,
but instead of federating over different <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces, it federates over different single-source quad pattern actors.</p>

      <p>At the end of the pipeline, different actors are available for serializing the results of a query in different ways.
For instance, there are actors for serializing the results according to
the <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">JSON</a> <span class="references">[<a href="#ref-112">112</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/rdf-sparql-XMLres/">XML</a> <span class="references">[<a href="#ref-113">113</a>]</span> result specifications,
but actors with more visual and developer-friendly formats are available as well.</p>

    </section>

    <section id="querying_implementation">
      <h3>Implementation</h3>

      <p>Comunica is implemented in TypeScript/JavaScript as a collection of Node modules, which are able to run in Web browsers using native Web technologies.
Comunica is available under an open license on <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​1202509#.Wq9GZhNuaHo">GitHub</a>
and on the <a href="https://www.npmjs.com/org/comunica" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​org/​comunica">NPM package manager</a>.
The 79 Comunica modules are tested thoroughly, with more than 1,200 unit tests reaching a test coverage of 100%.
In order to be compatible with existing JavaScript <abbr title='Resource Description Framework'>RDF</abbr> libraries,
Comunica follows the JavaScript <abbr title='Application Programming Interface'>API</abbr> specification by the <a href="https://www.w3.org/community/rdfjs/" class="mandatory" data-link-text="https:/​/​www.w3.org/​community/​rdfjs/​">RDFJS community group</a>,
and will <a href="https://www.w3.org/community/rdfjs/2018/04/23/rdf-js-the-new-rdf-and-linked-data-javascript-library/">actively be further aligned</a> within this community.
In order to encourage collaboration within the community, we extensively use the <a href="https://github.com/comunica/comunica/issues" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​issues">GitHub issue tracker</a>
for planned features, bugs and other issues.
Finally, we publish detailed <a href="https://comunica.readthedocs.io" class="mandatory" data-link-text="https:/​/​comunica.readthedocs.io">documentation</a> for the usage and development of Comunica.</p>

      <p>We provide a default Linked Data-based configuration file with all available actors for evaluating federated <em><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries</em> over heterogeneous sources.
This allows <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries to be evaluated using a command-line tool,
from a Web service implementing the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> protocol</a> <span class="references">[<a href="#ref-89">89</a>]</span>,
within a JavaScript application,
or within the browser.
We fully implemented <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.0</a> <span class="references">[<a href="#ref-114">114</a>]</span> and a subset of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1</a> <span class="references">[<a href="#ref-3">3</a>]</span> at the time of writing.
In future work, we intend to implement additional actors for supporting <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 completely.</p>

      <p>Comunica currently supports querying over the following types of <em>heterogeneous datasources and interfaces</em>:</p>

      <ul>
        <li><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments interfaces</a></span> <span class="references">[<a href="#ref-39">39</a>]</span></li>
        <li>Quad Pattern Fragments interfaces (<a href="https://github.com/LinkedDataFragments/Server.js/tree/feature-qpf-latest" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Server.js/​tree/​feature-​qpf-​latest">an experimental extension of <abbr title='Triple Pattern Fragments'>TPF</abbr> with a fourth graph element</a>)</li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints</a> <span class="references">[<a href="#ref-89">89</a>]</span></li>
        <li>Local and remote dataset dumps in <abbr title='Resource Description Framework'>RDF</abbr> serializations.</li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr> datasets</a> <span class="references">[<a href="#ref-54">54</a>]</span></li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-demo/">Versioned <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr> datasets</a> <span class="references">[<a href="#ref-115">115</a>]</span></li>
      </ul>

      <p>In order to demonstrate Comunica’s ability to evaluate <em>federated</em> query evaluation over <em>heterogeneous</em> sources,
the following guide shows how you can <a href="https://gist.github.com/rubensworks/34bb69fa6c83176bce60a5e8a25051e8" class="mandatory" data-link-text="https:/​/​gist.github.com/​rubensworks/​34bb69fa6c83176bce60a5e8a25051e8">try this out in Comunica yourself</a>.</p>

      <p>Support for new algorithms, query operators and interfaces can be implemented in an external module,
without having to create a custom fork of the engine.
The module can then be <em>plugged</em> into existing or new engines that are identified by
<a href="https://github.com/comunica/comunica/blob/master/packages/actor-init-sparql/config/config-default.json" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​blob/​master/​packages/​actor-​init-​sparql/​config/​config-​default.json"><abbr title='Resource Description Framework'>RDF</abbr> configuration files</a>.</p>

      <p>In the future, we will also look into adding support for other interfaces such as
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF</a></span> <span class="references">[<a href="#ref-95">95</a>]</span> for more efficient join operations
and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><abbr title='Versioned Triple Pattern Fragments'>VTPF</abbr></a> <span class="references">[<a href="#ref-41">41</a>]</span> for queries over versioned datasets.</p>

    </section>

    <section id="querying_comparison-tpf-client">
      <h3>Performance Analysis</h3>

      <p>One of the goals of Comunica is to replace the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client as a more <em>flexible</em> and <em>modular</em> alternative,
with at least the same <em>functionality</em> and similar <em>performance</em>.
The fact that Comunica supports multiple heterogeneous interfaces and sources as shown in the previous section
validates this flexibility and modularity, as the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client only supports querying over <abbr title='Triple Pattern Fragments'>TPF</abbr> interfaces.</p>

      <p>Next to a functional completeness, it is also desired that Comunica achieves similar <em>performance</em> compared to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
The higher modularity of Comunica is however expected to cause performance overhead,
due to the additional bus and mediator communication, which does not exist in the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Hereafter, we compare the performance of the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client and Comunica
and discover that Comunica has similar performance to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
As the main goal of Comunica is modularity, and not <em>absolute</em> performance, we do not compare with similar frameworks such as ARQ and RDFLib.
Instead, <em>relative</em> performance of evaluations using <em>the same engine</em> under <em>different configurations</em> is key for comparisons,
which will be demonstrated using Comunica hereafter.</p>

      <p>For the setup of this evaluation we used a single machine (Intel Core i5-3230M CPU at 2.60 GHz with 8 GB of RAM),
running the Linked Data Fragments server with a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><abbr title='Header Dictionary Triples'>HDT</abbr>-backend</a> <span class="references">[<a href="#ref-54">54</a>]</span> and the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client or Comunica,
for which the exact versions and configurations will be linked in the following workflow.
The main goal of this evaluation is to determine the performance impact of Comunica,
while keeping all other variables constant.</p>

      <p>In order to illustrate the benefit of modularity within Comunica,
we evaluate using two different configurations of Comunica.
The first configuration (<em>Comunica-sort</em>) implements a BGP algorithm that is similar to that of the original <abbr title='Triple Pattern Fragments'>TPF</abbr> Client:
it sorts triple patterns based on their estimated counts and evaluates and joins them in that order.
The second configuration (<em>Comunica-smallest</em>) implements a simplified version of this BGP algorithm that does not sort <em>all</em> triple patterns in a BGP,
but merely picks the triple pattern with the smallest estimated count to evaluate on each recursive call, leading to slightly different query plans.</p>

      <p>We used the following <a about="#evaluation-workflow" content="Comunica evaluation workflow" href="#evaluation-workflow" property="rdfs:label" rel="cc:license" resource="https://creativecommons.org/licenses/by/4.0/">evaluation workflow</a>:</p>

      <ol id="evaluation-workflow" property="schema:hasPart" resource="#evaluation-workflow" typeof="opmw:WorkflowTemplate">
<li id="workflow-data" about="#workflow-data" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Generate a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13"><a href="http://dx.doi.org/10.1007/978-3-319-11964-9_13">WatDiv</a></span> <span class="references">[<a href="#ref-116">116</a>]</span> dataset with scale factor=100.</p>
        </li>
<li id="workflow-queries" about="#workflow-queries" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Generate the corresponding default WatDiv <a href="https://github.com/comunica/test-comunica/tree/ISWC2018/sparql/watdiv-10M" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​test-​comunica/​tree/​ISWC2018/​sparql/​watdiv-​10M">queries</a> with query-count=5.</p>
        </li>
<li id="workflow-tpf-server" about="#workflow-tpf-server" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-config.jsonld" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​config.jsonld">the server software configuration</a>, implementing the <a href="https://www.hydra-cg.com/spec/latest/triple-pattern-fragments/" class="mandatory" data-link-text="https:/​/​www.hydra-​cg.com/​spec/​latest/​triple-​pattern-​fragments/​"><abbr title='Triple Pattern Fragments'>TPF</abbr> specification</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-setup.ttl">dependencies</a>.</p>
        </li>
<li id="workflow-tpf-client" about="#workflow-tpf-client" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://github.com/LinkedDataFragments/Client.js" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Client.js">the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client software</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 protocol</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-client.ttl" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​client.ttl">dependencies</a>.</p>
        </li>
<li id="workflow-tpf-run" about="#workflow-tpf-run" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client, after doing a warmup run, and record the execution times <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-ldf.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​ldf.csv">results</a>.</p>
        </li>
<li id="workflow-comunica-sort" about="#workflow-comunica-srt" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-sort.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​sort.json">the Comunica software configuration</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 protocol</a>, with its <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/comunica-npm.ttl" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​comunica-​npm.ttl">dependencies</a>, using the <em>Comunica-sort</em> algorithm.</p>
        </li>
<li id="workflow-comunica-run-sort" about="#workflow-comunica-run-sort" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica-sort.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica-​sort.csv">execution times</a>.</p>
        </li>
<li id="workflow-comunica-smallest" about="#workflow-comunica-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Update the Comunica installation to use a new <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-smallest.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​smallest.json">configuration</a> supporting the <em>Comunica-smallest</em> algorithm.</p>
        </li>
<li id="workflow-comunica-run-smallest" about="#workflow-comunica-run-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica.csv">execution times</a>.</p>
        </li>
</ol>

      <figure id="querying_performance-average">
<center>
<img src="querying/img/avg.svg" alt="[performance-average]" class="plot" />
<img src="querying/img/avg_c23.svg" alt="[performance-average]" class="plot" />
</center>
<figcaption>
          <p><span class="label">Fig. 59:</span> Average query evaluation times for the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client, Comunica-sort, and Comunica-smallest for all queries (shorter is better).
C2 and C3 are shown separately because of their higher evaluation times.</p>
        </figcaption>
</figure>

      <p>The results from <a href="#querying_performance-average">Fig. 59</a> show that Comunica is able to achieve similar performance compared to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client.
Concretely, both Comunica variants are faster for 11 queries, and slower for 9 queries.
However, the difference in evaluation times is in most cases very small,
and are caused by implementation details, as the implemented algorithms are equivalent.
Contrary to our expectations, the performance overhead of Comunica’s modularity is negligible.
Comunica therefore improves upon the <abbr title='Triple Pattern Fragments'>TPF</abbr> Client in terms of <em>modularity</em> and <em>functionality</em>, and achieves similar <em>performance</em>.</p>

      <p>These results also illustrate the simplicity of comparing different algorithms inside Comunica.
In this case, we compared an algorithm that is similar to that of the original <abbr title='Triple Pattern Fragments'>TPF</abbr> Client with a simplified variant.
The results show that the performance is very similar, but the original algorithm (Comunica-sort) is faster in most of the cases.
It is however not always faster, as illustrated by query C1, where Comunica-sort is almost a second slower than Comunica-smallest.
In this case, the heuristic algorithm of the latter was able to come up with a slightly better query plan.
Our goal with this result is to show that Comunica can easily be used to compare such different algorithms,
where future work can focus on smart mediator algorithms to choose the best BGP actor in each case.</p>

    </section>

    <section id="querying_conclusions">
      <h3>Conclusions</h3>

      <p>In this work, we introduced Comunica as a highly modular meta engine for federated <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation over heterogeneous interfaces.
Comunica is thereby the first system that accomplishes the Linked Data Fragments vision of a client that is able to query over heterogeneous interfaces.
Not only can Comunica be used as a client-side <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engine, it can also be customized to become a more lightweight engine and perform more specific tasks,
such as for example only evaluating BGPs over Turtle files,
evaluating the efficiency of different join operators,
or even serve as a complete server-side <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query endpoint that aggregates different datasources.
In future work, we will look into supporting supporting alternative (non-semantic) query languages as well, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://facebook.github.io/graphql/October2016/">GraphQL</a> <span class="references">[<a href="#ref-117">117</a>]</span>.</p>

      <p>If you are a Web researcher, then Comunica is the ideal research platform
for investigating new Linked Data publication interfaces,
and for experimenting with different query algorithms.
New modules can be implemented independently without having to fork existing codebases.
The modules can be combined with each other using an <abbr title='Resource Description Framework'>RDF</abbr>-based configuration file
that can be instantiated into an actual engine through dependency injection.
However, the target audience is broader than just the research community.
As Comunica is built on Linked Data and Web technologies,
and is extensively documented and has a ready-to-use <abbr title='Application Programming Interface'>API</abbr>,
developers of <abbr title='Resource Description Framework'>RDF</abbr>-consuming (Web) applications can also make use of the platform.
In the future, we will continue <a href="https://github.com/comunica/comunica/wiki/Sustainability-Plan" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​wiki/​Sustainability-​Plan">maintaining</a>
and developing Comunica and intend to support and collaborate with future researchers on this platform.</p>

      <p>The introduction of Comunica will trigger a <em>new generation of Web querying research</em>.
Due to its flexibility and modularity,
existing areas can be <em>combined</em> and <em>evaluated</em> in more detail,
and <em>new promising areas</em> that remained covered so far will be exposed.</p>

    </section>

    <div class="subfooter">
  <section id="querying_acknowledgements">
        <h3 class="no-label-increment">Acknowledgements</h3>

        <p>The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

      </section>

</div>
  </section>

  <section class="sub-paper">
    <h2 id="querying-evolving">Querying Evolving Data</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
      <li><a href="https://pietercolpaert.be/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
      <li><a href="https://www.ugent.be/ea/idlab/en/members/erik-mannens.htm" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/erik_mannens">Erik Mannens</a></li>
      <li><a href="https://www.ugent.be/ea/idlab/en/members/rik-van-de-walle.htm" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/rik_van_de_walle">Rik Van de Walle</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://www.rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf">Continuous Client-side Query Evaluation over Dynamic Linked Data</a></p>

    <section id="querying-evolving_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <p>Existing solutions to query dynamic Linked Data sources extend the <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> language,
and require continuous server processing for each query.
Traditional <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints already accept highly expressive queries,
so extending these endpoints for time-sensitive queries increases the server cost even further.
To make continuous querying over dynamic Linked Data more affordable,
we extend the low-cost Triple Pattern Fragments (<abbr title='Triple Pattern Fragments'>TPF</abbr>)
interface with support for time-sensitive queries.
In this paper, we introduce the <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer that allows clients to evaluate <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries
with continuously updating results.
Our experiments indicate that this extension significantly lowers the server complexity,
at the expense of an increase in the execution time per query.
We prove that by moving the complexity of continuously evaluating queries over
dynamic Linked Data to the clients and thus increasing bandwidth usage,
the cost at the server side is significantly reduced.
Our results show that this solution makes real-time querying more scalable for a large amount
of concurrent clients when compared to the alternatives.</p>

    </section>

    <section id="querying-evolving_introduction">
      <h3>Introduction</h3>

      <p>As the Web of Data is a <em>dynamic</em> dataspace, different results may be returned depending on when a question was asked.
The end-user might be interested in seeing the query results update over time,
for instance, by re-executing the entire query over and over again (“polling”).
This is, however, not very practical,
especially if it is unknown beforehand when data will change.
An additional problem is that
many public (even static) <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query endpoints suffer from a low availability <span class="references">[<a href="#ref-118">118</a>]</span>.
The unrestricted complexity of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries <span class="references">[<a href="#ref-119">119</a>]</span> combined
with the public character of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints entails a high server cost, which makes it expensive to host such an interface with high availability.
<em>Dynamic</em> <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> streaming solutions offer combined access to dynamic data streams and
static background data through continuously executing queries. Because of this continuous querying, the cost
for these servers is even higher than with static querying.</p>

      <p>In this work, we therefore devise a solution that enables clients to continuously evaluate non-high frequency
queries by polling specific fragments of the data.
The resulting framework performs this without the server needing to remember any client state.
Its mechanism requires the server to <em>annotate</em> its data so that the client can efficiently determine when to retrieve fresh data.
The generic approach in this paper is applied to the use case of public transit route planning.
It can be used in various other domains with continuously updating data, such as smart city dashboards, business intelligence, or sensor networks.
This paper extends our earlier work <span class="references">[<a href="#ref-120">120</a>]</span> with additional experiments.</p>

      <p>In the next section, we discuss related research on which our solution will be based.
After that, <a href="#querying-evolving_problem-statement">Subsection 5.4</a> gives a general problem statement.
In <a href="#querying-evolving_use-case">Subsection 5.5</a>, we present a motivating use case.
<a href="#querying-evolving_dynamic-data-representation">Subsection 5.6</a> discusses different techniques to represent dynamic data,
after which <a href="#querying-evolving_query-engine">Subsection 5.7</a> gives an explanation of our proposed query solution.
Next, <a href="#querying-evolving_evaluation">Subsection 5.8</a> shows an overview of our experimental setup and its results.
Finally, <a href="#querying-evolving_conclusions">Subsection 5.9</a> discusses the conclusions of this work with further research opportunities.</p>

    </section>

    <section id="querying-evolving_related-work">
      <h3>Related Work</h3>

      <p>In this section, we first explain techniques to perform <abbr title='Resource Description Framework'>RDF</abbr> annotation,
which will be used to determine freshness.
Then, we zoom in on possible representations of temporal data in <abbr title='Resource Description Framework'>RDF</abbr>.
We finish by discussing existing <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> streaming extensions
and a low-cost (static) Linked Data publication technique.</p>

      <h4 id="querying-evolving_related-work_annotations"><abbr title='Resource Description Framework'>RDF</abbr> Annotations</h4>

      <p>Annotations allow us to attach metadata to triples.
We might for example want to say that a triple is
only valid within a certain time interval, or that a triple is only valid in a certain geographical area.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/"><abbr title='Resource Description Framework'>RDF</abbr> 1.0</a> <span class="references">[<a href="#ref-121">121</a>]</span> allows triple annotation through <em>reification</em>.
This mechanism uses <em>subject</em>, <em>predicate</em>, and <em>object</em> as predicates, which allow the addition of annotations
to such reified <abbr title='Resource Description Framework'>RDF</abbr> triples.
The downside of this approach is that one triple is now transformed to three triples, which significantly increases the
total amount of triples.</p>

      <p>Singleton Properties <span class="references">[<a href="#ref-122">122</a>]</span> create unique instances (singletons) of predicates, which then can be used for further specifying
that relationship, for example, by adding annotations. New instances of predicates are created by relating them to the
old predicate through the <code>sp:singletonPropertyOf</code> predicate.
While this approach requires fewer triples than reification to represent the same information, it still has
the issue of the original triple being lost, because the predicate is changed in this approach.</p>

      <p>With <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><abbr title='Resource Description Framework'>RDF</abbr> 1.1</a> <span class="references">[<a href="#ref-2">2</a>]</span> came <em>graph</em> support, which allows triples to be encapsulated into named graphs, which can also be annotated.
Graph-based annotation requires fewer triples than both reification and singleton properties when representing
the same information. It requires the addition of a fourth element to the triple which transforms it to a quad.
This fourth element, the <em>graph</em>, can be used to add the annotations to.</p>

      <h4 id="temporal-data-in-the-rdf-model">Temporal data in the <abbr title='Resource Description Framework'>RDF</abbr> model</h4>

      <p>Regular <abbr title='Resource Description Framework'>RDF</abbr> triples cannot express the time and space in which the fact they describe is true.
In domains where data needs to be represented for certain times or time ranges, these traditional representations
should thus be extended.
There are two main mechanisms for adding time <span class="references">[<a href="#ref-123">123</a>]</span>.
<em>Versioning</em> will take snapshots of the complete graph every time a change occurs.
<em>Time labeling</em> will annotate triples with their change time.
The latter is believed to be a better approach in the context of <abbr title='Resource Description Framework'>RDF</abbr>,
because complete snapshots introduce overhead,
especially if only a small part of the graph changes.
Gutierrez et al. made a distinction between <em>point-based</em> and <em>interval-based</em> labeling,
which are interchangeable <span class="references">[<a href="#ref-124">124</a>]</span>.
The former states information about an element at a certain time instant, while the latter states
information at all possible times between two time instants.</p>

      <p>The same authors introduced a temporal vocabulary <span class="references">[<a href="#ref-124">124</a>]</span> for the discussed mechanisms, which will
be referred to as <code>tmp</code> in the remainder of this chapter. Its core predicates are:</p>

      <ul>
        <li><code>tmp:interval</code>: This predicate can be used on a subject to make it valid in a certain time interval.
  The range of this property is a time interval, which is represented by the two mandatory
  properties <code>tmp:initial</code> and <code>tmp:final</code>.</li>
        <li><code>tmp:instant</code>: Used on subjects to make it valid on a certain time instant as a point-based time representation.
  The range of this property is <code>xsd:dateTime</code>.</li>
        <li><code>tmp:initial</code> and <code>tmp:final</code>: The domain of these predicates is a time interval.
  Their range is a <code>xsd:dateTime</code>, and they respectively indicate the start and the
  end of the interval-based time representation.</li>
      </ul>

      <p>Next to these properties, we will also introduce our own predicate <code>tmp:expiration</code> with range <code>xsd:dateTime</code>
which indicates that the subject is only valid up until the given time.</p>

      <h4 id="sparql-streaming-extensions"><abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Streaming Extensions</h4>

      <p>Several <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> extensions exist that enable querying over data streams.
These data streams are traditionaly represented as a monotonically non-decreasing stream of triples that are annotated with their timestamp.
These require <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf"><em>continuous processing</em></a> <span class="references">[<a href="#ref-32">32</a>]</span> of queries because of the constantly changing data.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1526709.1526856"><abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr></a> <span class="references">[<a href="#ref-15">15</a>]</span> is an approach to querying over static and dynamic data.
This system requires the client to <em>register</em> a query to the server in an extended
<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> syntax that allows the use of <em>windows</em> over dynamic data.
This <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf"><em>query registration</em></a> <span class="references">[<a href="#ref-32">32</a>, <a href="#ref-125">125</a>]</span>
must occur by clients to make sure that the streaming-enabled <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoint can continuously re-evaluate this query,
as opposed to traditional endpoints where the query is evaluated only once.
A <em>window</em> <span class="references">[<a href="#ref-126">126</a>]</span> is a subsection of facts ordered by time so that not all available
information has to be taken into account while processing. These windows can have a certain size which
indicates the time range and is advanced in time by a <em>stepsize</em>.
<abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>’s execution of queries is based on the combination of a regular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engine with a
<em>Data Stream Management System</em> (<abbr title='Data Stream Management System'>DSMS</abbr>) <span class="references">[<a href="#ref-126">126</a>]</span>. The internal model of <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> 
creates queries that distribute work between the <abbr title='Data Stream Management System'>DSMS</abbr> and the <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> engine to respectively
process the dynamic and static data.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf"><em><abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr></em></a> <span class="references">[<a href="#ref-16">16</a>]</span> is a <em>white box</em> approach, as opposed to <em>black box</em>
approaches like <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>.
This means that <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> natively implements all query operators without transforming it to another
language, removing the overhead of delegating it to another system.
The syntax is similar to that of <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>, also supporting query registration and time windows.
According to previous research <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> performs much better than <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> for large
datasets; for simple queries and small datasets the opposite is true.</p>

      <h4 id="triple-pattern-fragments">Triple Pattern Fragments</h4>

      <p>Experiments have shown that more than half of public <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints have an availability of less than 95% <span class="references">[<a href="#ref-118">118</a>]</span>.
Any number of clients can send arbitrarily complex <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries, which could form a bottleneck in endpoints.
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><em>Triple Pattern Fragments</em> (<abbr title='Triple Pattern Fragments'>TPF</abbr>)</a></span> <span class="references">[<a href="#ref-39">39</a>]</span> aim to solve this issue of high interface cost by moving part of
the query evaluation to the client, which reduces the server load, at the cost of increased query times and bandwidth.
The purposely limited interface only accepts separate triple pattern queries.
Clients can use it to evaluate more complex <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries locally,
also over federations of interfaces.</p>

    </section>

    <section id="querying-evolving_problem-statement">
      <h3>Problem Statement</h3>

      <p>In order to lower server load during continuous query evaluation,
we move a significant part of the query evaluation from server to client.
We annotate dynamic data with their valid time to make it possible for clients
to derive an optimal query evaluation frequency.</p>

      <p>For this research, we identified the following research questions:</p>

      <blockquote id="querying-evolving_researchquestion1">
        <p>Can clients use volatility knowledge to perform more efficient continuous <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query evaluation by polling for data?</p>
      </blockquote>

      <blockquote id="querying-evolving_researchquestion2">
        <p>How does the client and server load of our solution compare to alternatives?</p>
      </blockquote>

      <blockquote id="querying-evolving_researchquestion3">
        <p>How do different time-annotation methods perform in terms of the resulting execution times?</p>
      </blockquote>

      <p>These research questions lead to the following hypotheses:</p>

      <ol>
    <li id="querying-evolving_hypothesis1">The proposed framework has a lower server cost than alternatives.</li>
    <li id="querying-evolving_hypothesis2">The proposed framework has a higher client cost than streaming-based <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> approaches for equivalent queries.</li>
    <li id="querying-evolving_hypothesis3">Client-side caching of static data reduces the execution times proportional to the fraction of static triple patterns that are present in the query.</li>
</ol>

    </section>

    <section id="querying-evolving_use-case">
      <h3>Use Case</h3>

      <p>A guiding use case, based on public transport, will be referred to in the remainder of this paper.
When public transport route planning applications return dynamic data,
they can account for factors such as train delays
as part of a continuously updating route plan.
In this use case, different clients need to obtain all train departure information for a certain station.
This requires the following concepts:</p>

      <ul>
        <li><strong>Departure</strong> (<em>static</em>): Unique <abbr title='Internationalized Resource Identifier'>IRI</abbr> for the departure of a certain train.</li>
        <li><strong>Headsign</strong> (<em>static</em>): The label of the train showing its destination.</li>
        <li><strong>Departure Time</strong> (<em>static</em>): The <em>scheduled</em> departure time of the train.</li>
        <li><strong>Route Label</strong> (<em>static</em>): The identifier for the train and its route.</li>
        <li><strong>Delay</strong> (<em>dynamic</em>): The delay of the train, which can increase through time.</li>
        <li><strong>Platform</strong> (<em>dynamic</em>): The platform number of the station at which the train will depart, which can be changed through time if delays occur.</li>
      </ul>

      <p><a href="#querying-evolving_listing:ta:originaltriples">Listing 9</a> shows example data in this model.
The <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query in <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a>
can retrieve all information using this basic data model.</p>

      <figure id="querying-evolving_listing:ta:originaltriples" class="listing">
<pre><code>@prefix t: &lt;http://example.org/train/&gt;.
</code><code>@prefix td: &lt;http://example.org/traindata/&gt;.
</code><code>td:departure-48 t:delay         &quot;0S&quot;^^xsd:xs:duration;
</code><code>                t:platform      td:platform-1a;
</code><code>                t:departureTime &quot;2014-12-05T10:37:00+01:00&quot;^^xsd:dateTimeStamp;
</code><code>                t:headSign      &quot;Ghent&quot;;
</code><code>                t:routeLabel    &quot;IC 1831&quot;.
</code></pre>
<figcaption>
          <p><span class="label">Listing 9:</span> Train information with static time information according to the basic data model.</p>
        </figcaption>
</figure>

      <figure id="querying-evolving_listing:usecase:basicquery" class="listing">
<pre><code>SELECT ?delay ?platform ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    _:id t:delay         ?delay.
</code><code>    _:id t:platform      ?platform.
</code><code>    _:id t:departureTime ?departureTime.
</code><code>    _:id t:headSign      ?headSign.
</code><code>    _:id t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 6:</span> The basic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query for retrieving all upcoming train departure information in a certain station.
The two first triple patterns are dynamic, the last three are static.</p>
        </figcaption>
</figure>

    </section>

    <section id="querying-evolving_dynamic-data-representation">
      <h3>Dynamic Data Representation</h3>

      <p>Our solution consists of a partial redistribution of query evaluation workload from the server to the client,
which requires the client to be able to access the server data.
There needs to be a distinction between regular static data and continuously updating dynamic data in the server’s
dataset.
For this, we chose to define a certain temporal range in which these dynamic facts are valid, as a consequence
the client will know when the data becomes invalid and has to fetch new data to remain up-to-date.
To capture the temporal scope of data triples, we annotate this data with time.
In this section, we discuss two different types of time labeling, and different methods to annotate this data.</p>

      <h4 id="query-evolving_subsec:temporaldomains">Time Labeling Types</h4>

      <p>We use interval-based labeling to indicate the <em>start and endpoint</em> of the period during which triples are valid.
Point-based labeling is used to indicate the <em>expiration time</em>.</p>

      <p>With expiration times, we only save the latest version of a given fact in a dataset, assuming that 
the old version can be removed when a newer one arrives.
These expiration times provide enough information to determine when a certain fact becomes invalid in time.
We use time intervals for storing multiple versions of the same fact, i.e., for maintaining a history of facts.
These time intervals must indicate a start- and endtime for making it possible to distinguish between different versions
of a certain fact. These intervals cannot overlap in time for the same facts.
When data is volatile, consecutive interval-based facts will accumulate quickly.
Without techniques to aggregate or remove old data, datasets will quickly grow, which can cause increasingly slower query executions.
This problem does not exist with expiration times because in this approach we decided to only save the latest version of a fact, so
this volatility will not have any effect on the dataset size.</p>

      <h4 id="query-evolving_sec:tatypes">Methods for Time Annotation</h4>

      <p>The two time labeling types introduced in the last section can be annotated on triples in different ways.
In <a href="#querying-evolving_related-work_annotations">Subsubsection 5.3.1</a> we discussed several methods for <abbr title='Resource Description Framework'>RDF</abbr> annotation.
We will apply time labels to triples using the singleton properties, graphs and implicit graphs annotation techniques.</p>

      <p><strong>Singleton Properties</strong>
<em>Singleton properties</em> annotation is done by creating
a singleton property for the predicate of each dynamic triple.
Each of these singleton properties can then be annotated with its time annotation, being either
a time interval or expiration times.</p>

      <h5 id="graphs">Graphs</h5>
      <p>To time-annotate triples using <em>graphs</em>, we can encapsulate triples inside contexts,
and annotate each context graph with a time annotation.</p>

      <h5 id="implicit-graphs">Implicit Graphs</h5>
      <p>A <abbr title='Triple Pattern Fragments'>TPF</abbr> interface gives a unique <abbr title='Internationalized Resource Identifier'>IRI</abbr> to each fragment corresponding to a triple pattern, including patterns without variables, i.e., actual triples.
Since Triple Pattern Fragments are the basis of our solution, we can interpret each fragment as a graph.
We will refer to these as <em>implicit graphs</em>.
This <abbr title='Internationalized Resource Identifier'>IRI</abbr> can then be used as graph identifier for this triple for adding time information.
For example, the <abbr title='Internationalized Resource Identifier'>IRI</abbr> for the triple <code>&lt;s&gt; &lt;p&gt; &lt;o&gt;</code> on the <abbr title='Triple Pattern Fragments'>TPF</abbr> interface located at <code>http:/​/​example.org/dataset/</code> is <code>http:/​/​example.org/dataset?subject=s&amp;predicate=p&amp;object=o</code>.</p>

      <p>The choice of time annotation method for publishing temporal data will also depend on its capability to
<em>group</em> time labels.
If certain dynamic triples have identical time labels, these annotations can be shared to further reduce the required
amount of triples if we are using singleton properies or graphs.
When we would have three train delay triples which are valid for the same time interval using
graph annotation, these three triples can be placed in the same graph.
This will make sure they refer to the same time interval without having to replicate this annotation two times more.
In the case of implicit graph annotation, this grouping of triples is not possible, because each triple has a unique
graph identifier determined by the interface.
This would be possible if these different identifiers are linked to each other with
for example <code>owl:sameAs</code> relationships that our query engine takes into account, which would introduce further overhead.</p>

      <p>We will execute our use case for each of these annotation methods.
In practise, an annotation method must be chosen depending on the requirements and available technologies.
If we have a datastore that supports quads, graph-based annotation is the best choice because of it requires the least amount of triples.
If our datastore does not support quads, we can use singleton properties.
If we have a <abbr title='Triple Pattern Fragments'>TPF</abbr>-like interface at which our data is hosted, we can use implicit graphs as annotation technique,
if however many of those triples can be grouped under the same time label, singleton properties are a better alternative because
the latter has grouping support.</p>

    </section>

    <section id="querying-evolving_query-engine">
      <h3>Query Engine</h3>

      <p><abbr title='Triple Pattern Fragments'>TPF</abbr> query evaluation involves server and client software, because the client actively takes part in the
query evaluation, as opposed to traditional <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> endpoints where the server does all of the work.
Our solution allows users to send a normal <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query to the local query engine
which autonomously detects the dynamic parts of the query and continuously sends back results
from that query to the user.
In this section, we discuss the architecture of our proposed solution and the most important
algorithms that were used to implement this.</p>

      <h4 id="architecture">Architecture</h4>

      <p>Our solution must be able to handle regular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 queries,
detect the dynamic parts, and produce continuously updating results for non-high frequency queries.
To achieve this, we chose to build an extra software layer on top of the existing <abbr title='Triple Pattern Fragments'>TPF</abbr> client that
supports each discussed labeling type and annotation method and is capable of doing
dynamic query transformation and result streaming.
At the <abbr title='Triple Pattern Fragments'>TPF</abbr> server, dynamic data must be annotated with time depending on the
used combination of labeling type and method.
The server expects dynamic data to be pushed to the platform by an external process with varying data.
In the case of graph-based annotation, we have to extend the <abbr title='Triple Pattern Fragments'>TPF</abbr> server implementation,
so that it supports quads.
This dynamic data should be pushed to the platform by an external process with varying data.</p>

      <figure id="querying-evolving_fig:architecture">
<img src="querying-evolving/img/solution-architecture.svg" alt="[<abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer architecture]&#8221; />
<figcaption>
          <p><span class="label">Fig. 60:</span> Overview of the proposed client-server architecture.</p>
        </figcaption>
</figure>

      <p><a href="#querying-evolving_fig:architecture">Fig. 60</a> shows an overview of the architecture for this extra layer on top of the
<abbr title='Triple Pattern Fragments'>TPF</abbr> client, which will be called the <em><abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer</em> from now on.
The left-hand side shows the <em>User</em> that can send a regular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query to the <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer
entry-point and receives a stream of query results.
The system can execute queries through the local <em>Basic Graph Iterator</em>, which is part of
the <abbr title='Triple Pattern Fragments'>TPF</abbr> client and executes queries against a <abbr title='Triple Pattern Fragments'>TPF</abbr> server.</p>

      <p>The <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer consists of six major components.
First, there is the <em>Rewriter</em> module which is executed only once at the start of the query streaming loop.
This module is able to transform the original input query into a <em>static</em> and a <em>dynamic query</em>
which will respectively retrieve the static background data and the time-annotated changing data.
This transformation happens by querying metadata of the triple patterns against the entry-point through
the local <abbr title='Triple Pattern Fragments'>TPF</abbr> client.
The <em>Streamer</em> module takes this dynamic query, executes it and forwards its results
to the <em>Time Filter</em>.
The <em>Time Filter</em> checks the time annotation for each of the results and rejects those that are
not valid for the current time.
The minimal expiration time of all these results is then determined and used as a delayed call to the
<em>Streamer</em> module to continue with the <em>streaming loop</em>, which is determined by the repeated
invocation of the <em>Streamer</em> module.
This minimal expiration time will make sure that when at least one of the results expire, a new set
of results will be fetched as part of the next query iteration.
The filtered dynamic results will be passed on to the <em>Materializer</em> which is responsible for
creating <em>materialized static queries</em>.
This is a transformation of the <em>static query</em> with the dynamic results filled in.
These <em>materialized static queries</em> are passed to the <em>Result Manager</em> which is able to cache
these queries.
Finally, the <em>Result Manager</em> retrieves previous <em>materialized static query</em> results from
the local cache or executes this query for the first time and stores its results in the cache.
These results are then sent to the client who had initiated continuous query.</p>

      <h4 id="algorithms">Algorithms</h4>

      <p><strong>Query rewriting</strong>
As mentioned in the previous section, the <em>Rewriter</em> module performs a preprocessing step
that can transform a regular <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 query into a static and dynamic query.
A first step in this transformation is to detect which triple patterns inside the original query
refer to static triples and which refer to dynamic triples.
We detect this by making a separate query for each of the triple patterns and transforming each of them
to a dynamic query.
An example of such a transformation can be found in <a href="#query-evolving_listing:example:dynamic"></a>.
We then evaluate each of these transformed queries and assume a triple pattern is
<em>dynamic</em> if its corresponding query has at least one result.
Another step before the actual query splitting is the conversion of blank nodes to variables.
We will end up with one static query and one dynamic query,
in case these graphs were originally connected, they still need to be connected after the query splitting.
This connection is only possible with variables that are visible,
meaning that these variables need to be part of the <code>SELECT</code> clause.
However, a variable can also be anonymous and not visible: these are blank nodes.
To make sure that we take into account blank nodes that connect the static and dynamic graph,
these nodes have to be converted to variables, while maintaining their semantics.
After this step, we iterate over each
triple pattern of the original query and assign them to either the static or the dynamic query
depending on whether or not the pattern is respectively static or dynamic.
This assignment must maintain the hierarchical structure of the original query,
in some cases this causes triple patterns to be present in the dynamic query when using complex operators
like \union to maintain correct query semantics.
An example of this query transformation for our basic query from <a href="#query-evolving_listing:usecase:basicquery"></a>
can be found in <a href="#query-evolving_listing:usecase:staticquery"></a> and <a href="#query-evolving_listing:usecase:dynamicquery"></a>.</p>

      <figure id="querying-evolving_listing:example:dynamic" class="listing">
<pre><code>SELECT ?s ?p ?o ?time WHERE {
</code><code>    GRAPH ?g0 { ?s ?p ?o }
</code><code>    ?g0 tmp:expiration ?time
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 7:</span> Dynamic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query for the triple pattern <code>?s ?p ?o</code> for graph-based annotation with expiration times.</p>
        </figcaption>
</figure>

      <figure id="querying-evolving_listing:usecase:dynamicquery" class="listing">
<pre><code>SELECT ?id ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    ?id t:departureTime ?departureTime.
</code><code>    ?id t:headSign      ?headSign.
</code><code>    ?id t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 8:</span> Static <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query which has been derived from the basic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query from <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a> by the <em>Rewriter</em> module.</p>
        </figcaption>
</figure>

      <figure id="querying-evolving_listing:ta:originaltriples" class="listing">
<pre><code>SELECT ?id ?delay ?platform ?final0 ?final1
</code><code>WHERE {
</code><code>    GRAPH ?g0 { ?id t:delay ?delay. }
</code><code>    ?g0 tmp:expiration ?final0.
</code><code>    GRAPH ?g1 { ?id t:platform ?platform. }
</code><code>    ?g1 tmp:expiration ?final1.
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 9:</span> Dynamic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query which has been derived from the basic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query from <a href="#querying-evolving_listing:usecase:basicquery">Listing 6</a> by the <em>Rewriter</em> module. Graph-based annotation is used with expiration times.</p>
        </figcaption>
</figure>

      <p><strong>Query materialization</strong>
The <em>Materializer</em> module is responsible for creating <em>materialized static queries</em>
from the static query and the current dynamic query results.
This is done by filling in each dynamic result into the static query variables.
It is possible that multiple results are returned from the dynamic query evaluation, which
is the same amount of materialized static queries that can be derived.
Assuming that we, for example, find the following single dynamic query result from the dynamic query in
<a href="#query-evolving_listing:usecase:dynamicquery"></a>:
<span class="kdmath">$\{ \texttt{?id} \mapsto \texttt{<http://example.org/train\#train4815>},
    \texttt{?delay} \mapsto \texttt{&#8220;P10S&#8221;\^{}\^{}xsd:duration}
\}$</span>
then we can derive the materialized static query by filling in these two variables into the static query from
<a href="#query-evolving_listing:usecase:staticquery"></a>, the resulting query can be found in 
<a href="#query-evolving_listing:usecase:materializedstaticquery"></a>.</p>

      <figure id="querying-evolving_listing:usecase:materializedstaticquery" class="listing">
<pre><code>SELECT ?headSign ?routeLabel ?departureTime
</code><code>WHERE {
</code><code>    &lt;http://example.org/train#train4815&gt; t:departureTime ?departureTime.
</code><code>    &lt;http://example.org/train#train4815&gt; t:headSign      ?headSign.
</code><code>    &lt;http://example.org/train#train4815&gt; t:routeLabel    ?routeLabel.
</code><code>    FILTER (?departureTime &gt; &quot;2015-12-08T10:20:00&quot;^^xsd:dateTime).
</code><code>    FILTER (?departureTime &lt; &quot;2015-12-08T11:20:00&quot;^^xsd:dateTime).
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 10:</span> Materialized static <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query derived by filling in the dynamic query results into the static query from <a href="#querying-evolving_listing:usecase:materializedstaticquery">Listing 10</a>.</p>
        </figcaption>
</figure>

      <p><strong>Caching</strong>
The <em>Result manager</em> is the last step in the streaming loop for returning the materialized
static query results of one time instance.
This module is responsible for either getting results for given queries from its cache,
or fetching the results from the <abbr title='Triple Pattern Fragments'>TPF</abbr> client.
First, an identifier will be determined for each materialized static query.
This identifier will serve as a key to cache static data and should correctly
and uniquely identify static results based on dynamic results.
This is equivalent to saying that this identifier should be the <em>connection</em>
between the static and dynamic graphs.
This connection is the intersection of the variables present in the \where clause of the
static and dynamic queries.
Since the dynamic query results are already available at this point, these variables
all have values, so this cache identifier can be represented by these variable results.
The graph connection between the static and dynamic queries from <a href="#query-evolving_listing:usecase:staticquery"></a> and <a href="#query-evolving_listing:usecase:dynamicquery"></a> is <code>?id</code>.
The cache identifier for a result where <code>?id</code> is <code>"train:4815"</code> is for example <code>"?id=train:4815"</code>.</p>

    </section>

    <section id="querying-evolving_evaluation">
      <h3>Evaluation</h3>

      <p>In order to validate our hypotheses from <a href="#querying-evolving_problem-statement">Subsection 5.4</a>, we set up an experiment to measure the
impact of our proposed redistribution of workload between the client and server by simultaneously executing
a set of queries against a server using our proposed solution.
We repeat this experiment for two
state-of-the-art solutions: <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.</p>

      <p>To test the client and server performance, our experiment consisted of one server and ten physical clients.
Each of these clients can execute from one to twenty unique concurrent queries
based on the use case from <a href="#querying-evolving_use-case">Subsection 5.5</a>.
The data for this experiment was derived from real-world Belgian railway data
using the <a href="https://hello.irail.be/api/1-0/" class="mandatory" data-link-text="https:/​/​hello.irail.be/​api/​1-​0/​">iRail <abbr title='Application Programming Interface'>API</abbr></a>.
This results in a series of 10 to 200 concurrent query executions.
This setup was used to test the client and server performance of different <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> streaming approaches.</p>

      <p>For comparing the efficiency of different time annotation methods
and for measuring the effectiveness of our client-side cache,
we measured the execution times of the query for our use case from <a href="#querying-evolving_use-case">Subsection 5.5</a>.
This measurement was done for different annotation methods, once with the cache and once without the cache.
For discovering the evolution of the query evaluation efficiency through time,
the measurements were done over each query stream iteration of the query.</p>

      <p>The discussed architecture was <a href="https://github.com/LinkedDataFragments/QueryStreamer.js/tree/eswc2016" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​QueryStreamer.js/​tree/​eswc2016">implemented in JavaScript using Node.js</a> to
allow for easy communication with the existing <abbr title='Triple Pattern Fragments'>TPF</abbr> client.</p>

      <p>The <a href="https://github.com/rubensworks/TPFStreamingQueryExecutor-experiments/" class="mandatory" data-link-text="https:/​/​github.com/​rubensworks/​TPFStreamingQueryExecutor-​experiments/​">tests</a> were executed on the
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://ilabt.iminds.be/virtualwall">Virtual Wall (generation 2) environment from imec</a> <span class="references">[<a href="#ref-127">127</a>]</span>.
Each machine had two Hexacore Intel E5645 (2.4GHz) CPUs with 24 GB RAM and was running Ubuntu 12.04 LTS.
For <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>, we used <a property="schema:citation http://purl.org/spar/cito/cites" href="https://code.google.com/p/cqels/wiki/<abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>_engine&#8221;>version 1.0.1 of the engine</a> <span class="references">[<a href="#ref-128">128</a>]</span>. For <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>, this was <a property="schema:citation http://purl.org/spar/cito/cites" href="http://streamreasoning.org/download">version 0.9</a> <span class="references">[<a href="#ref-129">129</a>]</span>.
The dataset for this use case consisted of about 300 static triples, and around
200 dynamic triples that were created and removed each ten seconds.
Even this relatively small dataset size already reveals important differences
in server and client cost, as we will discuss in the paragraphs below.</p>

      <h4 id="querying-evolving_subsec:Results-ServerCost">Server Cost</h4>

      <p>The server performance results from our main experiment can be seen in <a href="#querying-evolving_fig:res:scalability-server">Fig. 62</a>.
This plot shows an increasing CPU usage for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> for higher numbers of concurrent query executions.
On the other hand, our solution never reaches more than one percent of server CPU usage.
<a href="#querying-evolving_fig:res:scalability-server-200">Fig. 65</a> shows a detailed view on the measurements in the case of 200 simultaneous
query executions: the CPU peaks for the alternative approaches are much higher and more frequent than for our solution.</p>

      <h4 id="querying-evolving_subsec:Results-ClientCost">Client Cost</h4>

      <p>The results for the average CPU usage across the duration of the query evaluation
of all clients that sent queries to the server in our main experiment can be seen
in <a href="#querying-evolving_fig:res:scalability-client">Fig. 63</a> and <a href="#querying-evolving_fig:res:scalability-client-all">Fig. 66</a>.
The clients that were sending <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> queries to the server had a client
CPU usage of nearly zero percent for the whole duration of the query evaluation.
The clients using the client-side <abbr title='Triple Pattern Fragments'>TPF</abbr> Query Streamer solution that was presented in this work
had an initial CPU peak reaching about 80%, which dropped to about
5% after 4 seconds.</p>

      <h4 id="querying-evolving_subsec:Results-AnnotationMethods">Annotation Methods</h4>

      <p>The execution times for the different annotation methods, once with and once without cache can be seen in <a href="#querying-evolving_fig:res:overview">Fig. 67</a>.
The three annotation methods have about the same relative performance in all figures, but the execution
times are generally lower in the case where the client-side cache was used, except for the first
query iteration.
The execution times for expiration time annotation when no cache is used are constant,
while the execution times with caching slightly decrease over time.</p>

      <figure id="querying-evolving_fig:res:scalability">

<figure id="querying-evolving_fig:res:scalability-server" class="subfigure">
<center><strong>Server load</strong></center>
<img src="querying-evolving/img/scalability.png" />
<figcaption>
            <p><span class="label">Fig. 61:</span> The server CPU usage of our solution proves to be influenced less by the number of clients.</p>
          </figcaption>
</figure>

<figure id="querying-evolving_fig:res:scalability-client" class="subfigure">
<center><strong>Client load</strong></center>
<img src="querying-evolving/img/scalability-client.png" />
<figcaption>
            <p><span class="label">Fig. 63:</span> In the case of 200 concurrent clients,
client CPU usage initially is high after which it converges to about 5%.
The usage for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> is almost non-existing.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Average server and client CPU usage for one query stream for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> and the proposed solution.
Our solution effectively moves complexity from the server to the client.</p>
        </figcaption>
</figure>

      <figure id="querying-evolving_fig:res:boxplots">

<figure id="querying-evolving_fig:res:scalability-server-200" class="subfigure">
<center><strong>Server load</strong></center>
<img src="querying-evolving/img/scalabilityBoxplot-200.png" />
<figcaption>
            <p><span class="label">Fig. 64:</span> Server CPU peaks for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> compared to our solution.</p>
          </figcaption>
</figure>

<figure id="querying-evolving_fig:res:scalability-client-all" class="subfigure">
<center><strong>Client load</strong></center>
<img src="querying-evolving/img/clientScalabilityBoxplot.png" />
<figcaption>
            <p><span class="label">Fig. 66:</span> Client CPU usage for our solution is significantly higher.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Detailed view on all server and client CPU measurements for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr>, <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> and the solution presented in this work for 200 simultaneous query evaluations against the server.</p>
        </figcaption>
</figure>

      <figure id="querying-evolving_fig:res:overview">

<figure id="querying-evolving_fig:res:ItCf" class="subfigure">
<img src="querying-evolving/img/interval-true_caching-false.png" />
<figcaption>
            <p><span class="label">Fig. 67:</span> Time intervals without caching.</p>
          </figcaption>
</figure>

<figure id="querying-evolving_fig:res:ItCt" class="subfigure">
<img src="querying-evolving/img/interval-true_caching-true.png" />
<figcaption>
            <p><span class="label">Fig. 69:</span> Time intervals with caching.</p>
          </figcaption>
</figure>

<figure id="querying-evolving_fig:res:IfCf" class="subfigure">
<img src="querying-evolving/img/interval-false_caching-false.png" />
<figcaption>
            <p><span class="label">Fig. 70:</span> Expiration times without caching.</p>
          </figcaption>
</figure>

<figure id="querying-evolving_fig:res:IfCt" class="subfigure">
<img src="querying-evolving/img/interval-false_caching-true.png" />
<figcaption>
            <p><span class="label">Fig. 71:</span> Expiration times with caching.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Executions times for the three different types of dynamic data representation for several subsequent streaming requests.
The figures show a mostly linear increase when using time intervals and constant execution times for annotation using expiration times.
In general, caching results in lower execution times.
They also reveal that the graph approach has the lowest execution times.</p>
        </figcaption>
</figure>

    </section>

    <section id="querying-evolving_conclusions">
      <h3>Conclusions</h3>

      <p>In this paper, we researched a solution for querying over dynamic data with a low server cost,
by continuously polling the data based on volatility information.
In this section, we draw conclusions from our evaluation results to give an answer
to the research questions and hypotheses we defined in <a href="#querying-evolving_problem-statement">Subsection 5.4</a>.
First, the server and client costs for our solution will be compared with the alternatives.
After that, the effect of our client-side cache will be explained.
Next, we will discuss the effect of time annotation on the amount of requests to be sent, after which the
performance of our solution will be shown and the effects of the annotation methods.</p>

      <h4 id="server-cost">Server cost</h4>
      <p>The results from <a href="#querying-evolving_subsec:Results-ServerCost">Subsubsection 5.8.1</a> confirm <a href="#querying-evolving_hypothesis1">Hypothesis 1</a>, in which we wanted to
know if we could lower the server cost when compared to <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.
Not only is the server cost for our solution more than ten times lower on average when compared to the alternatives,
this cost also increases much slower for a growing number of simultaneous clients.
This makes our proposed solution more scalable for the server.
Another disadvantage of <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> is the fact that the server load for a large number of
concurrent clients varies significantly, as can be seen in <a href="#querying-evolving_fig:res:scalability-server-200">Fig. 65</a>.
This makes it hard to scale the required processing powers for servers using these technologies.
Our solution has a low and more constant CPU usage.</p>

      <h4 id="client-cost">Client cost</h4>
      <p>The results for the client load measurements from <a href="#querying-evolving_subsec:Results-ClientCost">Subsubsection 5.8.2</a> confirm
<a href="#querying-evolving_hypothesis2">Hypothesis 2</a>, which stated that our solution increases the client’s processing need.
The required client processing power using our solution is clearly much higher than for <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> and <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>.
This is because we redistributed the required processing power from the server to the client.
In our solution, it is the client that has to do most of the work for evaluating queries, which puts
less load on the server.
The load on the client still remains around 5% for the largest part of the query evaluation
as shown in <a href="#querying-evolving_fig:res:scalability-client">Fig. 63</a>. Only during the first few seconds, the query engines
CPU usage peaks, which is because of the processor-intensive rewriting step that needs to be done once
at the start of each dynamic query evaluation.</p>

      <h4 id="caching">Caching</h4>
      <p>We can also confirm <a href="#querying-evolving_hypothesis3">Hypothesis 3</a> about the positive effect of caching
from the results in <a href="#querying-evolving_subsec:Results-AnnotationMethods">Subsubsection 5.8.3</a>.
Our caching solution has a positive effect on the execution times.
In an optimal scenario for our use case, caching would lead to an execution time reduction of 60% because three of the five triple
patterns in the query for our use case from <a href="#querying-evolving_use-case">Subsection 5.5</a> are dynamic.
For our results, this caching leads to an average reduction of 56% which is close to the optimal case.
Since we are working with dynamic data, some required background-data is bound to overlap, in these
cases it is advantageous to have a client-side caching solution so that these redundant requests for
static data can be avoided.
The longer our query evaluation runs, the more static data the cache accumulates, so the bigger the
chance that there are cache hits when background data is needed in a certain query iteration.
Future research should indicate what the limits of such a client-side cache for static data are, and
whether or not it is advantageous to reuse this cache for different queries.</p>

      <h4 id="request-reduction">Request reduction</h4>
      <p>By annotating dynamic data with a time annotation, we successfully reduced the amount of required requests
for polling-based <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> querying to a minimum, which answers <a href="#querying-evolving_researchquestion1">Research Question 1</a>
about the question if clients can use volatility knowledge to perform continuous querying.
Because now, the client can derive the exact moment at which the data can change on the server, and this will be used
to schedule a new query execution on the server.
In future research, it is still possible to reduce the amount of requests our client engine needs to send
through a better caching strategy, which could for example also temporarily cache dynamic data which changes
at different frequencies.
We can also look into differential data transmission by only sending data to the client that has been changed since
the last time the client has requested a specific resource.</p>

      <h4 id="performance">Performance</h4>
      <p>For answering <a href="#querying-evolving_researchquestion2">Research Question 2</a>, the performance of our solution compared to alternatives,
we compared our solution with two state-of-the-art approaches for dynamic <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> querying.
Our solution significantly reduces the required server processing per client, this complexity is mostly moved to the client.
This comparison shows that our technique allows data providers to offer dynamic data which can be used to continuously
evaluate dynamic queries with a low server cost.
Our low-cost publication technique for dynamic data is useful when the number of potential simultaneous clients
is large. When this data is needed for only a small number of clients in a closed off environment
and query evaluation must happen fast, traditional approaches like <abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr> or <abbr title='Continuous sparql'>C-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr></abbr> are advised.
These are only two possible points on the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><em>Linked Data Fragments</em> axis</a></span> <span class="references">[<a href="#ref-39">39</a>]</span>, depending on the
publication requirements, combinations of these approaches can be used.</p>

      <h4 id="annotation-methods">Annotation methods</h4>
      <p>In <a href="#querying-evolving_researchquestion3">Research Question 3</a>, we wanted to know how the different annotation methods influenced the execution times.
From the results in <a href="#querying-evolving_subsec:Results-AnnotationMethods">Subsubsection 5.8.3</a>, we can conclude that graph-based annotation results in the lowest execution times.
It can also be seen that annotation with time intervals has the problem of continuously increasing execution times, because
of the continuously growing dataset. Time interval annotation can be desired if we for example want to maintain
the history of certain facts, as opposed to just having the last version of facts using expiration times.
In future work, we will investigate alternative techniques to support time interval annotation without the continuously increasing execution times.</p>

      <p>In this work, the frequency at which our queries are updated is purely data-driven using time intervals or expiration times.
In the future it might be interesting, to provide a control to the user to change this frequency, if for example this user
only desires query updates at a lower frequency than the data actually changes.</p>

      <p>In future work, it is important to test this approach with a larger variety of use cases.
The time annotation mechanisms we use are generic enough to
transform all static facts to dynamic data for any number of triples.
The <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf">CityBench</a> <span class="references">[<a href="#ref-33">33</a>]</span> benchmark can for example be
used to evaluate these different cases based on city sensor data.
These tests must be scaled (both in terms of clients as in terms of dataset size),
so that the maximum number of concurrent requests can be determined, with respect to the dataset size.</p>

    </section>

  </section>

  <section id="conclusions">
    <h2>Conclusions</h2>

    <p class="todo">Write me</p>

  </section>

</main>

<footer><section id="references">
<h2>References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="#semanticweb" typeof="schema:Article">Berners-Lee, T., Hendler, J., Lassila, O., others: The semantic web. Scientific American. 284, 28–37 (2001).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: <abbr title='Resource Description Framework'>RDF</abbr> 1.1: Concepts and Abstract Syntax. W3C, <a href="https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">https:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/" typeof="schema:CreativeWork">Harris, S., Seaborne, A., Prud’hommeaux, E.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 Query Language. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-query-20130321/</a> (2013).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: \rdf 1.1: Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">http:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="http://www.w3.org/DesignIssues/LinkedData.html" typeof="schema:CreativeWork">Berners-Lee, T.: Linked Data. <a href="http://www.w3.org/DesignIssues/LinkedData.html">http:/​/​www.w3.org/DesignIssues/LinkedData.html</a> (2006).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>-Benchmark-IJSWIS.pdf&#8221; typeof=&#8221;schema:Article&#8221;>Bizer, C., Schultz, A.: The Berlin \sparql benchmark. International Journal on Semantic Web and Information Systems. 5, 1–24 (2009).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="#lubmbenchmark" typeof="schema:Article">Guo, Y., Pan, Z., Heflin, J.: \textscLUBM: A benchmark for \textscOWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 3, 158–182 (2005).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="#sp2benchmark" typeof="schema:Article">Schmidt, M., Hornung, T., Lausen, G., Pinkel, C.: \textscSP^2Bench: a \sparql performance benchmark. In: 2009 IEEE 25th International Conference on Data Engineering. pp. 222–233. IEEE (2009).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="http://ceur-ws.org/Vol-1700/paper-02.pdf" typeof="schema:Article">Spasić, M., Jovanovik, M., Prat-Pérez, A.: An \rdf Dataset Generator for the Social Network Benchmark with Real-World Coherence. In: Fundulaki, I., Krithara, A., Ngonga Ngomo, A.-C., and Rentoumi, V. (eds.) Proceedings of the Workshop on Benchmarking Linked Data (2016).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="#rdfbenchmarksdatasets" typeof="schema:Article">Duan, S., Kementsietsidis, A., Srinivas, K., Udrea, O.: Apples and oranges: a comparison of \rdf benchmarks and real \rdf datasets. In: Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. pp. 145–156. ACM, New York, NY, USA (2011).</dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="#linkedconnections" typeof="schema:Article">Colpaert, P., Llaves, A., Verborgh, R., Corcho, O., Mannens, E., Van de Walle, R.: Intermodal public transit routing using Linked Connections. In: Proceedings of the 14th International Semantic Web Conference: Posters and Demos. pp. 1–5 (2015).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="#csa" typeof="schema:Chapter">Dibbelt, J., Pajor, T., Strasser, B., Wagner, D.: Intriguingly Simple and Fast Transit Routing. In: Bonifaci, V., Demetrescu, C., and Marchetti-Spaccamela, A. (eds.) Experimental Algorithms. pp. 43–54. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf" typeof="schema:Article">Kyzirakos, K., Karpathiotakis, M., Koubarakis, M.: Strabon: a semantic geospatial \dbms. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 295–311. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="http://www.semantic-web-journal.net/sites/default/files/swj176_3.pdf" typeof="schema:Article">Battle, R., Kolas, D.: Enabling the geospatial semantic web with parliament and \scshapegeosparql. Semantic Web. 3, 355–370 (2012).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="http://doi.acm.org/10.1145/1526709.1526856" typeof="schema:Article">Barbieri, D.F., Braga, D., Ceri, S., Della Valle, E., Grossniklaus, M.: C-\sparql: \sparql for continuous querying. In: Proceedings of the 18th international conference on World wide web. pp. 1061–1062. ACM (2009).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Parreira, J.X., Hauswirth, M.: A native and adaptive approach for unified processing of Linked Streams and Linked Data. In: Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2011. pp. 370–388. Springer Berlin Heidelberg, Berlin, Heidelberg (2011).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="http://dx.doi.org/10.1007/978-3-642-41338-4_22" typeof="schema:Article">Garbis, G., Kyzirakos, K., Koubarakis, M.: Geographica: A Benchmark for Geospatial \rdf Stores. In: Alani, H., Kagal, L., Fokoue, A., Groth, P., Biemann, C., Parreira, J.X., Aroyo, L., Noy, N., Welty, C., and Janowicz, K. (eds.) Proceedings of the 12th International Semantic Web Conference. pp. 343–359. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="#lsbench" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Pham, M.-D., Boncz, P., Eiter, T., Fink, M.: Linked stream data processing engines: Facts and figures. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 300–312. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="https://doi.org/10.1016/j.tra.2008.03.011" typeof="schema:Article">Guihaire, V., Hao, J.-K.: Transit network design and scheduling: A global review. Transportation Research Part A: Policy and Practice. 42, 1251–1273 (2008).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="#syntheticspatiotemporal" typeof="schema:Article">Nascimento, M.A., Pfoser, D., Theodoridis, Y.: Synthetic and real spatiotemporal datasets. IEEE Data Eng. Bull. 26, 26–32 (2003).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="https://doi.org/10.1023/A:1015231126594" typeof="schema:Article">Brinkhoff, T.: A framework for generating network-based moving objects. GeoInformatica. 6, 153–180 (2002).</dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="https://doi.org/10.1109/ITNG.2006.51" typeof="schema:Article">Lin, P.J., Samadi, B., Cipolone, A., Jeske, D.R., Cox, S., Rendon, C., Holt, D., Xiao, R.: Development of a synthetic data set generator for building and testing information discovery systems. In: Third International Conference on Information Technology: New Generations (ITNG’06). pp. 707–712. IEEE (2006).</dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="#ldbc" typeof="schema:Article">Angles, R., Boncz, P., Larriba-Pey, J., Fundulaki, I., Neumann, T., Erling, O., Neubauer, P., Martinez-Bazan, N., Kotsev, V., Toma, I.: The Linked Data Benchmark Council: a graph and \rdf industry benchmarking effort. ACM SIGMOD Record. 43, 27–31 (2014).</dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="https://doi.org/10.1007/978-3-642-02094-0_7" typeof="schema:Chapter">Delling, D., Sanders, P., Schultes, D., Wagner, D.: Engineering route planning algorithms. In: Lerner, J., Wagner, D., and Zweig, K.A. (eds.) Algorithmics of Large and Complex Networks: Design, Analysis, and Simulation. pp. 117–139. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="http://doi.acm.org/10.1145/1227161.1227166" typeof="schema:Article">Pyrga, E., Schulz, F., Wagner, D., Zaroliagis, C.: Efficient models for timetable information in public transportation systems. Journal of Experimental Algorithmics (JEA). 12, 2.4:1–2.4:39 (2008).</dd>
  <dt id="ref-26">[26]</dt>
  <dd resource="https://dx.doi.org/10.1137/1.9781611974317.2" typeof="schema:Article">Bast, H., Hertel, M., Storandt, S.: Scalable Transfer Patterns. 2016 Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments (ALENEX). 15–29</dd>
  <dt id="ref-27">[27]</dt>
  <dd resource="http://www2016.net/proceedings/companion/p873.pdf" typeof="schema:Article">Colpaert, P., Chua, A., Verborgh, R., Mannens, E., Van de Walle, R., Vande Moere, A.: What public transit \scshapeAPI logs tell us about travel flows. In: Proceedings of the 6\textsuperscriptth \scshapeUSEWOD Workshop on Usage Analysis and the Web of Data. pp. 873–878. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland (2016).</dd>
  <dt id="ref-28">[28]</dt>
  <dd resource="#benchmarkhandbook" typeof="schema:Book">Gray, J.: Benchmark handbook: for database and transaction processing systems. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (1992).</dd>
  <dt id="ref-29">[29]</dt>
  <dd resource="http://doi.acm.org/10.1145/3132218.3132242" typeof="schema:Article">Petzka, H., Stadler, C., Katsimpras, G., Haarmann, B., Lehmann, J.: Benchmarking Faceted Browsing Capabilities of Triplestores. Proceedings of the 13th International Conference on Semantic Systems. 128–135 (2017).</dd>
  <dt id="ref-30">[30]</dt>
  <dd resource="https://doi.org/10.1109/MIC.2008.55" typeof="schema:Article">Eno, J., Thompson, C.W.: Generating synthetic data to match data mining patterns. IEEE Internet Computing. 12, (2008).</dd>
  <dt id="ref-31">[31]</dt>
  <dd resource="http://arxiv.org/abs/1609.08764" typeof="schema:Article">Wong, S.C., Gatt, A., Stamatescu, V., McDonnell, M.D.: Understanding data augmentation for classification: when to warp? In: Digital Image Computing: Techniques and Applications (DICTA), 2016 International Conference on. pp. 1–6. IEEE (2016).</dd>
  <dt id="ref-32">[32]</dt>
  <dd resource="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf" typeof="schema:Article">Della Valle, E., Ceri, S., van Harmelen, F., Fensel, D.: It’s a Streaming World! Reasoning upon Rapidly Changing Information. Intelligent Systems, IEEE. 24, 83–89 (2009).</dd>
  <dt id="ref-33">[33]</dt>
  <dd resource="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf" typeof="schema:Article">Ali, M.I., Gao, F., Mileo, A.: CityBench: a configurable benchmark to evaluate \rsp engines using smart city datasets. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) International Semantic Web Conference. pp. 374–389. Springer International Publishing, Cham (2015).</dd>
  <dt id="ref-34">[34]</dt>
  <dd resource="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf" typeof="schema:Article">Georgala, K., Spasić, M., Jovanovik, M., Petzka, H., Röder, M., Ngomo, A.-C.N.: \scshapeMOCHA2017: The Mighty Storage Challenge at \scshapeESWC 2017. In: Dragoni, M., Solanki, M., and Blomqvist, E. (eds.) Semantic Web Challenges. pp. 3–15. Springer International Publishing, Cham (2017).</dd>
  <dt id="ref-35">[35]</dt>
  <dd resource="http://ceur-ws.org/Vol-1377/paper6.pdf" typeof="schema:Article">Fernández, J.D., Polleres, A., Umbrich, J.: Towards efficient archiving of Dynamic Linked Open Data. In: Debattista, J., d’Aquin, M., and Lange, C. (eds.) Proceedings of te First DIACHRON Workshop on Managing the Evolution and Preservation of the Data Web. pp. 34–49 (2015).</dd>
  <dt id="ref-36">[36]</dt>
  <dd resource="#datasetdynamics" typeof="schema:Article">Umbrich, J., Decker, S., Hausenblas, M., Polleres, A., Hogan, A.: Towards dataset dynamics: Change frequency of Linked Open Data sources. 3rd International Workshop on Linked Data on the Web (LDOW). (2010).</dd>
  <dt id="ref-37">[37]</dt>
  <dd resource="#diachronql" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Viglas, S., Stavrakas, Y., Pateritsas, C., Anagnostopoulos, I.: A Query Language for Multi-version Data Web Archives. Expert Systems. 33, 383–404 (2016).</dd>
  <dt id="ref-38">[38]</dt>
  <dd resource="#dbpedia" typeof="schema:Chapter">Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: DBpedia: A nucleus for a Web of open data. In: The semantic web. pp. 722–735. Springer (2007).</dd>
  <dt id="ref-39">[39]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, (2016).</dd>
  <dt id="ref-40">[40]</dt>
  <dd resource="#rdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: <abbr title='Resource Description Framework'>RDF</abbr>-3X: a RISC-style engine for <abbr title='Resource Description Framework'>RDF</abbr>. Proceedings of the VLDB Endowment. 1, 647–659 (2008).</dd>
  <dt id="ref-41">[41]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2017/vtpf.pdf" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R., Mannens, E.: Versioned Triple Pattern Fragments: A Low-cost Linked Data Interface Feature for Web Archives. In: Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web (2017).</dd>
  <dt id="ref-42">[42]</dt>
  <dd resource="http://linkeddatafragments.org/publications/jod2017.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Hochstenbach, P., Van de Sompel, H.: Towards sustainable publishing and querying of distributed Linked Data archives. Journal of Documentation. 73, (2017).</dd>
  <dt id="ref-43">[43]</dt>
  <dd resource="#memento" typeof="schema:Article">Van de Sompel, H., Nelson, M.L., Sanderson, R., Balakireva, L.L., Ainsworth, S., Shankar, H.: Memento: Time travel for the Web. arXiv preprint arXiv:0911.1112. (2009).</dd>
  <dt id="ref-44">[44]</dt>
  <dd resource="#virtuoso" typeof="schema:Chapter">Erling, O., Mikhailov, I.: Virtuoso: <abbr title='Resource Description Framework'>RDF</abbr> support in a native RDBMS. In: Semantic Web Information Management. pp. 501–519. Springer (2010).</dd>
  <dt id="ref-45">[45]</dt>
  <dd resource="#dsparq" typeof="schema:Article">Wallgrün, J.O., Frommberger, L., Wolter, D., Dylla, F., Freksa, C.: Qualitative spatial representation and reasoning in the SparQ-toolbox. In: International Conference on Spatial Cognition. pp. 39–58. Springer (2006).</dd>
  <dt id="ref-46">[46]</dt>
  <dd resource="#derivingemergentschemas" typeof="schema:Article">Pham, M.-D., Passing, L., Erling, O., Boncz, P.: Deriving an emergent relational schema from rdf data. In: Proceedings of the 24th International Conference on World Wide Web. pp. 864–874. International World Wide Web Conferences Steering Committee (2015).</dd>
  <dt id="ref-47">[47]</dt>
  <dd resource="#emergentschemas" typeof="schema:Article">Pham, M.-D., Boncz, P.: Exploiting emergent schemas to make <abbr title='Resource Description Framework'>RDF</abbr> systems more efficient. In: International Semantic Web Conference. pp. 463–479. Springer (2016).</dd>
  <dt id="ref-48">[48]</dt>
  <dd resource="#axondb" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Mamoulis, N., Anagnostopoulos, I.: Extended characteristic sets: graph indexing for <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> query optimization. In: Data Engineering (ICDE), 2017 IEEE 33rd International Conference on. pp. 497–508. IEEE (2017).</dd>
  <dt id="ref-49">[49]</dt>
  <dd resource="#odyssey" typeof="schema:Article">Montoya, G., Skaf-Molli, H., Hose, K.: The Odyssey approach for optimizing federated <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> queries. In: International Semantic Web Conference. pp. 471–489. Springer (2017).</dd>
  <dt id="ref-50">[50]</dt>
  <dd resource="#hexastore" typeof="schema:Article">Weiss, C., Karras, P., Bernstein, A.: Hexastore: sextuple indexing for semantic web data management. Proceedings of the VLDB Endowment. 1, 1008–1019 (2008).</dd>
  <dt id="ref-51">[51]</dt>
  <dd resource="#triplebit" typeof="schema:Article">Yuan, P., Liu, P., Wu, B., Jin, H., Zhang, W., Liu, L.: TripleBit: a fast and compact system for large scale <abbr title='Resource Description Framework'>RDF</abbr> data. Proceedings of the VLDB Endowment. 6, 517–528 (2013).</dd>
  <dt id="ref-52">[52]</dt>
  <dd resource="https://arxiv.org/pdf/1105.4004.pdf" typeof="schema:Article">Álvarez-Garcı́a Sandra, Brisaboa, N.R., Fernández, J.D., Martı́nez-Prieto Miguel A: Compressed k2-triples for full-in-memory <abbr title='Resource Description Framework'>RDF</abbr> engines. arXiv preprint arXiv:1105.4004. (2011).</dd>
  <dt id="ref-53">[53]</dt>
  <dd resource="#rdfcsa" typeof="schema:Article">Brisaboa, N.R., Cerdeira-Pena, A., Fariña, A., Navarro, G.: A compact <abbr title='Resource Description Framework'>RDF</abbr> store using suffix arrays. In: International Symposium on String Processing and Information Retrieval. pp. 103–115. Springer (2015).</dd>
  <dt id="ref-54">[54]</dt>
  <dd resource="http://www.websemanticsjournal.org/index.php/ps/article/view/328" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary <abbr title='Resource Description Framework'>RDF</abbr> Representation for Publication and Exchange (<abbr title='Header Dictionary Triples'>HDT</abbr>). Web Semantics: Science, Services and Agents on the World Wide Web. 19, 22–41 (2013).</dd>
  <dt id="ref-55">[55]</dt>
  <dd resource="#hdtfoq" typeof="schema:Article">Martı́nez-Prieto Miguel A, Gallego, M.A., Fernández, J.D.: Exchange and consumption of huge <abbr title='Resource Description Framework'>RDF</abbr> data. In: Extended Semantic Web Conference. pp. 437–452. Springer (2012).</dd>
  <dt id="ref-56">[56]</dt>
  <dd resource="#lodlaundromat" typeof="schema:Article">Beek, W., Rietveld, L., Bazoobandi, H.R., Wielemaker, J., Schlobach, S.: LOD laundromat: a uniform way of publishing other people’s dirty data. In: International Semantic Web Conference. pp. 213–228. Springer (2014).</dd>
  <dt id="ref-57">[57]</dt>
  <dd resource="https://github.com/datablend/fluxgraph" typeof="schema:CreativeWork">Suvee, D.: fluxgraph. <a href="https://github.com/datablend/fluxgraph">https:/​/​github.com/datablend/fluxgraph</a> (2012).</dd>
  <dt id="ref-58">[58]</dt>
  <dd resource="https://github.com/dmontag/neo4j-versioning" typeof="schema:CreativeWork">Montag, D.: Neo4j Versioning. <a href="https://github.com/dmontag/neo4j-versioning">https:/​/​github.com/dmontag/neo4j-versioning</a> (2011).</dd>
  <dt id="ref-59">[59]</dt>
  <dd resource="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j" typeof="schema:CreativeWork">Cattuto, C.: Representing time dependent graphs in Neo4j. <a href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">https:/​/​github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j</a> (2013).</dd>
  <dt id="ref-60">[60]</dt>
  <dd resource="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/" typeof="schema:CreativeWork">Staff, N.: Modeling a multilevel index in Neoj4. <a href="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/">https:/​/​neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/</a> (2012).</dd>
  <dt id="ref-61">[61]</dt>
  <dd resource="http://semantic-web-journal.org/system/files/swj1814.pdf" typeof="schema:Article">Fernández, J.D., Umbrich, J., Polleres, A., Knuth, M.: Evaluating Query and Storage Strategies for <abbr title='Resource Description Framework'>RDF</abbr> Archives. Semantic Web Journal. (2018).</dd>
  <dt id="ref-62">[62]</dt>
  <dd resource="#semversion" typeof="schema:Article">Volkel, M., Winkler, W., Sure, Y., Kruk, S.R., Synak, M.: Semversion: A versioning system for <abbr title='Resource Description Framework'>RDF</abbr> and ontologies. In: Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29–June 1, 2005. Proceedings (2005).</dd>
  <dt id="ref-63">[63]</dt>
  <dd resource="#vcrdf" typeof="schema:Article">Cassidy, S., Ballantine, J.: Version Control for <abbr title='Resource Description Framework'>RDF</abbr> Triple Stores. ICSOFT (ISDM/EHST/DC). 7, 5–12 (2007).</dd>
  <dt id="ref-64">[64]</dt>
  <dd resource="#rwbase" typeof="schema:Article">Vander Sande, M., Colpaert, P., Verborgh, R., Coppens, S., Mannens, E., Van de Walle, R.: R&amp;Wbase: git for triples. In: Proceedings of the 6th Workshop on Linked Data on the Web (2013).</dd>
  <dt id="ref-65">[65]</dt>
  <dd resource="#r43ples" typeof="schema:Article">Graube, M., Hensel, S., Urbas, L.: R43ples: Revisions for triples. In: Proceedings of the 1st Workshop on Linked Data Quality co-located with 10th International Conference on Semantic Systems (SEMANTiCS 2014) (2014).</dd>
  <dt id="ref-66">[66]</dt>
  <dd resource="#vcld" typeof="schema:Article">Hauptmann, C., Brocco, M., Wörndl, W.: Scalable Semantic Version Control for Linked Data Management. In: Proceedings of the 2nd Workshop on Linked Data Quality co-located with 12th Extended Semantic Web Conference (ESWC 2015), Portorož, Slovenia (2015).</dd>
  <dt id="ref-67">[67]</dt>
  <dd resource="#xrdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: x-<abbr title='Resource Description Framework'>RDF</abbr>-3X: fast querying, high update rates, and consistency for <abbr title='Resource Description Framework'>RDF</abbr> databases. Proceedings of the VLDB Endowment. 3, 256–263 (2010).</dd>
  <dt id="ref-68">[68]</dt>
  <dd resource="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf" typeof="schema:Article">Gao, S., Gu, J., Zaniolo, C.: <abbr title='Resource Description Framework'>RDF</abbr>-TX: A fast, user-friendly system for querying the history of <abbr title='Resource Description Framework'>RDF</abbr> knowledge bases. In: Proceedings of the 19th International Conference on Extending DatabaseTechnology. pp. 269–280 (2016).</dd>
  <dt id="ref-69">[69]</dt>
  <dd resource="#selfindexingarchives" typeof="schema:Article">Cerdeira-Pena, A., Farina, A., Fernández, J.D., Martı́nez-Prieto Miguel A: Self-indexing <abbr title='Resource Description Framework'>RDF</abbr> archives. In: Data Compression Conference (DCC), 2016. pp. 526–535. IEEE (2016).</dd>
  <dt id="ref-70">[70]</dt>
  <dd resource="#dydra" typeof="schema:Article">Anderson, J., Bendiken, A.: Transaction-time queries in dydra. In: Joint Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (MEPDaW 2016) and the 3rd Workshop on Linked Data Quality (LDQ 2016) co-located with 13th European Semantic Web Conference (ESWC 2016): MEPDaW-LDQ. pp. 11–19 (2016).</dd>
  <dt id="ref-71">[71]</dt>
  <dd resource="#tailr" typeof="schema:Article">Meinhardt, P., Knuth, M., Sack, H.: TailR: a platform for preserving history on the web of data. In: Proceedings of the 11th International Conference on Semantic Systems. pp. 57–64. ACM (2015).</dd>
  <dt id="ref-72">[72]</dt>
  <dd resource="http://darcs.net" typeof="schema:CreativeWork">Roundy, D.: Darcs. <a href="http://darcs.net">http:/​/​darcs.net</a> (2008).</dd>
  <dt id="ref-73">[73]</dt>
  <dd resource="https://www.w3.org/TR/trig/" typeof="schema:CreativeWork">Bizer, C., Cyganiak, R.: <abbr title='Resource Description Framework'>RDF</abbr> 1.1 TriG. World Wide Web Consortium, <a href="https://www.w3.org/TR/trig/">https:/​/​www.w3.org/TR/trig/</a> (2014).</dd>
  <dt id="ref-74">[74]</dt>
  <dd resource="#vmrdf" typeof="schema:Article">Im, D.-H., Lee, S.-W., Kim, H.-J.: A version management framework for <abbr title='Resource Description Framework'>RDF</abbr> triple stores. International Journal of Software Engineering and Knowledge Engineering. 22, 85–106 (2012).</dd>
  <dt id="ref-75">[75]</dt>
  <dd resource="#sesame" typeof="schema:Article">Broekstra, J., Kampman, A., Van Harmelen, F.: Sesame: A generic architecture for storing and querying <abbr title='Resource Description Framework'>RDF</abbr> and <abbr title='Resource Description Framework'>RDF</abbr> schema. In: International semantic web conference. pp. 54–68. Springer (2002).</dd>
  <dt id="ref-76">[76]</dt>
  <dd resource="#blazegraph" typeof="schema:Article">Thompson, B.B., Personick, M., Cutcher, M.: The Bigdata® <abbr title='Resource Description Framework'>RDF</abbr> Graph Database. Linked Data Management. 193–237 (2014).</dd>
  <dt id="ref-77">[77]</dt>
  <dd resource="#dbpedialive" typeof="schema:Article">Morsey, M., Lehmann, J., Auer, S., Stadler, C., Hellmann, S.: DBpedia and the live extraction of structured data from wikipedia. Program. 46, 157–181 (2012).</dd>
  <dt id="ref-78">[78]</dt>
  <dd resource="#opendataportalwatch" typeof="schema:Article">Neumaier, S., Umbrich, J., Polleres, A.: Automated quality assessment of metadata across open data portals. Journal of Data and Information Quality (JDIQ). 8, 2 (2016).</dd>
  <dt id="ref-79">[79]</dt>
  <dd resource="https://jena.apache.org/" typeof="schema:CreativeWork">Apache Jena. <a href="https://jena.apache.org/">https:/​/​jena.apache.org/</a></dd>
  <dt id="ref-80">[80]</dt>
  <dd resource="#evogen" typeof="schema:Article">Meimaris, M., Papastefanatos, G.: The EvoGen Benchmark Suite for Evolving <abbr title='Resource Description Framework'>RDF</abbr> Data. In: Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web. pp. 20–35 (2016).</dd>
  <dt id="ref-81">[81]</dt>
  <dd resource="#lubm" typeof="schema:Article">Guo, Y., Pan, Z., Heflin, J.: <abbr title='Lehigh University Benchmark'>LUBM</abbr>: A benchmark for OWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 3, 158–182 (2005).</dd>
  <dt id="ref-82">[82]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Mannens, E.: Exposing <abbr title='Resource Description Framework'>RDF</abbr> Archives using Triple Pattern Fragments. In: Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management: Posters and Demos (2016).</dd>
  <dt id="ref-83">[83]</dt>
  <dd resource="http://users.ics.forth.gr/~fgeo/files/ER14.pdf" typeof="schema:Article">Stefanidis, K., Chrysakis, I., Flouris, G.: On Designing Archiving Policies for Evolving <abbr title='Resource Description Framework'>RDF</abbr> Datasets on the Web. In: International Conference on Conceptual Modeling. pp. 43–56. Springer (2014).</dd>
  <dt id="ref-84">[84]</dt>
  <dd resource="https://dx.doi.org/10.1145/1804669.1804675" typeof="schema:Article">Schmidt, M., Meier, M., Lausen, G.: Foundations of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Query Optimization. In: Proceedings of the 13th International Conference on Database Theory. pp. 4–33 (2010).</dd>
  <dt id="ref-85">[85]</dt>
  <dd resource="https://dx.doi.org/10.1145/1367497.1367578" typeof="schema:Article">Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., Reynolds, D.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Basic Graph Pattern Optimization Using Selectivity Estimation. In: Proceedings of the 17th International Conference on World Wide Web. pp. 595–604 (2008).</dd>
  <dt id="ref-86">[86]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2" typeof="schema:CreativeWork">Erling, O., Mikhailov, I.: <abbr title='Resource Description Framework'>RDF</abbr> Support in the Virtuoso DBMS. In: Pellegrini, T., Auer, S., Tochtermann, K., and Schaffert, S. (eds.) Networked Knowledge - Networked Media: Integrating Knowledge Management, New Media Technologies and Semantic Systems. pp. 7–24. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-87">[87]</dt>
  <dd resource="#fSPARQL" typeof="schema:Article">Cheng, J., Ma, Z.M., Yan, L.: f-<abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>: A Flexible Extension of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>. In: Bringas, P.G., Hameurlain, A., and Quirchmayr, G. (eds.) Database and Expert Systems Applications. pp. 487–494. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).</dd>
  <dt id="ref-88">[88]</dt>
  <dd resource="https://www.w3.org/DesignIssues/LinkedData.html" typeof="schema:CreativeWork">Berners-Lee, T.: Linked Data. <a href="https://www.w3.org/DesignIssues/LinkedData.html">https:/​/​www.w3.org/DesignIssues/LinkedData.html</a> (2009).</dd>
  <dt id="ref-89">[89]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/" typeof="schema:CreativeWork">Feigenbaum, L., Todd Williams, G., Grant Clark, K., Torres, E.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 Protocol. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-protocol-20130321/</a> (2013).</dd>
  <dt id="ref-90">[90]</dt>
  <dd resource="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf" typeof="schema:Article">Hartig, O.: An overview on execution strategies for Linked Data queries. Datenbank-Spektrum. 13, 89–99 (2013).</dd>
  <dt id="ref-91">[91]</dt>
  <dd resource="http://linkeddatafragments.org/publications/eswc2015.pdf" typeof="schema:Article">Van Herwegen, J., Verborgh, R., Mannens, E., Van de Walle, R.: Query Execution Optimization for Clients of Triple Pattern Fragments. In: The Semantic Web. Latest Advances and New Domains (2015).</dd>
  <dt id="ref-92">[92]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-amf.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Van Herwegen, J., Mannens, E., Van de Walle, R.: Opportunistic Linked Data Querying through Approximate Membership Metadata. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 92–110. Springer (2015).</dd>
  <dt id="ref-93">[93]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-substring.pdf" typeof="schema:Article">Van Herwegen, J., De Vocht, L., Verborgh, R., Mannens, E., Van de Walle, R.: Substring Filtering for Low-Cost Linked Data Interfaces. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 128–143. Springer (2015).</dd>
  <dt id="ref-94">[94]</dt>
  <dd resource="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf" typeof="schema:Article">Acosta, M., Vidal, M.-E.: Networks of Linked Data Eddies: An Adaptive Web Query Processing Engine for <abbr title='Resource Description Framework'>RDF</abbr> Data. In: The Semantic Web – ISWC 2015. pp. 111–127 (2015).</dd>
  <dt id="ref-95">[95]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48" typeof="schema:Article">Hartig, O., Buil-Aranda, C.: Bindings-Restricted Triple Pattern Fragments. In: Proceedings of the 15th International Conference on Ontologies, DataBases, and Applications of Semantics. pp. 762–779 (2016).</dd>
  <dt id="ref-96">[96]</dt>
  <dd resource="#cyclades" typeof="schema:Article">Folz, P., Skaf-Molli, H., Molli, P.: CyCLaDEs: a decentralized cache for triple pattern fragments. In: International Semantic Web Conference. pp. 455–469. Springer (2016).</dd>
  <dt id="ref-97">[97]</dt>
  <dd resource="#tpfqs" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E.: Continuous client-side query evaluation over dynamic Linked Data. In: International Semantic Web Conference. pp. 273–289. Springer (2016).</dd>
  <dt id="ref-98">[98]</dt>
  <dd resource="#allegrograph" typeof="schema:Article">Aasman, J.: AllegroGraph: <abbr title='Resource Description Framework'>RDF</abbr> triple database. Cidade: Oakland Franz Incorporated. 17, (2006).</dd>
  <dt id="ref-99">[99]</dt>
  <dd resource="https://rdflib.readthedocs.io/en/stable/" typeof="schema:CreativeWork">RDFLib. <a href="https://rdflib.readthedocs.io/en/stable/">https:/​/​rdflib.readthedocs.io/en/stable/</a></dd>
  <dt id="ref-100">[100]</dt>
  <dd resource="https://github.com/linkeddata/rdflib.js" typeof="schema:CreativeWork">rdflib.js. <a href="https://github.com/linkeddata/rdflib.js">https:/​/​github.com/linkeddata/rdflib.js</a></dd>
  <dt id="ref-101">[101]</dt>
  <dd resource="https://github.com/antoniogarrote/rdfstore-js" typeof="schema:CreativeWork">rdfstore-js. <a href="https://github.com/antoniogarrote/rdfstore-js">https:/​/​github.com/antoniogarrote/rdfstore-js</a></dd>
  <dt id="ref-102">[102]</dt>
  <dd resource="http://arxiv.org/abs/1609.07108" typeof="schema:Article">Verborgh, R., Dumontier, M.: A Web <abbr title='Application Programming Interface'>API</abbr> ecosystem through feature-based reuse. CoRR. abs/1609.07108, (2016).</dd>
  <dt id="ref-103">[103]</dt>
  <dd resource="#hydra" typeof="schema:Article">Lanthaler, M., Gütl, C.: Hydra: A Vocabulary for Hypermedia-Driven Web APIs. LDOW. 996, (2013).</dd>
  <dt id="ref-104">[104]</dt>
  <dd resource="https://linkeddatafragments.github.io/Article-Declarative-Hypermedia-Responses/" typeof="schema:Article">Taelman, R., Verborgh, R.: Declaratively Describing Responses of Hypermedia-Driven Web APIs. In: Proceedings of the 9th International Conference on Knowledge Capture (2017).</dd>
  <dt id="ref-105">[105]</dt>
  <dd resource="#publishsubscribepattern" typeof="schema:Book">Birman, K., Joseph, T.: Exploiting virtual synchrony in distributed systems. ACM (1987).</dd>
  <dt id="ref-106">[106]</dt>
  <dd resource="#actormodel" typeof="schema:Article">Hewitt, C., Bishop, P., Steiger, R.: Session 8 formalisms for artificial intelligence a universal modular actor formalism for artificial intelligence. In: Advance Papers of the Conference. p. 235. Stanford Research Institute (1973).</dd>
  <dt id="ref-107">[107]</dt>
  <dd resource="#mediatorpattern" typeof="schema:Book">Gamma, E.: Design patterns: elements of reusable object-oriented software. Pearson Education India (1995).</dd>
  <dt id="ref-108">[108]</dt>
  <dd resource="https://martinfowler.com/articles/injection.html" typeof="schema:CreativeWork">Fowler, M.: Inversion of Control Containers and the Dependency Injection pattern. <a href="https://martinfowler.com/articles/injection.html">https:/​/​martinfowler.com/articles/injection.html</a> (2004).</dd>
  <dt id="ref-109">[109]</dt>
  <dd resource="http://componentsjs.readthedocs.io/en/latest/" typeof="schema:CreativeWork">Taelman, R.: Components.js. <a href="http://componentsjs.readthedocs.io/en/latest/">http:/​/​componentsjs.readthedocs.io/en/latest/</a></dd>
  <dt id="ref-110">[110]</dt>
  <dd resource="https://linkedsoftwaredependencies.org/articles/describing-experiments/" typeof="schema:Article">Van Herwegen, J., Taelman, R., Capadisli, S., Verborgh, R.: Describing configurations of software experiments as Linked Data. In: Proceedings of the 1st Workshop on Enabling Open Semantic Science (2017).</dd>
  <dt id="ref-111">[111]</dt>
  <dd resource="#jsonld" typeof="schema:Article">Consortium, W.W.W., others: JSON-LD 1.0: a JSON-based serialization for linked data. (2014).</dd>
  <dt id="ref-112">[112]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/" typeof="schema:CreativeWork">Grant Clark, K., Feigenbaum, L., Torres, E.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> 1.1 Query Results JSON Format. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-results-json-20130321/</a> (2013).</dd>
  <dt id="ref-113">[113]</dt>
  <dd resource="https://www.w3.org/TR/rdf-sparql-XMLres/" typeof="schema:CreativeWork">Hawke, S.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Query Results XML Format (Second Edition). W3C, <a href="https://www.w3.org/TR/rdf-sparql-XMLres/">https:/​/​www.w3.org/TR/rdf-sparql-XMLres/</a> (2013).</dd>
  <dt id="ref-114">[114]</dt>
  <dd resource="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" typeof="schema:CreativeWork">Prud’hommeaux, E., Seaborne, A.: <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr> Query Language for <abbr title='Resource Description Framework'>RDF</abbr>. W3C, <a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/">https:/​/​www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/</a> (2008).</dd>
  <dt id="ref-115">[115]</dt>
  <dd resource="https://rdfostrich.github.io/article-demo/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: <abbr title='Offset-Enabled Triple Store for Changesets'>OSTRICH</abbr>: Versioned Random-Access Triple Store. In: Proceedings of the 27th International Conference Companion on World Wide Web (2018).</dd>
  <dt id="ref-116">[116]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13" typeof="schema:Article">Aluç, G., Hartig, O., Özsu, M.T., Daudjee, K.: Diversified Stress Testing of <abbr title='Resource Description Framework'>RDF</abbr> Data Management Systems. In: Proceedings of the 13th International Semantic Web Conference - Part I. pp. 197–212. Springer-Verlag New York, Inc. (2014).</dd>
  <dt id="ref-117">[117]</dt>
  <dd resource="http://facebook.github.io/graphql/October2016/" typeof="schema:CreativeWork">Facebook, I.: GraphQL. Working Draft, Oct. 2016. <a href="http://facebook.github.io/graphql/October2016/">http:/​/​facebook.github.io/graphql/October2016/</a></dd>
  <dt id="ref-118">[118]</dt>
  <dd resource="#buil2013sparql" typeof="schema:Chapter">Buil-Aranda, C., Hogan, A., Umbrich, J., Vandenbussche, P.-Y.: \sparql Web-Querying Infrastructure: Ready for Action? In: The Semantic Web–ISWC 2013. pp. 277–293 (2013).</dd>
  <dt id="ref-119">[119]</dt>
  <dd resource="#sparqlcomplexity" typeof="schema:Article">Pérez, J., Arenas, M., Gutierrez, C.: Semantics and Complexity of <abbr title='SPARQL Procotol and RDF Query Language'>SPARQL</abbr>. In: International semantic web conference. pp. 30–43. Springer (2006).</dd>
  <dt id="ref-120">[120]</dt>
  <dd resource="#taelman_mepdaw_2016" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E., Van de Walle, R.: Continuously Updating Query Results over Real-Time Linked Data. In: Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (2016).</dd>
  <dt id="ref-121">[121]</dt>
  <dd resource="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/" typeof="schema:CreativeWork">Klyne, G., J. Carroll, J.: Resource Description Framework (\rdf): Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/">http:/​/​www.w3.org/TR/2004/REC-rdf-concepts-20040210/</a> (2004).</dd>
  <dt id="ref-122">[122]</dt>
  <dd resource="https://dx.doi.org/10.1145/2566486.2567973" typeof="schema:Article">Nguyen, V., Bodenreider, O., Sheth, A.: Don’t Like \rdf Reification? Making Statements About Statements Using Singleton Property. In: Proceedings of the 23rd International Conference on World Wide Web. pp. 759–770. , New York, NY, USA (2014).</dd>
  <dt id="ref-123">[123]</dt>
  <dd resource="#temporalrdf" typeof="schema:Chapter">Gutierrez, C., Hurtado, C., Vaisman, A.: Temporal \rdf. In: The Semantic Web: Research and Applications. pp. 93–107 (2005).</dd>
  <dt id="ref-124">[124]</dt>
  <dd resource="https://dx.doi.org/10.1109/TKDE.2007.34" typeof="schema:Article">Gutierrez, C., Hurtado, C.A., Vaisman, A.: Introducing Time into \rdf. Knowledge and Data Engineering, IEEE Transactions on. 19, 207–218 (2007).</dd>
  <dt id="ref-125">[125]</dt>
  <dd resource="#streamreasoningsofar" typeof="schema:Article">Barbieri, D., Braga, D., Ceri, S., Della Valle, E., Grossniklaus, M.: Stream Reasoning: Where We Got So Far. In: Proceedings of the NeFoRS2010 Workshop (2010).</dd>
  <dt id="ref-126">[126]</dt>
  <dd resource="#arasu2004stream" typeof="schema:Article">Arasu, A., Babcock, B., Babu, S., Cieslewicz, J., Datar, M., Ito, K., Motwani, R., Srivastava, U., Widom, J.: \stream: The Stanford Data Stream Management System. Book chapter. (2004).</dd>
  <dt id="ref-127">[127]</dt>
  <dd resource="http://ilabt.iminds.be/virtualwall" typeof="schema:CreativeWork">iLab.t, iMinds: Virtual Wall: wired networks and applications. <a href="http://ilabt.iminds.be/virtualwall">http:/​/​ilabt.iminds.be/virtualwall</a></dd>
  <dt id="ref-128">[128]</dt>
  <dd resource="https://code.google.com/p/cqels/wiki/<abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>_engine&#8221; typeof=&#8221;schema:CreativeWork&#8221;>Levan, C.: \cqels engine: Instructions on experimenting \cqels. <a href="https://code.google.com/p/cqels/wiki/<abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>_engine&#8221;>https:/​/​code.google.com/p/cqels/wiki/<abbr title='Continuous Query Evaluation over Linked Stream'>CQELS</abbr>_engine</a></dd>
  <dt id="ref-129">[129]</dt>
  <dd resource="http://streamreasoning.org/download" typeof="schema:CreativeWork">StreamReasoning: Continuous \sparql (\csparql) Ready To Go Pack. <a href="http://streamreasoning.org/download">http:/​/​streamreasoning.org/download</a></dd>
</dl>
</section>
</footer>

<script type="text/javascript">
    window.addEventListener('load', updateIndex);
    window.addEventListener('scroll', updateIndex);
    
    function updateIndex(){
        // Unselect all other entries
        const entries = document.querySelectorAll('a.index-entry-name');
        for (var i = 0; i < entries.length; i++) {
            entries[i].classList.remove('index-entry-active');
        }
    
        // Select the hovered entry
        var pos = getTocPos();
        if (pos) {            
            var toc = document.querySelector('.index-entries-root');
            var match = toc.querySelector('a[href="#' + pos + '"]');
            if (match) {
                match.classList.add('index-entry-active');
            }
        }
    }
    
    // Get the first section that is visible
    function getTocPos() {
        var sections = document.getElementsByTagName('section');
        for (var i = 0; i < sections.length; i++) {
            var section = sections[i];
            if (section.id && section.getElementsByTagName('section').length === 0 && section.getBoundingClientRect().bottom >= 0) {
                return section.id;
            }
        }
    }
</script>



</body>
</html>
